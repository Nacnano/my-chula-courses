{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW3: Transformer from Scratch\n",
        "\n",
        "In this exercise, you are replicating character-level transformer from scratch with Pytorch Lightning. You should end up with similar code to [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "We have prepared for you a dataset and dataloader of พระอภัยมณี by สุนทรภู่ , a famous Thai poet. You should receive your very own nanoสุนทรภู่ by the end of this exercise.\n",
        "\n",
        "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).  \n",
        "Data Source: [Vajirayana - พระอภัยมณี](https://vajirayana.org/%E0%B8%9E%E0%B8%A3%E0%B8%B0%E0%B8%AD%E0%B8%A0%E0%B8%B1%E0%B8%A2%E0%B8%A1%E0%B8%93%E0%B8%B5)"
      ],
      "metadata": {
        "id": "8vdx6X6c8kJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install lightning\n",
        "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt"
      ],
      "metadata": {
        "id": "u544827zmHRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import lightning as L\n",
        "from datetime import datetime\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # B: how many independent sequences will we process in parallel?\n",
        "seq_len = 256    # T: what is the maximum context length for predictions?\n",
        "n_embd = 64     # C: text embedding size\n",
        "n_head = 4      # number of heads\n",
        "n_layer = 4     # number of blocks\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "hRBXmNByL5hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pra-apai-manee-ch1-50.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "JE7GsqOy7Dzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick implementation of character tokenizer\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"All Characters: {''.join(chars)}\")\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"สวัสดีครับ\"))\n",
        "print(decode(encode(\"สวัสดีครับ\")))"
      ],
      "metadata": {
        "id": "x2wvWIFaqbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, seq_len):\n",
        "    self.data = data\n",
        "    self.seq_len = seq_len\n",
        "  def __len__(self):\n",
        "    return len(self.data)-seq_len\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx:idx+seq_len], self.data[idx+1:idx+seq_len+1]\n",
        "\n",
        "train_dataset = TextDataset(train_data, seq_len)\n",
        "val_dataset = TextDataset(val_data, seq_len)\n",
        "print(train_dataset[0])\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "GWBxs6HNuYun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Self-Attention Head (Scaled Dot-Product Attention)\n",
        "\n",
        "This part implements the 3.2.1 Scaled Dot-Product Attention in the paper _Attention is All You Need_.\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
        "$$"
      ],
      "metadata": {
        "id": "Qjjq6aoIV-J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = batch_size,seq_len,n_embd # batch, time, channels\n",
        "head_size = n_embd//n_head    # 16"
      ],
      "metadata": {
        "id": "X05dtVO7l8VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Implementing Queries, Keys, Values\n",
        "\n",
        "This should be the easiest step of the self-attention. Given $x$ with the shape of $B \\times T \\times C$ (batch size, time/sequence length, channel/text embedding size), multiply it with the Query, Key, and Value embedding matrix to get $q$,$k$,$v$ vectors of shape $B \\times T \\times d_k$. Where $d_k$ is the head size (size of each query, key, value vector).\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "Use `nn.Linear` to define the `key`, `query`, and `value` embedding weights (take note to not include the bias). And calculate the `k`, `q`, and `v` vectors from $x$."
      ],
      "metadata": {
        "id": "MtC0yuI0idPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "#### FILL CODE HERE ####\n",
        "# Fill in these weight matrices\n",
        "key = None\n",
        "query = None\n",
        "value = None\n",
        "\n",
        "# Calculate k,q,v vectors\n",
        "k = None   # (B, T, d_k)\n",
        "q = None   # (B, T, d_k)\n",
        "v = None   # (B, T, d_k)\n",
        "######################"
      ],
      "metadata": {
        "id": "HoQqUTxGf-AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_0_0 = torch.Tensor([0.7020,  0.6227, -0.2063, -1.0227,  0.5020, -1.4468,  0.7205, -0.3498, -0.8498,  0.9094,  0.1168, -0.9637, -0.4064, -0.0979,  1.5379, -0.0558])\n",
        "q_0_0 = torch.Tensor([-1.0709, -1.1625,  0.0260,  0.3891, -0.5746,  0.1046, -0.5273,  0.1213, 1.1707,  0.2108,  0.4636,  0.3899,  1.4501, -0.0414,  0.9155,  0.0261])\n",
        "print(torch.allclose(k_0_0, k[0][0].data, atol=1e-4, rtol=0))\n",
        "print(torch.allclose(q_0_0, q[0][0].data, atol=1e-4, rtol=0))"
      ],
      "metadata": {
        "id": "6UMoK-A6nzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: What is the first number of v[0][0]?"
      ],
      "metadata": {
        "id": "aCDb0rYfHhkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjVgWGl4Hj_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Calculate Dot Product of Query and Value\n",
        "\n",
        "Perform dot product of `q` and `k` using `torch.matmul` or `@` such that it has shape $B \\times T \\times T$. Do note that `transpose` is required for this to work, since both are at shape $B \\times T \\times d_k$. And normalize the resulting weights `wei` by $\\sqrt{d_k}$.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/self-attention_softmax.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "Please take a look at the resulting `q` and `k` dot product. In a single batch, a `q` matrix has dimensions $T \\times d_k$ (each row represent the sequence length and columns are the embeddings of head size). We can view each row of `q` as the $\\vec{q}_1$ query vector represented above. The dot product would represent the following resulting matrix:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \\color{red}{q_{1,1}} & \\color{red}{q_{1,2}} & \\color{red}{q_{1,3}} & \\color{red}\\cdots \\\\ q_{2,1} & q_{2,2} & q_{2,3} & \\cdots \\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} \\color{blue}{k_{1,1}} & \\color{blue}{k_{1,2}} & \\color{blue}{k_{1,3}} & \\color{blue}\\cdots \\\\ k_{2,1} & k_{2,2} & k_{2,3} & \\cdots \\end{bmatrix}^T\n",
        "=\n",
        "\\begin{bmatrix} \\color{red}{\\vec{q}_1} \\cdot \\color{blue}{\\vec{k}_1} & \\color{red}{\\vec{q}_1} \\cdot \\vec{k}_2 \\\\ \\vec{q}_2 \\cdot \\color{blue}{\\vec{k}_1} & \\vec{q}_2 \\cdot \\vec{k}_2  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The resulting matrix would have the dimensions of $T \\times T$, where each row is the attention score of each word. For instance, referencing the image above, the first row is the attention scores of the first word \"Thinking\" compared with the other words in the sequence."
      ],
      "metadata": {
        "id": "BupSuPVNqwiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### FILL CODE HERE ####\n",
        "wei =  None # (B, T, d_k) @ (B, d_k, T) ---> (B, T, T)\n",
        "######################"
      ],
      "metadata": {
        "id": "9MICq8ZAqwT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: What shape is `wei` after the dot product of `q` and `k`?"
      ],
      "metadata": {
        "id": "9_RHqPbOJS2v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RUAqf5d8Aqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Perform Masked Attention with `torch.tril`\n",
        "\n",
        "Since we are making an autoregressive decoder-only block, it would be weird for the current token to be able to attend to future tokens. If we look at the figure above, it doesn't make any sense for the word \"Thinking\" to be able to see \"Machines\", else you already know the result to be generated. Hence, you need to \"mask\" these attentions.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "To do this, there is a special kind of matrix called triangular matrix. See the result of `torch.tril` below:"
      ],
      "metadata": {
        "id": "ptQ6-2lfta6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(T,T))[:8,:8]"
      ],
      "metadata": {
        "id": "XUechefjblXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referring to the resulting $Q \\cdot K$ of dimensions $T \\times T$, each row index represents the time dimension, and the columns are all the other words in the sequence. For instance, when we are at the current word \"Thinking\" at time $t=1$, the generation of the second word should not be able to to access $\\vec{q}_1 \\cdot \\vec{k}_1$, and not $\\vec{q}_1 \\cdot \\vec{k}_2$ (corresponding to keys of \"Machines\" and \"are\"). This is illustrated in the matrix below:\n",
        "\n",
        "$$\\require{cancel}$$\n",
        "$$\n",
        "\\begin{matrix} .\\qquad \\text{Thinking} & \\text{Machines} & \\text{are} \\end{matrix} \\\\\n",
        "\\begin{matrix} \\color{blue}{\\text{Thinking}} \\\\ \\text{Machines} \\\\ \\text{are} \\end{matrix}\n",
        "\\begin{bmatrix}  \\color{blue}{\\vec{q}_1 \\cdot \\vec{k}_1} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_2} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_3} \\\\ \\vec{q}_2 \\cdot \\vec{k}_1 & \\vec{q}_2 \\cdot \\vec{k}_2 & \\cancel{\\vec{q}_2 \\cdot \\vec{k}_3} \\\\ \\vec{q}_3 \\cdot \\vec{k}_1 & \\vec{q}_3 \\cdot \\vec{k}_2 & \\vec{q}_3 \\cdot \\vec{k}_3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This is very similar to the triangular matrix. If we are able to filter out the attentions where `tril == 0`, masked self attention will be achieved.\n",
        "\n",
        "Use `masked_fill` on `wei` such that the resulting attention softmax is `0` just like the matrix above.  \n",
        "Note: $-\\infty$ or `float('-inf')` may be required."
      ],
      "metadata": {
        "id": "ybmEPx38-Yrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "#### FILL CODE HERE ####\n",
        "wei = None\n",
        "######################\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "wei[0][:8, :8]"
      ],
      "metadata": {
        "id": "mGv2qPjMnrA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0][0][1:] == 0"
      ],
      "metadata": {
        "id": "Po5K7HhJI_az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Calculate Dot Product of Softmax and Value\n",
        "\n",
        "Should be self-explanatory. We are at the final stage of the equation to get a resulting matrix of shape $B \\times T \\times d_k$ (we use the same dimensions for key and value).\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
        "$$\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"500\" />\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "RYHkY6_RKPmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### FILL CODE HERE ####\n",
        "out = None\n",
        "######################"
      ],
      "metadata": {
        "id": "A0ouoYAiI2bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: What shape is the resulting attention?"
      ],
      "metadata": {
        "id": "76P5Pu2_Mo0k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9QGn_ZaK6PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Putting it all together!\n",
        "\n",
        "Now it's time to code the Attention Head `nn.Module`.\n",
        "1. Initialize all the `nn.Linear` in the constructor. Use the hyperparameter `n_embd` for the embedding size.\n",
        "2. Perform the self-attention calculations in the `forward` function\n",
        "\n",
        "Note that there may be some differences in the implemented version:\n",
        "- Since `tril` is not a parameter in the PyTorch module, it is registered as a `buffer` instead.\n",
        "- A `dropout` is appended after the softmax for regularization"
      ],
      "metadata": {
        "id": "3zhvJj79LiNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.key = None\n",
        "        self.query = None\n",
        "        self.value = None\n",
        "        ######################\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        tril = self.tril[:T, :T] == 0\n",
        "        #### FILL CODE HERE ####\n",
        "        k = None                            # (B,T,d_k)\n",
        "        q = None                            # (B,T,d_k)\n",
        "        v = None                            # (B,T,d_k)\n",
        "\n",
        "        # Calculate the attention scores\n",
        "        wei = None                                       # Dot product of q * k & normalization (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n",
        "        wei = None                                       # Use masked_fill on tril (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)                     # Apply softmax (B, T, T)\n",
        "        wei = self.dropout(wei)                          # Added dropout\n",
        "        out = None                                       # (B, T, T) @ (B, T, d_k) -> (B, T, d_k)\n",
        "        ######################\n",
        "        return out"
      ],
      "metadata": {
        "id": "wkpyS4Ff9qBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: What is Head's output shape?"
      ],
      "metadata": {
        "id": "vj4hooukM8Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(B,T,C)\n",
        "head = Head(head_size)\n",
        "#### FILL CODE HERE ####\n",
        "out = None\n",
        "######################"
      ],
      "metadata": {
        "id": "bpXKLTGiLMQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Multi-Head Attention\n",
        "\n",
        "This part implements the 3.2.2 Multi-Head Attention in the paper _Attention is All You Need`.\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ... ,\\text{head}_h)W^O\\\\\n",
        "\\text{where}\\: \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n",
        "$$"
      ],
      "metadata": {
        "id": "ILFCwZEkOLof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With multiple heads running in parallel, this would give rise to multiple representation subspaces, where each head would have multiple sets of Q/K/V matrices.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "The resulting attention would be all concatenated to a long matrix.\n",
        "\n",
        "To preserve the shape of the vector back to the embedding size $C$ before the feed-forward layer (if the concatenation does not have the same size as the embedding), we use a projection layer $W^O$ with dimensions $hd_k \\times C$.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "There are two things you need to implement:\n",
        "- The `self.heads` which is an `nn.ModuleList` of all the Attention `Head` of $h$ (`num_heads`) layers (these will be computed in parallel). And `torch.cat` to concatenante the heads in `forward`.\n",
        "- The `self.proj` projection layer $W^O$ (with dimensions $C \\times C$ as noted below). Use the hyperparameter `n_embd` for the embedding size. Apply the projection accordingly in `forward`.\n",
        "\n",
        "Note that in the paper, they use $d_k = C/h = 64$. The head size is equal to the embedding size divided by the number of heads. So the higher number of heads, the lower the dimension of the head to preserve computational cost equivalent to single head."
      ],
      "metadata": {
        "id": "jzzokhk_C1Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.heads = None\n",
        "        self.proj = None\n",
        "        ######################\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #### FILL CODE HERE ####\n",
        "        out = None\n",
        "        out = None\n",
        "        ######################\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "XVVLYJs_OR5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5: What is MultiHead's output shape?"
      ],
      "metadata": {
        "id": "xpIrUDkZN8lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(B,T,C)\n",
        "heads = MultiHeadAttention(n_head, head_size)\n",
        "#### FILL CODE HERE ####\n",
        "out = None\n",
        "######################"
      ],
      "metadata": {
        "id": "JKMC83IJVHZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Feed Forward\n",
        "\n",
        "At each block, there is a fully connected feed-forward network. Implement the 3.3 Position-wise Feed-Forward Networks in the paper _Attention is All You Need_.\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1+b_1)W_2 + b_2\n",
        "$$"
      ],
      "metadata": {
        "id": "21lJ8cn8P_Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part gets the resulting matrix of shape $B \\times T \\times C$ from the Multi-Head Attention. The paper noted that they used an embedding size dimensionality of $C=512$ and the feed-forward inner-layer dimensionality $d_{ff}=2048$, which is pretty much $4C$.\n",
        "\n",
        "Implement the Feed-forward equation up top with `nn.Linear` and `nn.ReLU` with the correct embedding size. Use `n_embd` for embedding size. Preserve the shape of $B \\times T \\times C$ in the resulting matrix."
      ],
      "metadata": {
        "id": "J5JbegqUUJuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            #### FILL CODE HERE ####\n",
        "\n",
        "            ######################\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "MQzzhaynP-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(B,T,C)\n",
        "_module = FeedFoward(n_embd)\n",
        "out = _module(x)\n",
        "print(out.shape)\n",
        "out.shape == torch.Size([B,T,n_embd])"
      ],
      "metadata": {
        "id": "X-HQV2C-VDBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6: How many parameters are in a FeedForward module?"
      ],
      "metadata": {
        "id": "kAC-UdkXO54K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1rW9x2UO7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Transformer Block\n",
        "\n",
        "Putting all the blocks together! We have initialize the constructor with all the defined previous components along with `nn.LayerNorm`.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" width=\"400\" />\n",
        "</div>\n",
        "**Note: DO NOT reference this image**\n",
        "\n",
        "Now I would like you to implement the residual connections, self-attention, feed forward, and the layer norm in `forward`. As a slight deviation from the paper, now it is more common to do pre-norm, which is to apply `LayerNorm` before self-attention.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QnTkcVlyoseiZk65-IHbQhESpwmX6Zfx\" width=\"400\" />\n",
        "</div>\n",
        "\n",
        "1. Apply the first layer norm to $x$, put it through self-attention layer, and add in the residual connection.\n",
        "2. Apply the second layer norm, put it through feed forward layer, and add in the residual connection.\n",
        "\n",
        "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).   \n",
        "[Prenorm](https://arxiv.org/pdf/2002.04745)"
      ],
      "metadata": {
        "id": "YKqz0rrqQRF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #### FILL CODE HERE ####\n",
        "        x = None\n",
        "        x = None\n",
        "        ######################\n",
        "        return x"
      ],
      "metadata": {
        "id": "IJdorNIAQRfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Language Model"
      ],
      "metadata": {
        "id": "GYaGUls7QhZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)    # (B,T,C)\n",
        "        x = self.ln_f(x)      # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -seq_len:]            # crop idx to the last block_size tokens\n",
        "            logits, loss = self(idx_cond)           # get the predictions\n",
        "            logits = logits[:, -1, :]               # focus only on the last time step - becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)       # apply softmax to get probabilities - (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution - (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # append sampled index to the running sequence - (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "oql3z2JGQgUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLMModule(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = TransformerLanguageModel()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        xb, yb = batch\n",
        "        # evaluate the loss\n",
        "        logits, loss = self.model(xb, yb)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        xb, yb = val_batch\n",
        "        logits, loss = self.model(xb, yb)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "\n",
        "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
        "            now = datetime.now()\n",
        "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx}/{self.trainer.max_steps} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
        "\n",
        "L.pytorch.seed_everything(42)\n",
        "m = TransformerLMModule()\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
      ],
      "metadata": {
        "id": "TMgeIGxiSqDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7: How many parameters are in your nanoGPT model?"
      ],
      "metadata": {
        "id": "J48ANUs4QHOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
        "                    max_steps=max_iters,\n",
        "                    val_check_interval = eval_interval,\n",
        "                    log_every_n_steps =eval_interval,\n",
        "                    enable_checkpointing =False,\n",
        "                    limit_val_batches = eval_iters)\n",
        "trainer.fit(m, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "PFCvaRPkjFOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8: What's the perplexity (from validation loss) on Step 5000?"
      ],
      "metadata": {
        "id": "3UMES7hETe06"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5G655a_8TipY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9: What's your output from the generated text?"
      ],
      "metadata": {
        "id": "9LpwBtAOTlDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L.pytorch.seed_everything(42)\n",
        "# generate from the model\n",
        "context = torch.tensor([encode(\"๏ อาจารย์พีรพลสอนเอ็นแอลพี\t\")], dtype=torch.long, device=device)\n",
        "print(decode(m.model.to(device).generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "xAbQtCkEh4UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Transformer from HuggingFace\n",
        "\n",
        "In this part you will be using `transformers` from HuggingFace, the go-to library for many models. We will be, in similar fashion, training DistilGPT2 on the same dataset. I have provided you the tokenizer and Dataloader for ease of use."
      ],
      "metadata": {
        "id": "g5ND0b5nVUvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
      ],
      "metadata": {
        "id": "RqoVrwdmVUBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ],
      "metadata": {
        "id": "358YrYXlV9Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len=768\n",
        "batch_size=8\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(text)) # first 90% will be train, rest val\n",
        "train_text = text[:n]\n",
        "val_text = text[n:]\n",
        "\n",
        "class GPT2TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text, tokenizer, seq_len=768):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    for i in range(0, len(text)//700):\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ text[i*700:(i+1)*700] + '<|endoftext|>', truncation=True, max_length=seq_len, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "train_dataset = GPT2TextDataset(train_text, tokenizer, seq_len)\n",
        "val_dataset = GPT2TextDataset(val_text, tokenizer, seq_len)\n",
        "print(len(train_dataset), len(val_dataset))\n",
        "print(train_dataset[0][0].shape)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Ohb0NHQOWB22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read how Auto Classes work in HuggingFace https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes , and use `AutoModelForCausalLM.from_pretrained` to initialize your `distilgpt2` model."
      ],
      "metadata": {
        "id": "Lfz2LJijWY9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "class DistilGPT2(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.model = None\n",
        "        ######################\n",
        "        self.model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        xb, mask = batch\n",
        "        # evaluate the loss\n",
        "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        xb, mask = val_batch\n",
        "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "\n",
        "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
        "            now = datetime.now()\n",
        "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        beginning_text = tokenizer('<|startoftext|>'+\"แต่ปางหลังยังมีกรุงกษัตริย์\", return_tensors=\"pt\").to(device)\n",
        "        sample_outputs = self.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =300, num_return_sequences=1)\n",
        "        for i, sample_output in enumerate(sample_outputs):\n",
        "              print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True).strip()))\n",
        "\n",
        "L.pytorch.seed_everything(42)\n",
        "distilgpt2 = DistilGPT2()\n",
        "print(sum(p.numel() for p in distilgpt2.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
      ],
      "metadata": {
        "id": "pPWfz-UaWEOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10: How many parameters are in the DistilGPT2 model?"
      ],
      "metadata": {
        "id": "eTf-fav6XIne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Training should take around ~18 minutes</font>"
      ],
      "metadata": {
        "id": "G7UG1UIScyjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
        "                    max_epochs =5,\n",
        "                    log_every_n_steps =len(train_dataloader),\n",
        "                    enable_checkpointing =False,\n",
        "                    )\n",
        "trainer.fit(distilgpt2, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "btFuxXQWXIXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q11: What's the perplexity (from validation loss) on the last step?"
      ],
      "metadata": {
        "id": "3fcy3dcGc2RM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnHTZjgJc5yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12: What's the output from the generated text?"
      ],
      "metadata": {
        "id": "LUcvTGYMc6ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L.pytorch.seed_everything(40)\n",
        "beginning_text = tokenizer('<|startoftext|>'+\"อาจารย์พีรพลสอนเอ็นแอลพี\", return_tensors=\"pt\")\n",
        "output = distilgpt2.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =500, num_return_sequences=1)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True).strip())"
      ],
      "metadata": {
        "id": "eh6UNBw6c8TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
