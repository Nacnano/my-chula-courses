{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"pDRbfJqgPfdr","cell_type":"markdown","source":"# HW7: Beam Search Decoding - News Headline Generation","metadata":{"id":"pDRbfJqgPfdr"}},{"id":"fSDxXAqSPeBB","cell_type":"markdown","source":"In this exercise, you are going to learn and implement decoding techniques for sequence generation. Usually, the sequence is generated word-by-word from a model. In each step, the model predicted the most likely word based on the predicted words in previous steps (this is called auto-regressive decoding).\n\nAs such, it is very important how you decide on what to predicted at each step, as it will be conditioned on to predicted all of the following steps. We will implement two of main decoding techniques introduced in the lecture: **Greedy Decoding** and **Beam Search Decoding**. Greedy Decoding immediately chooses the word with best score at each step, while Beam Search Decoding focuses on the sequence that give the best score overall.\n\nTo complete this exercise, you will need to complete the methods for decoding for a text generation model trained on [New York Times Comments and Headlines dataset](https://www.kaggle.com/aashita/nyt-comments). The model is trained to predict a headline for the news given seed text. You do not need to train any model model in this exercise as we provide both the pretrained model and dictionary.\n","metadata":{"id":"fSDxXAqSPeBB"}},{"id":"YFlSAvCfiZXf","cell_type":"markdown","source":"## Download model and vocab and setup","metadata":{"id":"YFlSAvCfiZXf"}},{"id":"q5gRmwtdiYjp","cell_type":"code","source":"!wget -O vocab.txt https://www.dropbox.com/s/ht12ua9vpkep6l8/hw9_vocab.txt?dl=0\n!wget -O model.bin https://www.dropbox.com/s/okmri7cnd729rr5/hw9_model.bin?dl=0","metadata":{"id":"q5gRmwtdiYjp","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:20.468866Z","iopub.execute_input":"2025-02-23T03:43:20.469117Z","iopub.status.idle":"2025-02-23T03:43:23.510012Z","shell.execute_reply.started":"2025-02-23T03:43:20.469097Z","shell.execute_reply":"2025-02-23T03:43:23.509169Z"}},"outputs":[{"name":"stdout","text":"--2025-02-23 03:43:20--  https://www.dropbox.com/s/ht12ua9vpkep6l8/hw9_vocab.txt?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/zlkw3il9cj4c121vtrrxh/hw9_vocab.txt?rlkey=m5gflik1lhvpenwydd3gfwvp7&dl=0 [following]\n--2025-02-23 03:43:20--  https://www.dropbox.com/scl/fi/zlkw3il9cj4c121vtrrxh/hw9_vocab.txt?rlkey=m5gflik1lhvpenwydd3gfwvp7&dl=0\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com/cd/0/inline/CkrtZtEgx6PQK1-KFycEvxwloI7tt0YfdFT5lblpzxPCZDCKMAZq_ElKYP8rco4VVrnPuDlo_apAGKo3Tff3of2RP_BqD1kL6Pyrp5lOfuPOjJBVsj5zz6JxUuOMSCm917K2ZvOuEWKXM8nOIo6qqsk0/file# [following]\n--2025-02-23 03:43:21--  https://uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com/cd/0/inline/CkrtZtEgx6PQK1-KFycEvxwloI7tt0YfdFT5lblpzxPCZDCKMAZq_ElKYP8rco4VVrnPuDlo_apAGKo3Tff3of2RP_BqD1kL6Pyrp5lOfuPOjJBVsj5zz6JxUuOMSCm917K2ZvOuEWKXM8nOIo6qqsk0/file\nResolving uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com (uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\nConnecting to uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com (uc640fc2445b68b147b94ca48de3.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 78729 (77K) [text/plain]\nSaving to: ‘vocab.txt’\n\nvocab.txt           100%[===================>]  76.88K  --.-KB/s    in 0.02s   \n\n2025-02-23 03:43:21 (3.53 MB/s) - ‘vocab.txt’ saved [78729/78729]\n\n--2025-02-23 03:43:21--  https://www.dropbox.com/s/okmri7cnd729rr5/hw9_model.bin?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/es8o6240q6qogbulewk2s/hw9_model.bin?rlkey=r4rjl9zuwwg72g7ws5mecv47r&dl=0 [following]\n--2025-02-23 03:43:21--  https://www.dropbox.com/scl/fi/es8o6240q6qogbulewk2s/hw9_model.bin?rlkey=r4rjl9zuwwg72g7ws5mecv47r&dl=0\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com/cd/0/inline/CkoDYezyaip9fojvEJWtthVkXM1XfZIopPHktV1tX7rEe4v5F6S-Y0WjBvOczHjY2kHoPcadfJDcfjYrvn4q8YDdTNk-g0zmEGKOI2xD9AzLYJUNfTbrLLMQiHdR_QHci7Wv_rZ4xTCH5S75oaddNsPe/file# [following]\n--2025-02-23 03:43:22--  https://uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com/cd/0/inline/CkoDYezyaip9fojvEJWtthVkXM1XfZIopPHktV1tX7rEe4v5F6S-Y0WjBvOczHjY2kHoPcadfJDcfjYrvn4q8YDdTNk-g0zmEGKOI2xD9AzLYJUNfTbrLLMQiHdR_QHci7Wv_rZ4xTCH5S75oaddNsPe/file\nResolving uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com (uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\nConnecting to uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com (uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /cd/0/inline2/CkqYyN11HnRmdYLd8CJiSAHp1vvEbNUEDeL5Wu0tb1mK6fInjSE6O-ca5ZT0ki9E2XX47yyKJ3RRaNR0hsCnPaXhf-cKKAfs1IcUSxNGfgNek33O1kx-PWsSns91jPw0kCgFI_XfOCZ4dDxw5y92T7kcfOZssGWKcWgoz8fVg6Ok_b1HtyX2JE13Xev0D35VP5meWrqrAnPMGm9FgP2PZL-_r222LvwWu03l8aB5JGqrNCswjn_3KXmHgz-xTKfKDU6HdtvcKCCh6SGkZ2PFXm37oDBnZUBPFbu6yS7BraUEEHOhclxy3x6s7V1D13R2HsYEflAM4deMh3uFWf9Fw8UwX712MaxGWrbaNa71YGyySmZmR9kE1SlBvZsEhDNVyEM/file [following]\n--2025-02-23 03:43:22--  https://uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com/cd/0/inline2/CkqYyN11HnRmdYLd8CJiSAHp1vvEbNUEDeL5Wu0tb1mK6fInjSE6O-ca5ZT0ki9E2XX47yyKJ3RRaNR0hsCnPaXhf-cKKAfs1IcUSxNGfgNek33O1kx-PWsSns91jPw0kCgFI_XfOCZ4dDxw5y92T7kcfOZssGWKcWgoz8fVg6Ok_b1HtyX2JE13Xev0D35VP5meWrqrAnPMGm9FgP2PZL-_r222LvwWu03l8aB5JGqrNCswjn_3KXmHgz-xTKfKDU6HdtvcKCCh6SGkZ2PFXm37oDBnZUBPFbu6yS7BraUEEHOhclxy3x6s7V1D13R2HsYEflAM4deMh3uFWf9Fw8UwX712MaxGWrbaNa71YGyySmZmR9kE1SlBvZsEhDNVyEM/file\nReusing existing connection to uc4cdb7272d0962c2c9856d0cef1.dl.dropboxusercontent.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 8690974 (8.3M) [application/octet-stream]\nSaving to: ‘model.bin’\n\nmodel.bin           100%[===================>]   8.29M  45.8MB/s    in 0.2s    \n\n2025-02-23 03:43:23 (45.8 MB/s) - ‘model.bin’ saved [8690974/8690974]\n\n","output_type":"stream"}],"execution_count":1},{"id":"i9ITxmo5i-s6","cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Whitespace","metadata":{"id":"i9ITxmo5i-s6","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:23.510953Z","iopub.execute_input":"2025-02-23T03:43:23.511179Z","iopub.status.idle":"2025-02-23T03:43:29.768194Z","shell.execute_reply.started":"2025-02-23T03:43:23.511160Z","shell.execute_reply":"2025-02-23T03:43:29.767538Z"}},"outputs":[],"execution_count":2},{"id":"VcDZCYkEi0b4","cell_type":"code","source":"class RNNmodel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, dropout_rate):\n\n        super().__init__()\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, 128, num_layers=2,\n                     batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(128, vocab_size)\n\n    def forward(self, src):\n        embedding = self.embedding(src)\n        output,_ = self.rnn(embedding)\n        output = self.dropout(output)\n        prediction = self.fc2(output)\n        return prediction","metadata":{"id":"VcDZCYkEi0b4","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:29.768864Z","iopub.execute_input":"2025-02-23T03:43:29.769170Z","iopub.status.idle":"2025-02-23T03:43:29.774106Z","shell.execute_reply.started":"2025-02-23T03:43:29.769150Z","shell.execute_reply":"2025-02-23T03:43:29.773256Z"}},"outputs":[],"execution_count":3},{"id":"6bZt9R0JjL8l","cell_type":"code","source":"with open(\"vocab.txt\") as f:\n  vocab_file = f.readlines()\nembedding_dim = 64\ndropout_rate = 0.2\n\nmodel = RNNmodel(len(vocab_file), embedding_dim, dropout_rate)\nmodel.load_state_dict(torch.load(\"model.bin\",map_location='cpu'))\nmodel.eval()","metadata":{"id":"6bZt9R0JjL8l","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:29.774937Z","iopub.execute_input":"2025-02-23T03:43:29.775122Z","iopub.status.idle":"2025-02-23T03:43:29.926647Z","shell.execute_reply.started":"2025-02-23T03:43:29.775106Z","shell.execute_reply":"2025-02-23T03:43:29.925730Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-e56f35018f72>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"model.bin\",map_location='cpu'))\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"RNNmodel(\n  (embedding): Embedding(10054, 64)\n  (rnn): LSTM(64, 128, num_layers=2, batch_first=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc2): Linear(in_features=128, out_features=10054, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"id":"mOEWXsXzjpn6","cell_type":"code","source":"vocab = [v.strip() for v in vocab_file]\nvocab_size = len(vocab)\nprint(f\"Vocab Size: {vocab_size}\")\nvocab[:10]","metadata":{"id":"mOEWXsXzjpn6","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:29.927526Z","iopub.execute_input":"2025-02-23T03:43:29.927859Z","iopub.status.idle":"2025-02-23T03:43:29.934359Z","shell.execute_reply.started":"2025-02-23T03:43:29.927831Z","shell.execute_reply":"2025-02-23T03:43:29.933710Z"}},"outputs":[{"name":"stdout","text":"Vocab Size: 10054\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['<unk>', '<pad>', '<eos>', 'the', 'a', 'to', 'of', 's', 'in', 'for']"},"metadata":{}}],"execution_count":5},{"id":"sem3jbjoF_d8","cell_type":"code","source":"stoi = { ch:i for i,ch in enumerate(vocab) }\ntokenizer = Tokenizer(WordLevel(stoi, unk_token=\"<unk>\"))\ntokenizer.pre_tokenizer = Whitespace()\ntokenized_text = tokenizer.encode(\"the a of to unknowns\")\nprint(tokenized_text)\nprint(tokenized_text.ids)\nprint(tokenized_text.tokens)\nprint(tokenizer.decode(tokenized_text.ids))","metadata":{"id":"sem3jbjoF_d8","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:43:29.936369Z","iopub.execute_input":"2025-02-23T03:43:29.936587Z","iopub.status.idle":"2025-02-23T03:43:30.004102Z","shell.execute_reply.started":"2025-02-23T03:43:29.936569Z","shell.execute_reply":"2025-02-23T03:43:30.003481Z"}},"outputs":[{"name":"stdout","text":"Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n[3, 4, 6, 5, 0]\n['the', 'a', 'of', 'to', '<unk>']\nthe a of to <unk>\n","output_type":"stream"}],"execution_count":6},{"id":"rSt1yuR19co-","cell_type":"markdown","source":"## 1. TODO: Greedy decode\nNormally, in sequence generation task, the model will continue generating tokens until an end-of-sequence symbol appear or the maximum length is reached. For this task:\n- The end-of-sequence symbol is \"< eos >\" and its index is 2\n- Use the maximum generation length of 15","metadata":{"id":"rSt1yuR19co-"}},{"id":"oUCajb2BvKnN","cell_type":"code","source":"eos_token = '<eos>'\neos_id = vocab.index(eos_token)\nmax_gen_length = 15","metadata":{"id":"oUCajb2BvKnN","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:30:06.238554Z","iopub.execute_input":"2025-02-23T05:30:06.238882Z","iopub.status.idle":"2025-02-23T05:30:06.242540Z","shell.execute_reply.started":"2025-02-23T05:30:06.238857Z","shell.execute_reply":"2025-02-23T05:30:06.241725Z"}},"outputs":[],"execution_count":87},{"id":"e6638613","cell_type":"code","source":"from typing import List, Tuple\nimport torch\nimport torch.nn.functional as F\n\ndef greedy_decode(\n    seed_text: str, tokenizer, model = model, eos_id: int = eos_id, max_gen_length: int = 15\n) -> Tuple[List[int], List[float]]:\n    \"\"\"Greedy decodes with seed text.\n\n        Args:\n        seed_text: The seed string to be used as initial input to the model.\n        tokenizer: The tokenizer for converting word to index and back.\n\n        Your code should do the followings:\n          1. Convert current_text to sequences of indices\n          2. Predict the next token using the model and choose the token with the highest score as output\n          3. Append the predicted index to current_text\n          4. Loop until completion\n          5. Return text prediction and a list of probabilities of each step\n\n        You do not need to stop early when end-of-sequence token is generated and can continue decoding\n        until max_gen_length is reached. We can filter the eos token out later.\n    \"\"\"\n    # Convert seed text into token IDs\n    current_tokens = torch.tensor([tokenizer.encode(seed_text).ids], dtype=torch.long)\n    probs: List[float] = []\n    found_eos = False\n\n    while current_tokens.shape[1] < max_gen_length:\n        # Predict next token probabilities\n        pred = model(current_tokens)[0]  # Shape: (1, seq_len, vocab_size)\n        pred_probs = F.softmax(pred, dim=-1)  # Get the last token's probabilities\n\n        # Select the token with the highest probability (greedy selection)\n        top_prob, top_token_id = torch.topk(pred_probs, k=1)\n        top_prob.squeeze_(1)\n        top_token_id.squeeze_(1)\n\n        # if EOS token is generated\n        if top_token_id[-1].item() == eos_id:\n            found_eos = True\n            \n        # Append probability if the EOS token is not found\n        if not found_eos:\n            probs.append(top_prob[-1].item())\n\n        # Append new token to the sequence\n        current_tokens = torch.cat([current_tokens, torch.tensor([[top_token_id[-1]]])], dim=1)\n\n    output =  current_tokens.squeeze(0).tolist()\n\n    return output,probs","metadata":{"id":"e6638613","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:30:07.518074Z","iopub.execute_input":"2025-02-23T05:30:07.518368Z","iopub.status.idle":"2025-02-23T05:30:07.524962Z","shell.execute_reply.started":"2025-02-23T05:30:07.518344Z","shell.execute_reply":"2025-02-23T05:30:07.524096Z"}},"outputs":[],"execution_count":88},{"id":"9f78f6c8","cell_type":"code","source":"def clean_output(token_ids, eos_token_id):\n    \"\"\"Drop eos_token and every words that follow\"\"\"\n    if eos_token_id in token_ids:\n        eos_index = token_ids.index(eos_token_id)\n        token_ids = token_ids[:eos_index]\n    return tokenizer.decode(token_ids)","metadata":{"id":"9f78f6c8","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T04:58:18.181471Z","iopub.execute_input":"2025-02-23T04:58:18.181802Z","iopub.status.idle":"2025-02-23T04:58:18.185707Z","shell.execute_reply.started":"2025-02-23T04:58:18.181779Z","shell.execute_reply":"2025-02-23T04:58:18.184841Z"}},"outputs":[],"execution_count":68},{"id":"bdd42c8a","cell_type":"code","source":"sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\"]\nfor seed in sample_seeds:\n    output_text, probs = greedy_decode(seed, tokenizer)\n    cleaned_text = clean_output(output_text, stoi[eos_token])\n    print(f\"Seed: {seed}\\nOutput: {cleaned_text}\\nProbabilities: {probs}\\n\")","metadata":{"id":"bdd42c8a","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T04:58:19.187350Z","iopub.execute_input":"2025-02-23T04:58:19.187606Z","iopub.status.idle":"2025-02-23T04:58:19.374472Z","shell.execute_reply.started":"2025-02-23T04:58:19.187586Z","shell.execute_reply":"2025-02-23T04:58:19.373785Z"}},"outputs":[{"name":"stdout","text":"Seed: to\nOutput: to encourage creativity in the new york bill\nProbabilities: [0.04071049019694328, 0.2661301791667938, 0.2581588327884674, 0.23116441071033478, 0.07794325798749924, 0.06978006660938263, 0.22445181012153625, 0.06381542235612869]\n\nSeed: america\nOutput: america s lethal export\nProbabilities: [0.449653685092926, 0.04639982804656029, 0.7206945419311523, 0.9869417548179626]\n\nSeed: people\nOutput: people to balloon to make a criminal with a dog with a callous rival\nProbabilities: [0.3584842383861542, 0.039465781301259995, 0.26952996850013733, 0.03377291187644005, 0.2815666198730469, 0.030896710231900215, 0.326255202293396, 0.32243970036506653, 0.03489113971590996, 0.22298309206962585, 0.5808280110359192, 0.04674474895000458, 0.2661650776863098, 0.9633642435073853]\n\nSeed: next\nOutput: next phenom english clubs 2 call another deal in the same arrivals\nProbabilities: [0.1724235862493515, 0.23235639929771423, 0.7396265268325806, 0.07888377457857132, 0.044043008238077164, 0.057693321257829666, 0.03296952322125435, 0.3304007947444916, 0.6040365099906921, 0.03174588456749916, 0.07468770444393158, 0.9989264607429504]\n\nSeed: picture\nOutput: picture perfect chapter a spot of view of banning care\nProbabilities: [0.25000011920928955, 0.017607450485229492, 0.025983257219195366, 0.053818922489881516, 0.31066474318504333, 0.04046903923153877, 0.2489873468875885, 0.045758090913295746, 0.08654455095529556, 0.6452388763427734]\n\nSeed: on\nOutput: on the catwalk in saudi arabia\nProbabilities: [0.3077561855316162, 0.02204967476427555, 0.24775120615959167, 0.18319880962371826, 0.7695555090904236, 0.9782545566558838]\n\n","output_type":"stream"}],"execution_count":69},{"id":"h99jCVvjUvFT","cell_type":"markdown","source":"Your output should be:\n\n*   to encourage creativity in the new york bill\n*   america s lethal export\n*   people to balloon to make a criminal with a dog with a callous rival\n*   next phenom english clubs 2 call another deal in the same arrivals\n*   picture perfect chapter a spot of view of banning care  \n*   on the catwalk in saudi arabia\n\n\n\n\n\n","metadata":{"id":"h99jCVvjUvFT"}},{"id":"7553f608","cell_type":"markdown","source":"## 2. TODO: Beam search decode\n\nAnother well-known decoding method is beam search decoding that focuses more on the overall sequence score.\n\nInstead of greedily choosing the token with the highest score for each step, beam search decoding expands all possible next tokens and keeps the __k__ most likely sequence at each step, where __k__ is a user-specified beam size. A sequence score is also calculated according user-specified cal_score() function.\nThe beam with the highest score after the decoding process is done will be the output.","metadata":{"id":"7553f608"}},{"id":"8kArvA-6xLmQ","cell_type":"markdown","source":"There are a few things that you need to know before implementing a beam search decoder:\n- When the eos token is produced, you can stop expanding that beam\n- However, the ended beams must be sorted together with active beams\n- The decoding ends when every beams are either ended or reached the maximum length, but for this task, you can continue decoding until the max_gen_len is reached\n- We usually work with probability in log scale to avoid numerical underflow. You should use np.log(score) before any calculation\n- **As probabilities for some classes will be very small, you must add a very small value to the score before taking log e.g np.log(prob + 0.00000001)**","metadata":{"id":"8kArvA-6xLmQ"}},{"id":"MQRZftUYxcCU","cell_type":"markdown","source":"#### Sequence Score\nThe naive way to calculate the sequence score is to __multiply every token scores__ together. However, doing so will make the decoder prefer shorter sequence as you multiply the sequence score with a value between \\[0,1\\] for every tokens in the sequence. Thus, we usually normalize the sequence score with its length by calculating its __geometric mean__ instead.\n\n**You should do this in log scale**","metadata":{"id":"MQRZftUYxcCU"}},{"id":"d76c6f52","cell_type":"code","source":"import numpy as np\ndef cal_score(score_list, length, normalized=False): #cal score for each beam from a list of probs\n    # TODO: cal score for each beam from a list of probs\n    log_scores = [np.log(s + 1e-15) for s in score_list]\n    if normalized:\n        return np.sum(log_scores) / length\n    else:\n        return np.sum(log_scores)","metadata":{"id":"d76c6f52","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:26:54.082474Z","iopub.execute_input":"2025-02-23T05:26:54.082831Z","iopub.status.idle":"2025-02-23T05:26:54.087478Z","shell.execute_reply.started":"2025-02-23T05:26:54.082805Z","shell.execute_reply":"2025-02-23T05:26:54.086760Z"}},"outputs":[],"execution_count":84},{"id":"bb1dd75a","cell_type":"code","source":"def beam_search_decode(seed_text, max_gen_len, tokenizer, beam_size=5, normalized=False):\n    \"\"\"We will do beam search decoing using seed text in this function.\n\n    Output:\n    beams: A list of top k beams after the decoding ended, each beam is a list of\n      [seed_text, list of scores, length]\n\n    Your code should do the followings:\n    1.Loop until max_gen_len is reached.\n    2.During each step, loop thorugh each beam and use it to predict the next word.\n      If a beam is already ended, continues without expanding.\n    3.Sort all hypotheses according to cal_score().\n    4.Keep top k hypotheses to be used at the next step.\n    \"\"\"\n    # Initialize beams with (tokenized sequence, list of scores, current length, is_finished)\n    seed_token_ids = torch.tensor([tokenizer.encode(seed_text).ids], dtype=torch.long)  # (1, seq_len)\n    beams = [[seed_token_ids, [], 0, False]]  # Start with the seed tokenized text\n\n    for _ in range(max_gen_len):\n        new_beams = []\n\n        for beam in beams:\n            token_ids, scores, length, is_finished = beam\n\n            # Skip finished beams\n            if is_finished:\n                new_beams.append(beam)\n                continue\n\n            # Get model predictions\n            pred = model(token_ids)[0]  # Shape: (1, seq_len, vocab_size)\n            pred_probs = F.softmax(pred, dim=-1)  # Get last token probabilities\n            next_token_prob = pred_probs[-1]\n\n            # Expand beams with new token predictions\n            for next_token_id, next_token_prob in enumerate(next_token_prob):\n                new_text = torch.cat((token_ids, torch.tensor([[next_token_id]])), dim=1)\n                new_length = length + 1\n                new_finished = (next_token_id == stoi[eos_token])  # Check if eos token reached\n                new_score = scores + [next_token_prob.item()]  # Append new probability score\n                new_beams.append((new_text, new_score, new_length, new_finished))\n\n\n        # Sort and keep top k beams based on `cal_score`\n        new_beams.sort(key=lambda b: cal_score(b[1], b[2], normalized), reverse=True)\n        beams = new_beams[:beam_size]\n\n    return beams","metadata":{"id":"bb1dd75a","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:27:09.843178Z","iopub.execute_input":"2025-02-23T05:27:09.843465Z","iopub.status.idle":"2025-02-23T05:27:09.850411Z","shell.execute_reply.started":"2025-02-23T05:27:09.843444Z","shell.execute_reply":"2025-02-23T05:27:09.849649Z"}},"outputs":[],"execution_count":85},{"id":"i_eqGDA09zqk","cell_type":"markdown","source":"## 3. Generate!\nGenerate 6 sentences based on the given seed texts.\n\nDecode with the provided seed texts with beam_size 5, max_gen_len 10. Compare the results between greedy, normalized, and unnormalized decoding.\n\nPrint the result using greedy decoding and top 2 results each using unnormalized and normalized decoing for each seed text.\n\nAlso, print scores of each candidate according to cal_score(). Use normalization for greedy decoding.","metadata":{"id":"i_eqGDA09zqk"}},{"id":"d40a3cb0","cell_type":"code","source":"sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\"]\nmax_gen_len=10\n\ndef generate(sample_seeds):\n    for seed in sample_seeds:\n        def _greedy():\n            print(\"-Greedy-\")\n            output_text, probs = greedy_decode(seed, tokenizer)\n            cleaned_text = clean_output(output_text, stoi[eos_token])\n            score = cal_score(probs, len(probs), normalized=True)\n            print(f\"{cleaned_text}  {round(np.exp(score), 2)}\")\n        _greedy()\n\n        def _unnormalized():\n            print(\"-Unnormalized-\")\n            beams = beam_search_decode(seed, max_gen_length, tokenizer, beam_size=5, normalized=False)\n            for beam in beams[:2]:\n                output_text = clean_output(beam[0][0].tolist(), stoi[eos_token])\n                if beam[3]:\n                    score = cal_score(beam[1][:-1], beam[2]-1, normalized=False)\n                else:\n                    score = cal_score(beam[1], beam[2], normalized=False)\n                print(f\"{output_text.title()}  {round(np.exp(score), 2)}\")\n        _unnormalized()\n\n        def _normalized():\n            print(\"-Normalized-\")\n            beams = beam_search_decode(seed, max_gen_length, tokenizer, beam_size=5, normalized=True)\n            for beam in beams[:2]:\n                output_text = clean_output(beam[0][0].tolist(), stoi[eos_token])\n                if beam[3]:\n                    score = cal_score(beam[1][:-1], beam[2]-1, normalized=True)\n                else:\n                    score = cal_score(beam[1], beam[2], normalized=True)\n                print(f\"{output_text.title()}  {round(np.exp(score), 2)}\")\n        _normalized()\n\n        print()\n\ngenerate(sample_seeds)","metadata":{"id":"d40a3cb0","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:30:11.942649Z","iopub.execute_input":"2025-02-23T05:30:11.942979Z","iopub.status.idle":"2025-02-23T05:32:50.079498Z","shell.execute_reply.started":"2025-02-23T05:30:11.942953Z","shell.execute_reply":"2025-02-23T05:32:50.078660Z"}},"outputs":[{"name":"stdout","text":"-Greedy-\nto encourage creativity in the new york bill  0.13\n-Unnormalized-\nTo Consult Exploring Recipes For New Jersey  0.0\nTo Consult Exploring Recipes Up The Pacific Northwest  0.0\n-Normalized-\nTo Consult Exploring Recipes Up The Pacific Northwest  0.17\nTo Consult Exploring Recipes Up The Least Of The Week  0.16\n\n-Greedy-\namerica s lethal export  0.25\n-Unnormalized-\nAmerica S Lethal Export  0.02\nAmerica S Desert Aisles  0.01\n-Normalized-\nAmerica S Lethal Export  0.25\nAmerica S Desert Aisles  0.2\n\n-Greedy-\npeople to balloon to make a criminal with a dog with a callous rival  0.14\n-Unnormalized-\nPeople To Balloon For A Criminal  0.0\nPeople To Balloon For A Criminal With Trump  0.0\n-Normalized-\nPeople To Balloon For A Criminal With A Second Fiddle  0.13\nPeople To Balloon For A Criminal With Trump  0.13\n\n-Greedy-\nnext phenom english clubs 2 call another deal in the same arrivals  0.12\n-Unnormalized-\nNext S Blist Revue  0.0\nNext Phenom English Clubs 1 A Chance To Be Back  0.0\n-Normalized-\nNext S Blist Revue  0.14\nNext Phenom English Clubs 1 A Chance To Be Back  0.14\n\n-Greedy-\npicture perfect chapter a spot of view of banning care  0.08\n-Unnormalized-\nPicture Perfect Use Coffee  0.0\nPicture Korean A Bonanza Of Pancakes  0.0\n-Normalized-\nPicture Korean A Bonanza Of Contemplation Times Of Trump S Son  0.12\nPicture Korean A Bonanza Of Contemplation Times Of Trump S Prime Directive  0.11\n\n-Greedy-\non the catwalk in saudi arabia  0.19\n-Unnormalized-\nOn The Billboard Chart  0.0\nOn The Catwalk In Saudi Arabia  0.0\n-Normalized-\nOn The Whole30 Diet Vowing To Eat Smarter Carbs To Be Insufficient  0.25\nOn The Whole30 Diet Vowing To Eat Smarter Carbs For Because You Want  0.23\n\n","output_type":"stream"}],"execution_count":89},{"id":"icfu6pOzWUSt","cell_type":"markdown","source":"Your output should be:\n\n\n```\n-Greedy-\nto encourage creativity in the new york bill  0.13\n-Unnormalized-\nTo Consult Exploring Recipes For New Jersey 0.00\nTo Consult Exploring Recipes Up The Pacific Northwest 0.00\n-Normalized-\nTo Consult Exploring Recipes Up The Pacific Northwest 0.17\nTo Consult Exploring Recipes Up The Least Of The Week 0.16\n\n-Greedy-\namerica s lethal export  0.25\n-Unnormalized-\nAmerica S Lethal Export 0.02\nAmerica S Desert Aisles 0.01\n-Normalized-\nAmerica S Lethal Export 0.25\nAmerica S Desert Aisles 0.20\n\n-Greedy-\npeople to balloon to make a criminal with a dog with a callous rival  0.14\n-Unnormalized-\nPeople To Balloon For A Criminal 0.00\nPeople To Balloon For A Criminal With Trump 0.00\n-Normalized-\nPeople To Balloon For A Criminal With A Second Fiddle 0.13\nPeople To Balloon For A Criminal With Trump 0.13\n\n-Greedy-\nnext phenom english clubs 2 call another deal in the same arrivals  0.12\n-Unnormalized-\nNext S Blist Revue 0.00\nNext Phenom English Clubs 1 A Chance To Be Back 0.00\n-Normalized-\nNext S Blist Revue 0.14\nNext Phenom English Clubs 1 A Chance To Be Back 0.14\n\n-Greedy-\npicture perfect chapter a spot of view of banning care  0.08\n-Unnormalized-\nPicture Perfect Use Coffee 0.00\nPicture Korean A Bonanza Of Pancakes 0.00\n-Normalized-\nPicture Korean A Bonanza Of Contemplation Times Of Trump S Son 0.12\nPicture Korean A Bonanza Of Pancakes 0.07\n\n-Greedy-\non the catwalk in saudi arabia  0.19\n-Unnormalized-\nOn The Billboard Chart 0.00\nOn The Catwalk In Saudi Arabia 0.00\n-Normalized-\nOn The Whole30 Diet Vowing To Eat Smarter Carbs To Be 0.27\nOn The Whole30 Diet Vowing To Eat Smarter Carbs For Because 0.26\n\n```\n\n","metadata":{"id":"icfu6pOzWUSt"}},{"id":"tquJVskBeM9m","cell_type":"markdown","source":"# Answer Questions in MyCourseVille!\n\nUse the seed word \"usa\" to answer questions in MCV.","metadata":{"id":"tquJVskBeM9m"}},{"id":"7ddf5e01-ba03-47f1-bfb4-3fee5c9c24df","cell_type":"code","source":"generate(['usa'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:33:32.226485Z","iopub.execute_input":"2025-02-23T05:33:32.226870Z","iopub.status.idle":"2025-02-23T05:34:09.380645Z","shell.execute_reply.started":"2025-02-23T05:33:32.226841Z","shell.execute_reply":"2025-02-23T05:34:09.379628Z"}},"outputs":[{"name":"stdout","text":"-Greedy-\nusa s duty to investigate  0.09\n-Unnormalized-\nUsa S Duty To Investigate  0.0\nUsa S Bleak Season 3 Episode 4 Recap Chicken  0.0\n-Normalized-\nUsa S Bleak Season 1 Episode 2 Darkness Descends  0.19\nUsa S Bleak Season 3 Episode 4 Recap A Seduction  0.18\n\n","output_type":"stream"}],"execution_count":90}]}