{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YbUOJOIJEiEc"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_rMUycDEXIc"
      },
      "source": [
        "# HW 8: Low Rank Adaptation (LoRA)\n",
        "\n",
        "In this assignment, you will learn to implement low-rank adaptation both from scratch and using a libraryâ€”specifically, with PyTorch and the PEFT library, respectively.\n",
        "\n",
        "This assignment is divided into two sections:\n",
        "\n",
        "In the first section, we introduce the parameter-efficient transfer learning (PET) method. We use LoRA to adapt the GPT2 model for the SST-2 dataset. This section will teach you how LoRA works and how to implement it from scratch using forward_hook.\n",
        "\n",
        "In the second section, we introduce the PEFT library, which allows us to perform LoRA easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZEXsKKEyCA"
      },
      "source": [
        "# Part 1: LoRA from Scratch\n",
        "With the discovery of scaling properties in deep learning models, several researchers tend to increase model size to achieve emergent properties, especially in the natural language processing (NLP) field. For example, GPT-3 contains 175 billion parameters, making it nearly impossible to fine-tune on limited resources. This trend prevents students like us from adapting these enormous foundation models on a single GPU (or with small resources).\n",
        "\n",
        "To alleviate this problem, researchers have developed new fine-tuning methods, known as parameter-efficient transfer learning, which allow us to train large models with limited resources. The benefits of these methods extend not only to the training process but also to deployment. After fine-tuning, we only need to save a small number of parameters (the LoRA weights), enabling us to deploy the foundation model to various downstream tasks using minimal storage. One of the prevailing methods is Low Rank Adaptation (LoRA).\n",
        "\n",
        "Another popular option is prompt tuning, where we only train special tokens that are prepended to the input. However, this is not the focus of this homework.\n",
        "\n",
        "In this section, we will introduce Low-Rank Adaptation. You are assigned to implement LoRA on GPT2 model. We will finetune the model to SST-2 dataset using the traditional and LoRA method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPm3OUpHhLF"
      },
      "source": [
        "## Load Dataset and Model\n",
        "In this step, we will prepare the GPT-2 model and the SST-2 dataset.\n",
        "\n",
        "SST-2 is a widely used dataset for sentiment analysis, extracted from movie reviews, containing sentences labeled as either positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__O2H4x6Cf4h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2ForSequenceClassification, GPT2TokenizerFast\n",
        "from datasets import load_dataset\n",
        "from tqdm.autonotebook import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
        "\n",
        "# Load the GPT-2 model for sequence classification and its tokenizer\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# GPT-2 does not have a pad token by default so we set it to the EOS token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the SST-2 dataset\n",
        "train_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "train_dataset_raw, val_dataset_raw = train_dataset_raw.train_test_split(test_size=0.2).values()\n",
        "test_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Preview dataset\n",
        "print(\"Sample sentence:\")\n",
        "for data in test_dataset_raw:\n",
        "    print(data)\n",
        "    break\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset_raw.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset_raw.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset_raw.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTqlSrsePb0T"
      },
      "source": [
        "## Traditional Fine tuning\n",
        "In the traditional fine-tuning method, the entire model is trained, which is computationally expensive. An alternative approach is to fine-tune only certain layers of the model to reduce resource usage while still adapting the model to a specific task.\n",
        "\n",
        "To keep the implementation simple, you are assigned to train only the attention weights in the self-attention layers.\n",
        "\n",
        "The code below displays the names of all layers in the GPT-2 model. This will help you identify which layers to set as trainable or keep frozen. For more details on the attention layers in GPT-2, please refer to the following link: [GPT-2 Attention Layer Details](https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/gpt2/modeling_gpt2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MKM2IWJ4PBRc"
      },
      "outputs": [],
      "source": [
        "for name, module in model.named_modules():\n",
        "  print(name, type(module))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhxNv79ctMz"
      },
      "source": [
        "###TODO 1: Freeze the Model and Train Only Attention Weights\n",
        "You are assigned to freeze the entire model, except for the last two attention weights and the classification head. Note that, in this context, the attention weights do not include the projection layer of the transformer. Instead, they refer only to the weights of the query, key, and value.\n",
        "\n",
        "**HINT**: `c_proj` is projection layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds1GqFA4R5ok"
      },
      "outputs": [],
      "source": [
        "for n, p in model.named_parameters():\n",
        "    # TODO 1: freeze every layer except the last two attention weights and classification head.\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlV2DoY0f1sS"
      },
      "source": [
        "**Check Your Answer:** The number of learnable parameters is around 3545088."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaZ4l7Cmfo2f"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of Trainable Parameters:\", pytorch_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAbUMyXaiUjS"
      },
      "source": [
        "You are assigned to train the GPT-2 model on the SST-2 dataset. Due to the long training time, you will train the model for only 3 epochs. Your model should have around 86-88% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWOmFS6QgGaJ"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCFVbkuajL-g"
      },
      "source": [
        "As you can see, fine-tuning in the traditional way takes a long time to complete and also requires a high-computation GPU to fine-tune the entire model. Therefore, it is not feasible for most people.\n",
        "\n",
        "In the next part, we will introduce a better method: parameter-efficient learning, which requires lower computation. We will focus on the state-of-the-art method, Low-Rank Adaptation (LoRA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhCP0ZqrpeHO"
      },
      "source": [
        "## Low Rank Adaptation\n",
        "The concept of LoRA is that we are going to estimate the gradient (adaptation matrix) with two smaller matrices ($A$ and $B$):\n",
        "\n",
        "$$\n",
        "\\text{Adaptation Matrix} = B \\times A\n",
        "$$\n",
        "\n",
        "where $\\text{Adaptation Matrix} \\in \\mathbb{R}^{m \\times n}$, $A \\in \\mathbb{R}^{r \\times n}$, and $B \\in \\mathbb{R}^{m \\times r}$. We could make this approximation based on the assumption that $\\text{Adaptation Matrix}$ has a rank of $r$. Therefore, the fine-tuned weight becomes:\n",
        "\n",
        "$$\n",
        "W = W_0 + \\Delta W\n",
        "$$\n",
        "$$\n",
        "= W_0 + \\frac{\\alpha}{r} BA\n",
        "$$\n",
        "\n",
        "where $W$ denotes the fine-tuned weight, $W_0$ represents pre-trained weight, $\\Delta W$ is the gradient and $\\alpha$ can be seen as a learning rate. $A$ is initialized using a common initialization, like Kaiming initialization, during the initialization process. On the other hand, $B$ is set to 0 such that the model's output remains the same after injecting LoRA, resulting in a stabilized training process.\n",
        "\n",
        "To summarize, when injecting LoRA into a layer, we insert new parameters called matrix A and B and initialize them using the above description. Then, we modify the forward pass with `forward_hook` such that the output becomes:\n",
        "\n",
        "$$\n",
        "h = W x + \\frac{\\alpha}{r} BA x\n",
        "$$\n",
        "\n",
        "where $x$ and $h$ are the input and output, respectively. We recommend you read this [blog](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/#forward-hooks-101) to learn more about `forward_hook`.\n",
        "\n",
        "**LoRA on Linear Layer**\n",
        "\n",
        "- TODO 2: initialize A and B to ones (every entry in the matrix is one), such that we can verify your forward pass after attaching the hook.\n",
        "- TODO 3: implement the forward hook such that new output $h$ is\n",
        "\n",
        "$$\n",
        "h = W x + \\frac{\\alpha}{r} BA x\n",
        "$$\n",
        "\n",
        "**Hint**: When you want to declare and initialize a parameter, you can use `torch.nn.Parameter` and `torch.nn.init`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI1r0lZqs0A-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Initialize LoRA and attach a hook.\n",
        "def attach_lora(layer, r, lora_alpha, in_features, out_features):\n",
        "    assert r > 0, \"rank must greater than 0.\"\n",
        "    # TODO 2: Declare A and B matrices and initialize A and B to ones.\n",
        "    layer.lora_A = None\n",
        "    layer.lora_B = None\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        assert len(input) == 1, \"The length of the input must be 1.\"\n",
        "        # TODO 3: Compute adapatation matrix (BA) and modify the forward pass.\n",
        "        pass\n",
        "\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nl2dlr8ttef"
      },
      "source": [
        "To test your `forward_hook`, we will check the difference of the output before and after injecting the LoRA when you initialize matrices A and B with ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW8gI58KjLiw"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_utils import Conv1D\n",
        "# The Conv1D layer from the Transformer library is actually a linear layer. (https://github.com/huggingface/transformers/blob/main/src/transformers/pytorch_utils.py#L100)\n",
        "\n",
        "class DummyLinear(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = Conv1D(20, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "r, lora_alpha = 1, 4\n",
        "input_ = torch.arange(10, dtype=torch.float32).unsqueeze(0)\n",
        "dummy_linear = DummyLinear()\n",
        "output_before = dummy_linear(input_)\n",
        "for name, module in dummy_linear.named_modules():\n",
        "  if isinstance(module, Conv1D):\n",
        "    in_features, out_features = module.weight.shape\n",
        "    h = module.register_forward_hook(attach_lora(module, r, lora_alpha, in_features, out_features))\n",
        "output_after = dummy_linear(input_)\n",
        "\n",
        "if torch.all(torch.isclose(output_after - output_before, lora_alpha * input_.sum() * torch.ones_like(output_before))):\n",
        "  print(\"Your forward hook seems to be correct.\")\n",
        "else:\n",
        "  print(\"There is something wrong with your forward hook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_ucaXctylG"
      },
      "source": [
        "**Instruction**\n",
        "\n",
        "TODO 4: Change the initialization of A and B where A is initialized with Kaiming Uniform (a = sqrt(5)), and B is set to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqf4yyWtyY5"
      },
      "outputs": [],
      "source": [
        "# Initialize LoRA and attach a hook.\n",
        "def attach_lora(layer, r, lora_alpha, in_features, out_features):\n",
        "    assert r > 0, \"rank must greater than 0.\"\n",
        "    # TODO 4: initialize A with kaiming uniform with a = sqrt(5) and initialize B to 0.\n",
        "    layer.lora_A = None\n",
        "    layer.lora_B = None\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        assert len(input) == 1, \"The length of the input must be 1.\"\n",
        "        # Copy from TODO 3\n",
        "        pass\n",
        "\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EnhdRMjt_63"
      },
      "source": [
        "Similar to TODO 1, You are assigned to inject lora into the last two attention weights.\n",
        "\n",
        "TODO 5: inject lora into the last two attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOCO97bJhffz"
      },
      "outputs": [],
      "source": [
        "r, lora_alpha = 1, 4\n",
        "def attach_lora_to_maskformer(model, r, lora_alpha):\n",
        "  hooks = []\n",
        "  for name, module in model.named_modules():\n",
        "      # TODO 5: inject lora into the last two attention weights\n",
        "      pass\n",
        "  return hooks\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "hooks = attach_lora_to_maskformer(model, r, lora_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQdCaQLwqjS"
      },
      "source": [
        "###TODO 6: Freeze the Model and Train Only LoRA Weights\n",
        "You are assigned to freeze the entire model, except for the bias of the last two attention weights, LoRA weights, and the classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsxEx0fowBhm"
      },
      "outputs": [],
      "source": [
        "for n, p in model.named_parameters():\n",
        "    # TODO 6: freeze every layer except the bias of the last two attention weights, LoRA weights, and classification head.\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXsHsYzR7TVW"
      },
      "source": [
        "**Check Your Answer:** The number of learnable parameters is around 12288."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy_KtJTgyPul"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEObZRzFyWC8"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1O6Y8hzFLQ"
      },
      "source": [
        "# Part 2: PEFT Library\n",
        "\n",
        "In the first part, you learned how to implement LoRA from scratch. However, in real-world applications, we can simplify this process by using pre-built libraries. One such library is [`peft`](https://huggingface.co/docs/peft/main/en/quicktour), which allows us to inject LoRA into a model more efficiently. By declaring the injected modules in the LoRAConfig, we can easily integrate LoRA without having to implement it ourselves. In this section, you will use the `peft` library to apply LoRA to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETbzxntZ2p0L"
      },
      "source": [
        "## TODO 7-8: Initialize the LoRA Config and Set trainable parameters\n",
        "\n",
        "Your task is to initialize `LoRAConfig` using the same hyperparameters as in TODO 6 (`r=1`, `lora_alpha=4`). Apply LoRA only to the last two attention layers. Then, make sure to freeze the entire model except for the LoRA weights, the bias in the LoRA-injected layers, and the classification head. You only need to set the classification head to be trainable where the rest parameters are already set according to our `LoRAConfig`.  \n",
        "\n",
        "**HINT**: The total number of trainable parameters should match the result from Part 1 (TODO 6).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJGuWtHOyY0n"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# TODO 7: Initialize LoRAConfig\n",
        "lora_config = LoraConfig(\n",
        "    # Insert the parameters\n",
        "    pass\n",
        ")\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model = model.to(device)\n",
        "\n",
        "# TODO 8: Set classification head to trainable\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow2eIkHz4438"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSPiI4u45yJq"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRJ5pWrX6kKd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
