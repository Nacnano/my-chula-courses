{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Language Modeling using Ngram","metadata":{"collapsed":true,"id":"15QfB7RAuXAc","jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"In this Exercise, we are going to create a bigram language model and its variation. We will build one model for each of the following type and calculate their perplexity:\n- Unigram Model\n- Bigram Model\n- Bigram Model with Laplace smoothing\n- Bigram Model with Interpolation\n- Bigram Model with Kneser-ney Interpolation\n\nWe will also use NLTK which is a natural language processing library for python to make our lives easier.\n\n","metadata":{"id":"gucid6KNuXAe"}},{"cell_type":"code","source":"# #download corpus\n!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n!unzip BEST2010.zip","metadata":{"id":"MRRrn78ZjL54","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:04.427393Z","iopub.execute_input":"2025-01-17T09:32:04.427704Z","iopub.status.idle":"2025-01-17T09:32:05.988831Z","shell.execute_reply.started":"2025-01-17T09:32:04.427652Z","shell.execute_reply":"2025-01-17T09:32:05.987183Z"}},"outputs":[{"name":"stdout","text":"--2025-01-17 09:32:04--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n--2025-01-17 09:32:04--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7423530 (7.1M) [application/zip]\nSaving to: ‘BEST2010.zip’\n\nBEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.04s   \n\n2025-01-17 09:32:05 (170 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n\nArchive:  BEST2010.zip\n   creating: BEST2010/\n  inflating: BEST2010/article.txt    \n  inflating: BEST2010/encyclopedia.txt  \n  inflating: BEST2010/news.txt       \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv","metadata":{"id":"qeyvLSptdKXj","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:05.990531Z","iopub.execute_input":"2025-01-17T09:32:05.991006Z","iopub.status.idle":"2025-01-17T09:32:07.874132Z","shell.execute_reply.started":"2025-01-17T09:32:05.990964Z","shell.execute_reply":"2025-01-17T09:32:07.872700Z"}},"outputs":[{"name":"stdout","text":"--2025-01-17 09:32:06--  https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.67.18, 2620:100:6023:18::a27d:4312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.67.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01 [following]\n--2025-01-17 09:32:06--  https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com/cd/0/inline/CiWPCCup6N0bFzs6hu5--hIejjQ4UL1S_4YJgPPJV3GQ5kGXcP2CcK9BkMi3aSjjI-a2mRzGR4gH5ehZ-YMpxiaoSeAhrtrSrJo0-XL68abHLtgiQUJ-RMkbS9UL61XsuPw0qaR8F5t8oxEdykK7Lgkc/file# [following]\n--2025-01-17 09:32:06--  https://uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com/cd/0/inline/CiWPCCup6N0bFzs6hu5--hIejjQ4UL1S_4YJgPPJV3GQ5kGXcP2CcK9BkMi3aSjjI-a2mRzGR4gH5ehZ-YMpxiaoSeAhrtrSrJo0-XL68abHLtgiQUJ-RMkbS9UL61XsuPw0qaR8F5t8oxEdykK7Lgkc/file\nResolving uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com (uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com)... 162.125.67.15, 2620:100:6023:15::a27d:430f\nConnecting to uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com (uc78e8b5d5f1c591dc2f886cc1d2.dl.dropboxusercontent.com)|162.125.67.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4399477 (4.2M) [text/plain]\nSaving to: ‘tokenized_wiki_sample.csv’\n\ntokenized_wiki_samp 100%[===================>]   4.20M  16.3MB/s    in 0.3s    \n\n2025-01-17 09:32:07 (16.3 MB/s) - ‘tokenized_wiki_sample.csv’ saved [4399477/4399477]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#First we import necessary library such as math, nltk, bigram, and collections.\nimport math\nimport nltk\nimport io\nimport random\nfrom random import shuffle\nfrom nltk import bigrams, trigrams\nfrom collections import Counter, defaultdict\nrandom.seed(999)","metadata":{"id":"GjJDeG03uXAf","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:07.875481Z","iopub.execute_input":"2025-01-17T09:32:07.875910Z","iopub.status.idle":"2025-01-17T09:32:09.634463Z","shell.execute_reply.started":"2025-01-17T09:32:07.875852Z","shell.execute_reply":"2025-01-17T09:32:09.633311Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"BEST2010 is a free Thai NLP dataset by NECTEC usually used as a standard benchmark for various NLP tasks including language modeling. It is separated into 4 domains including article, encyclopedia, news, and novel. The data is already  tokenized using '|' as a separator.\n\nFor example,\n\nตาม|ที่|นางประนอม ทองจันทร์| |กับ| |ด.ช.กิตติพงษ์ แหลมผักแว่น| |และ| |ด.ญ.กาญจนา กรองแก้ว| |ป่วย|สงสัย|ติด|เชื้อ|ไข้|ขณะ|นี้|ยัง|ไม่|ดี|ขึ้น|","metadata":{"id":"HugXBHNEuXAh"}},{"cell_type":"code","source":"total_word_count = 0\nbest2010 = []\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n  for i,line in enumerate(f):\n    line=line.strip()[:-1] #remove the trailing |\n    total_word_count += len(line.split(\"|\"))\n    best2010.append(line)","metadata":{"id":"iu-AJSZZuXAi","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:09.637295Z","iopub.execute_input":"2025-01-17T09:32:09.637735Z","iopub.status.idle":"2025-01-17T09:32:09.899925Z","shell.execute_reply.started":"2025-01-17T09:32:09.637707Z","shell.execute_reply":"2025-01-17T09:32:09.898854Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#For simplicity, we assumes that each line is a sentence.\nprint (f'Total sentences in BEST2010 news dataset :\\t{len(best2010)}')\nprint (f'Total word counts in BEST2010 news dataset :\\t{total_word_count}')","metadata":{"id":"3WfpGgbruXAj","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:09.901927Z","iopub.execute_input":"2025-01-17T09:32:09.902235Z","iopub.status.idle":"2025-01-17T09:32:09.908084Z","shell.execute_reply.started":"2025-01-17T09:32:09.902210Z","shell.execute_reply":"2025-01-17T09:32:09.906787Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news dataset :\t30969\nTotal word counts in BEST2010 news dataset :\t1660190\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"We separate the input into 2 sets, train and test data with 70:30 ratio","metadata":{"id":"_JD9iXF1uXAm"}},{"cell_type":"code","source":"sentences = best2010\n# The data is separated to train and test set with 70:30 ratio.\ntrain = sentences[:int(len(sentences)*0.7)]\ntest = sentences[int(len(sentences)*0.7):]\n\n#Training data\ntrain_word_count =0\nfor line in train:\n    for word in line.split('|'):\n        train_word_count+=1\nprint ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\nprint ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))","metadata":{"id":"_WGcQq_juXAm","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:09.909643Z","iopub.execute_input":"2025-01-17T09:32:09.910131Z","iopub.status.idle":"2025-01-17T09:32:10.103165Z","shell.execute_reply.started":"2025-01-17T09:32:09.910090Z","shell.execute_reply":"2025-01-17T09:32:10.102105Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news training dataset :\t21678\nTotal word counts in BEST2010 news training dataset :\t1042797\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Here we load the data from Wikipedia which is also already tokenized. It will be used for answering questions in MyCourseville.","metadata":{"id":"17x6tW-3ae7Z"}},{"cell_type":"code","source":"import pandas as pd\nwiki_data = pd.read_csv(\"tokenized_wiki_sample.csv\")","metadata":{"id":"0fAl6dTg_9HG","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:10.104321Z","iopub.execute_input":"2025-01-17T09:32:10.104721Z","iopub.status.idle":"2025-01-17T09:32:10.579793Z","shell.execute_reply.started":"2025-01-17T09:32:10.104674Z","shell.execute_reply":"2025-01-17T09:32:10.578845Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Data Preprocessing\n\nBefore training any language models, the first step we always do is process the data into the format suited for the LM.\n\nFor this exercise, we will use NLTK to help process our data.","metadata":{"id":"H1W5bm-hbQXa"}},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:10.580799Z","iopub.execute_input":"2025-01-17T09:32:10.581220Z","iopub.status.idle":"2025-01-17T09:32:18.545705Z","shell.execute_reply.started":"2025-01-17T09:32:10.581181Z","shell.execute_reply":"2025-01-17T09:32:18.544465Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.9.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from nltk.lm.preprocessing import pad_both_ends, flatten\nfrom nltk.lm.vocabulary import Vocabulary\nfrom nltk import ngrams","metadata":{"id":"4OIqxJB7P29D","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:18.547090Z","iopub.execute_input":"2025-01-17T09:32:18.547519Z","iopub.status.idle":"2025-01-17T09:32:18.559329Z","shell.execute_reply.started":"2025-01-17T09:32:18.547477Z","shell.execute_reply":"2025-01-17T09:32:18.558410Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"We begin by \"tokenizing\" our training set. Note that the data is already tokenized so we can just split it.","metadata":{"id":"Oy0ZN2_0bzRr"}},{"cell_type":"code","source":"tokenized_train = [[\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in train] # \"tokenize\" and pad each sentence","metadata":{"id":"WQM0PXnXbzCN","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:18.560350Z","iopub.execute_input":"2025-01-17T09:32:18.560736Z","iopub.status.idle":"2025-01-17T09:32:18.903043Z","shell.execute_reply.started":"2025-01-17T09:32:18.560698Z","shell.execute_reply":"2025-01-17T09:32:18.901981Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Next we create a vocabulary with the ```Vocabulary``` class from NLTK. It accepts a list of tokens so we flatten our sentences into one long sentence first.\n\n\n\n\n\n","metadata":{"id":"TM2ylNRNcrg9"}},{"cell_type":"code","source":"flat_tokens = list(flatten(tokenized_train)) #join all sentences into one long sentence\nvocab = Vocabulary(flat_tokens, unk_cutoff=3) #Words with frequency **below** 3 (not exactly 3) will not be considered in our vocab and will be converted to <UNK>.","metadata":{"id":"Tbp-VmkHcq4d","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:18.903982Z","iopub.execute_input":"2025-01-17T09:32:18.904271Z","iopub.status.idle":"2025-01-17T09:32:19.105237Z","shell.execute_reply.started":"2025-01-17T09:32:18.904247Z","shell.execute_reply":"2025-01-17T09:32:19.104323Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Then we replace low frequency words.\n\nNow *each* sentence is going to look something like this:\n\\[\"\\<s\\>\", \"hello\", \"my\", \"name\", \"is\", \"\\<UNK\\>\", \"\\</s\\>\" \\]","metadata":{"id":"oFnBHe6ScAaV"}},{"cell_type":"code","source":"tokenized_train = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_train]","metadata":{"id":"9q6QakuibxqN","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:19.106492Z","iopub.execute_input":"2025-01-17T09:32:19.106772Z","iopub.status.idle":"2025-01-17T09:32:19.775743Z","shell.execute_reply.started":"2025-01-17T09:32:19.106749Z","shell.execute_reply":"2025-01-17T09:32:19.774755Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Finally, we do the same for the test set and the wiki dataset.","metadata":{"id":"Dn6GxaSFeSpD"}},{"cell_type":"code","source":"tokenized_test = [t.split(\"|\") for t in test]\ntokenized_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_test]\n\ntokenized_wiki_test = [t.split(\"|\") for t in wiki_data['tokenized'].tolist()]\ntokenized_wiki_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_wiki_test]","metadata":{"id":"D4N6qKrPadIj","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:19.779348Z","iopub.execute_input":"2025-01-17T09:32:19.779685Z","iopub.status.idle":"2025-01-17T09:32:20.418204Z","shell.execute_reply.started":"2025-01-17T09:32:19.779656Z","shell.execute_reply":"2025-01-17T09:32:20.417044Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Unigram","metadata":{"id":"pHtCMFMluXAo"}},{"cell_type":"markdown","source":"In this section, we will demonstrate how to build a unigram language model <br>\n**Important note:** <br>\n**\\<s\\>** = sentence start symbol <br>\n**\\</s\\>** = sentence end symbol","metadata":{"id":"2V1WQTGzuXAp"}},{"cell_type":"markdown","source":"# VERY IMPORTANT:\n- In this notebook, we will *not* default the unknown token probability to ```1/len(vocab)``` but instead will treat it as a normal word and let the model learn its probability so that we can compare our results to NLTK.\n- **Also make sure that the code in this notebook can be executed without any problem. If we find that you used NLTK to answer questions in MyCourseVille and did not finish the assignment, you will receive a grade of 0 for this assignment.**","metadata":{"id":"Xd7qOd7KAYWM"}},{"cell_type":"code","source":"class UnigramModel():\n  def __init__(self, data, vocab):\n    self.unigram_count = defaultdict(lambda: 0.0)\n    self.word_count = 0\n    self.vocab = vocab\n    for sentence in data:\n        for w in sentence: #[(word1, ), (word2, ), (word3, )...]\n          w = w[0]\n          self.unigram_count[w] +=1.0\n          self.word_count+=1\n\n  def __getitem__(self, w):\n    w = w[0]  #[(word1, ), (word2, ), (word3, )...]\n    if w in self.vocab:\n      return self.unigram_count[w]/(self.word_count)\n    else:\n      return self.unigram_count[\"<UNK>\"]/(self.word_count)","metadata":{"id":"CTV-i9kdse58","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:20.420457Z","iopub.execute_input":"2025-01-17T09:32:20.420758Z","iopub.status.idle":"2025-01-17T09:32:20.427567Z","shell.execute_reply.started":"2025-01-17T09:32:20.420733Z","shell.execute_reply":"2025-01-17T09:32:20.426372Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_train] #creating the unigrams by setting n=1\nmodel = UnigramModel(train_unigrams, vocab)","metadata":{"id":"FnWJJ8Hqs8Qs","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:20.429056Z","iopub.execute_input":"2025-01-17T09:32:20.429465Z","iopub.status.idle":"2025-01-17T09:32:21.285075Z","shell.execute_reply.started":"2025-01-17T09:32:20.429425Z","shell.execute_reply":"2025-01-17T09:32:21.284177Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def getLnValue(x):\n      if x == 0:\n        return -math.inf\n      return math.log(x)","metadata":{"id":"6coGxSY7uXAt","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:21.286165Z","iopub.execute_input":"2025-01-17T09:32:21.286556Z","iopub.status.idle":"2025-01-17T09:32:21.291744Z","shell.execute_reply.started":"2025-01-17T09:32:21.286518Z","shell.execute_reply":"2025-01-17T09:32:21.290702Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#problability of 'นายก'\nprint(getLnValue(model[('นายก',)]))\n\n#for example, problability of 'นายกรัฐมนตรี' which is an unknown word is equal to\nprint(getLnValue(model[('นายกรัฐมนตรี',)]))\n\n#problability of 'นายก' 'ได้' 'ให้' 'สัมภาษณ์' 'กับ' 'สื่อ'\nprob = getLnValue(model[('นายก',)])+getLnValue(model[('ได้',)])+ getLnValue(model[('ให้',)])+getLnValue(model[('สัมภาษณ์',)])+getLnValue(model[('กับ',)])+getLnValue(model[('สื่อ',)])+getLnValue(model[('</s>',)])\nprint ('Problability of a sentence', math.exp(prob))","metadata":{"id":"cFy8yhZjuXAv","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:32:21.292824Z","iopub.execute_input":"2025-01-17T09:32:21.293209Z","iopub.status.idle":"2025-01-17T09:32:21.315782Z","shell.execute_reply.started":"2025-01-17T09:32:21.293182Z","shell.execute_reply":"2025-01-17T09:32:21.314512Z"}},"outputs":[{"name":"stdout","text":"-6.571687039690381\n-3.952132570275872\nProblability of a sentence 4.877889285183675e-18\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Perplexity\n\nIn order to compare language model we need to calculate perplexity. In this task you should write a perplexity calculation code for the unigram model. The result perplexity should be around 448.90 and\n392.74 on train and test data.","metadata":{"id":"D8EfqnDsuXAw"}},{"cell_type":"markdown","source":"## TODO #1 Calculate perplexity","metadata":{"id":"hZHQ-6tVuXAx"}},{"cell_type":"code","source":"def getLnValue(x):\n    return math.log(x)\n\ndef calculate_sentence_ln_prob(sentence, model):\n    ret = 0\n    for word in sentence:\n        ret += getLnValue(model[word])\n    return ret\n\ndef perplexity(test,model):\n    length = 0\n    prob = 0\n    for sentence in test:\n        prob += calculate_sentence_ln_prob(sentence, model)\n        length += len(sentence)\n        \n    return math.exp(-prob/length)","metadata":{"id":"kh0DwzoouXAx","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:34:45.883517Z","iopub.execute_input":"2025-01-17T09:34:45.883889Z","iopub.status.idle":"2025-01-17T09:34:45.890158Z","shell.execute_reply.started":"2025-01-17T09:34:45.883864Z","shell.execute_reply":"2025-01-17T09:34:45.889007Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"test_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_test]","metadata":{"id":"X-t_8mEzRxT-","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:34:49.395504Z","iopub.execute_input":"2025-01-17T09:34:49.395866Z","iopub.status.idle":"2025-01-17T09:34:49.607092Z","shell.execute_reply.started":"2025-01-17T09:34:49.395837Z","shell.execute_reply":"2025-01-17T09:34:49.605948Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(perplexity(train_unigrams,model))\nprint(perplexity(test_unigrams,model))","metadata":{"id":"PztVYprdtBja","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:34:50.487597Z","iopub.execute_input":"2025-01-17T09:34:50.487971Z","iopub.status.idle":"2025-01-17T09:34:52.212381Z","shell.execute_reply.started":"2025-01-17T09:34:50.487937Z","shell.execute_reply":"2025-01-17T09:34:52.211249Z"}},"outputs":[{"name":"stdout","text":"448.89690751824827\n392.74028966757214\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Q1 MCV\nCalculate the perplexity of the model on the wiki test set and answer in MyCourseVille","metadata":{"id":"PHnBXtt3b-OY"}},{"cell_type":"code","source":"wiki_test_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_wiki_test]","metadata":{"id":"JRd6hF_WSBl_","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:35:02.389090Z","iopub.execute_input":"2025-01-17T09:35:02.389430Z","iopub.status.idle":"2025-01-17T09:35:02.448995Z","shell.execute_reply.started":"2025-01-17T09:35:02.389397Z","shell.execute_reply":"2025-01-17T09:35:02.447777Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_unigrams))], model))","metadata":{"id":"I_LiSohADNLC","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:35:04.216165Z","iopub.execute_input":"2025-01-17T09:35:04.216695Z","iopub.status.idle":"2025-01-17T09:35:04.459099Z","shell.execute_reply.started":"2025-01-17T09:35:04.216654Z","shell.execute_reply":"2025-01-17T09:35:04.457955Z"}},"outputs":[{"name":"stdout","text":"485.7336366066887\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Bigram","metadata":{"id":"lK0gaMf0uXA2"}},{"cell_type":"markdown","source":"Next, you will create a better language model than a unigram (which is not much to compare with). But first, it is very tedious to count every pair of words that occur in our corpus by ourselves. Lucky for us, nltk provides us a simple library which will simplify the process.","metadata":{"id":"dmTkAY_QuXA3"}},{"cell_type":"code","source":"#example of nltk usage for bigram\nsentence = 'I always search google for an answer .'\npadded_sentence = list(pad_both_ends(sentence.split(), n=2))\n\nprint('This is how nltk generate bigram.')\nfor w1,w2 in bigrams(padded_sentence):\n    print(w1,w2)\nprint('\\n<s> and </s> are used as a start and end of sentence symbol. respectively.')","metadata":{"id":"Lv6r2LJ1uXA4","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:35:12.108707Z","iopub.execute_input":"2025-01-17T09:35:12.109104Z","iopub.status.idle":"2025-01-17T09:35:12.118603Z","shell.execute_reply.started":"2025-01-17T09:35:12.109075Z","shell.execute_reply":"2025-01-17T09:35:12.117584Z"}},"outputs":[{"name":"stdout","text":"This is how nltk generate bigram.\n<s> I\nI always\nalways search\nsearch google\ngoogle for\nfor an\nan answer\nanswer .\n. </s>\n\n<s> and </s> are used as a start and end of sentence symbol. respectively.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"Now, you should be able to implement a bigram model by yourself. Also, you must create a new perplexity calculation for bigram. The result perplexity should be around 56.46 and inf on train and test data.","metadata":{"id":"5R2T-6i9uXA6"}},{"cell_type":"markdown","source":"## TODO #3 Write Bigram Model","metadata":{"id":"9aYkjzTzuXA7"}},{"cell_type":"code","source":"from collections import defaultdict\n\nclass BigramModel:\n    def __init__(self, data, vocab):\n        self.unigram_count = defaultdict(float) \n        self.bigram_count = defaultdict(float)  \n        self.vocab = vocab\n        \n        for sentence in data:\n            for w1, w2 in sentence:\n                self.bigram_count[(w1, w2)] += 1.0\n                self.unigram_count[w1] += 1.0\n\n            # Include the last word in the unigram count\n            last_word = sentence[-1][-1]\n            self.unigram_count[last_word] += 1.0\n\n    def __getitem__(self, bigram):\n        w1, w2 = bigram\n        bigram_count = self.bigram_count[(w1, w2)]\n        unigram_count = self.unigram_count[w1]\n\n        return bigram_count / unigram_count if unigram_count > 0 else 0.0\n","metadata":{"id":"l4s7oSmjkNuU","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:42:02.939565Z","iopub.execute_input":"2025-01-17T09:42:02.939974Z","iopub.status.idle":"2025-01-17T09:42:02.947464Z","shell.execute_reply.started":"2025-01-17T09:42:02.939948Z","shell.execute_reply":"2025-01-17T09:42:02.945765Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## TODO #4 Write Perplexity for Bigram Model\n\nSum perplexity score at a sentence level, instead of word level","metadata":{"id":"i3_Cgu6guXA-"}},{"cell_type":"code","source":"def getLnValueBigram(x):\n    return math.log(x)\n\ndef calculate_sentence_ln_prob(sentence, model):\n    ret = 0\n    for bigram in sentence:\n        ret += getLnValueBigram(model[bigram]) if model[bigram] != 0 else -math.inf\n    return ret\n\ndef perplexity(test,model):\n    length = 0\n    prob = 0\n    for sentence in test:\n        prob += calculate_sentence_ln_prob(sentence, model)\n        length += len(sentence)\n        \n    return math.exp(-prob/length)","metadata":{"id":"hICoAhZjAxo1","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:43:55.468080Z","iopub.execute_input":"2025-01-17T09:43:55.468532Z","iopub.status.idle":"2025-01-17T09:43:55.475607Z","shell.execute_reply.started":"2025-01-17T09:43:55.468502Z","shell.execute_reply":"2025-01-17T09:43:55.474483Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"train_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_train]\ntest_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_test]","metadata":{"id":"NxJYI3_TS2gf","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:43:57.777730Z","iopub.execute_input":"2025-01-17T09:43:57.778085Z","iopub.status.idle":"2025-01-17T09:43:58.584241Z","shell.execute_reply.started":"2025-01-17T09:43:57.778059Z","shell.execute_reply":"2025-01-17T09:43:58.583139Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"bigram_model_scratch = BigramModel(train_bigrams, vocab)","metadata":{"id":"A4DD_RPFtxUo","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:43:59.964232Z","iopub.execute_input":"2025-01-17T09:43:59.964591Z","iopub.status.idle":"2025-01-17T09:44:00.939009Z","shell.execute_reply.started":"2025-01-17T09:43:59.964559Z","shell.execute_reply":"2025-01-17T09:44:00.938119Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"print(perplexity([list(flatten(train_bigrams))], bigram_model_scratch))\nprint(perplexity([list(flatten(test_bigrams))[:16]], bigram_model_scratch)) #can be used to compare with nltk\nprint(perplexity([list(flatten(test_bigrams))], bigram_model_scratch))","metadata":{"id":"yw4BubpbtuV7","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:44:02.173696Z","iopub.execute_input":"2025-01-17T09:44:02.174101Z","iopub.status.idle":"2025-01-17T09:44:04.882749Z","shell.execute_reply.started":"2025-01-17T09:44:02.174067Z","shell.execute_reply":"2025-01-17T09:44:04.881625Z"}},"outputs":[{"name":"stdout","text":"56.45504870219316\n24.598691603876187\ninf\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## Q2 MCV","metadata":{"id":"PRv294uQcZFC"}},{"cell_type":"code","source":"wiki_test_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_wiki_test]","metadata":{"id":"kCeRCyOIUWTS","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:44:35.365280Z","iopub.execute_input":"2025-01-17T09:44:35.365659Z","iopub.status.idle":"2025-01-17T09:44:35.451250Z","shell.execute_reply.started":"2025-01-17T09:44:35.365629Z","shell.execute_reply":"2025-01-17T09:44:35.450319Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],bigram_model_scratch))","metadata":{"id":"q47hutRqIg1z","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:44:39.871709Z","iopub.execute_input":"2025-01-17T09:44:39.872281Z","iopub.status.idle":"2025-01-17T09:44:40.280175Z","shell.execute_reply.started":"2025-01-17T09:44:39.872229Z","shell.execute_reply":"2025-01-17T09:44:40.279129Z"}},"outputs":[{"name":"stdout","text":"inf\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# Smoothing","metadata":{"id":"9BAF9DQbuXBC"}},{"cell_type":"markdown","source":"Usually any ngram models have a sparsity problem, which means it does not have every possible ngram of words in the dataset. Smoothing techniques can alleviate this problem. In this section, you will implement three basic smoothing methods laplace smoothing, interpolation for bigram, and Knesey-Ney smoothing.","metadata":{"id":"jlm75BWLuXBC"}},{"cell_type":"markdown","source":"## TODO #5 write Bigram with Laplace smoothing (Add-One Smoothing)\n\nThe result perplexity on training and testing should be:\n\n    370.28, 369.16 for Laplace smoothing","metadata":{"id":"jwa7YQiouXBD"}},{"cell_type":"code","source":"class BigramWithLaplaceSmoothing:\n    def __init__(self, data, vocab):\n        self.unigram_count = defaultdict(lambda: 0.0)\n        self.bigram_count = defaultdict(lambda: 0.0)\n        self.vocab = vocab\n\n        for sentence in data:\n            for w1, w2 in sentence:\n                self.bigram_count[(w1, w2)] += 1.0\n                self.unigram_count[w1] += 1.0\n\n            self.unigram_count[sentence[-1][-1]] += 1.0\n\n    def __getitem__(self, bigram):\n        w1, w2 = bigram\n\n        bigram_count = self.bigram_count[(w1, w2)]\n        unigram_count = self.unigram_count[w1]\n        \n        return (bigram_count+1) / (unigram_count + len(self.vocab))\n\nmodel = BigramWithLaplaceSmoothing(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"j2Bw4C9T_UEs","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:46:58.457662Z","iopub.execute_input":"2025-01-17T09:46:58.458048Z","iopub.status.idle":"2025-01-17T09:47:02.782108Z","shell.execute_reply.started":"2025-01-17T09:46:58.458020Z","shell.execute_reply":"2025-01-17T09:47:02.780626Z"}},"outputs":[{"name":"stdout","text":"370.28232024056035\n369.1605485652555\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## Q3 MCV","metadata":{"id":"mFT4uhIGhP0c"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"jSH60cshIpDy","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T09:47:11.621185Z","iopub.execute_input":"2025-01-17T09:47:11.621526Z","iopub.status.idle":"2025-01-17T09:47:12.071565Z","shell.execute_reply.started":"2025-01-17T09:47:11.621501Z","shell.execute_reply":"2025-01-17T09:47:12.070442Z"}},"outputs":[{"name":"stdout","text":"735.0253999215859\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## TODO #6 Write Bigram with Interpolation\nSet the lambda value as 0.7 for bigram, 0.25 for unigram, and 0.05 for unknown word.\n\nThe result perplexity on training and testing should be:\n\n    70.07, 102.99 for Interpolation","metadata":{"id":"5JDswBSIuXBG"}},{"cell_type":"code","source":"class BigramWithInterpolation:\n    def __init__(self, data, vocab, l=0.7):\n        self.unigram_count = defaultdict(lambda: 0.0)\n        self.bigram_count = defaultdict(lambda: 0.0)\n        self.total_word_count = 0\n        self.vocab = vocab\n        self.l = l\n        \n        for sentence in data:\n            for w1, w2 in sentence:\n                self.bigram_count[(w1, w2)] += 1.0\n                self.unigram_count[w1] += 1.0\n                self.total_word_count += 1\n\n            self.unigram_count[w2] += 1.0\n            self.total_word_count += 1\n\n    def __getitem__(self, bigram):\n        w1, w2 = bigram\n        unigram_prob = self.unigram_count[w2] / self.total_word_count\n        bigram_prob = self.bigram_count[(w1, w2)] / self.unigram_count[w1]\n\n        return 0.7 * bigram_prob + 0.25 * unigram_prob + 0.05 * (1 / len(self.vocab))\n\nmodel = BigramWithInterpolation(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"PIeDBLarvZUT","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:27.394502Z","iopub.execute_input":"2025-01-17T10:11:27.394959Z","iopub.status.idle":"2025-01-17T10:11:32.499208Z","shell.execute_reply.started":"2025-01-17T10:11:27.394916Z","shell.execute_reply":"2025-01-17T10:11:32.497584Z"}},"outputs":[{"name":"stdout","text":"70.06754793707988\n102.99295310997627\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## Q4 MCV","metadata":{"id":"i-GlmJUIhN7s"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"EilXywU-IuNU","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:37.521635Z","iopub.execute_input":"2025-01-17T10:11:37.522028Z","iopub.status.idle":"2025-01-17T10:11:38.054426Z","shell.execute_reply.started":"2025-01-17T10:11:37.521999Z","shell.execute_reply":"2025-01-17T10:11:38.052268Z"}},"outputs":[{"name":"stdout","text":"251.60709095654997\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## Language modeling on multiple domains\n\nSometimes, we do not have enough data to create a language model for a new domain. In that case, we can improvised by combining several models to improve result on the new domain.\n\nIn this exercise you will try to merge two language models from news and article domains to create a language model for the encyclopedia domain.","metadata":{"id":"pUorP-EWuXBM"}},{"cell_type":"code","source":"def get_bigram_data(data, vocab):\n    tokenized_sentences = [sentence.split(\"|\") for sentence in data]\n    \n    processed_sentences = [\n        [token if token in vocab else \"<UNK>\" for token in sentence]\n        for sentence in tokenized_sentences\n    ]\n    \n    padded_sentences = [\n        list(pad_both_ends(sentence, n=2)) for sentence in processed_sentences\n    ]\n    \n    bigrams = [list(ngrams(sentence, n=2)) for sentence in padded_sentences]\n\n    return bigrams","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:50.478607Z","iopub.execute_input":"2025-01-17T10:11:50.478985Z","iopub.status.idle":"2025-01-17T10:11:50.484685Z","shell.execute_reply.started":"2025-01-17T10:11:50.478956Z","shell.execute_reply":"2025-01-17T10:11:50.483499Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# create encyclopeida data (test data)\nencyclo_data=[]\nwith open('BEST2010/encyclopedia.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        encyclo_data.append(line.strip()[:-1])","metadata":{"id":"Jel9Hx69uXBN","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:55.144863Z","iopub.execute_input":"2025-01-17T10:11:55.145300Z","iopub.status.idle":"2025-01-17T10:11:55.278048Z","shell.execute_reply.started":"2025-01-17T10:11:55.145261Z","shell.execute_reply":"2025-01-17T10:11:55.277006Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"(news) First, you should try to calculate perplexity of your bigram with interpolation on encyclopedia data. The  perplexity should be around 236.33","metadata":{"id":"Jlla-S8YYRur"}},{"cell_type":"code","source":"encyclopedia_bigrams = get_bigram_data(encyclo_data, vocab)","metadata":{"id":"gkRm8W4UWyfc","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:57.834038Z","iopub.execute_input":"2025-01-17T10:11:57.834452Z","iopub.status.idle":"2025-01-17T10:12:00.449881Z","shell.execute_reply.started":"2025-01-17T10:11:57.834420Z","shell.execute_reply":"2025-01-17T10:12:00.448760Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# 1) news only on \"encyclopedia\"\nprint(perplexity([list(flatten(encyclopedia_bigrams))], model))","metadata":{"id":"x0l91qLEuXBP","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:12:02.094588Z","iopub.execute_input":"2025-01-17T10:12:02.094987Z","iopub.status.idle":"2025-01-17T10:12:05.080474Z","shell.execute_reply.started":"2025-01-17T10:12:02.094957Z","shell.execute_reply":"2025-01-17T10:12:05.079400Z"}},"outputs":[{"name":"stdout","text":"236.3286142031601\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"## TODO #7 - Langauge Modelling on Multiple Domains\nCombine news and article datasets to create another bigram model and evaluate it on the encyclopedia data.","metadata":{"id":"hwV9j9U-uXBR"}},{"cell_type":"markdown","source":"\n\n(article) For your information, a bigram model with interpolation using article data to test on encyclopedia data has a perplexity of 218.57","metadata":{"id":"9skdgo8muXBO"}},{"cell_type":"code","source":"def get_vocab(data):\n    tokenized_data = [[\"<s>\"] + sentence.split(\"|\") + [\"</s>\"] for sentence in data]\n    \n    flat_tokens = list(flatten(tokenized_data))\n    \n    vocab = Vocabulary(flat_tokens, unk_cutoff=3)\n\n    return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:12:07.672187Z","iopub.execute_input":"2025-01-17T10:12:07.672576Z","iopub.status.idle":"2025-01-17T10:12:07.678058Z","shell.execute_reply.started":"2025-01-17T10:12:07.672546Z","shell.execute_reply":"2025-01-17T10:12:07.676791Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# 2) article only on \"encyclopedia\"\nbest2010_article=[]\nwith open('BEST2010/article.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        best2010_article.append(line.strip()[:-1])\n\ncombined_total_word_count = 0\nfor line in best2010_article:\n    combined_total_word_count += len(line.split('|'))\n\narticle_vocab = get_vocab(best2010_article)\narticle_bigrams =  get_bigram_data(best2010_article, article_vocab)\n\nmodel = BigramWithInterpolation(article_bigrams, article_vocab)","metadata":{"id":"LOA8fd53uXBU","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:12:11.965384Z","iopub.execute_input":"2025-01-17T10:12:11.965745Z","iopub.status.idle":"2025-01-17T10:12:15.813974Z","shell.execute_reply.started":"2025-01-17T10:12:11.965715Z","shell.execute_reply":"2025-01-17T10:12:15.812977Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"encyclopedia_bigrams_article_vocab = get_bigram_data(encyclo_data, article_vocab)\n\nprint('Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams_article_vocab))], model))","metadata":{"id":"7bLYcPvXYHkB","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:14:09.521194Z","iopub.execute_input":"2025-01-17T10:14:09.521577Z","iopub.status.idle":"2025-01-17T10:14:14.538879Z","shell.execute_reply.started":"2025-01-17T10:14:09.521552Z","shell.execute_reply":"2025-01-17T10:14:14.537840Z"}},"outputs":[{"name":"stdout","text":"Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data 218.57479345888848\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"# 3) train on news + article, test on \"encyclopedia\"\nbest2010_article_and_news = best2010_article.copy()\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        best2010_article_and_news.append(line.strip()[:-1])\n\ncombined_vocab = get_vocab(best2010_article_and_news)\ncombined_bigrams = get_bigram_data(best2010_article_and_news, combined_vocab)\n\nencyclopedia_bigrams_combined_vocab = get_bigram_data(encyclo_data, combined_vocab)\n\ncombined_model = BigramWithInterpolation(combined_bigrams, combined_vocab)\nprint('Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams_combined_vocab))], combined_model))","metadata":{"id":"wBjmLhUcuXBS","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:15:41.559672Z","iopub.execute_input":"2025-01-17T10:15:41.560018Z","iopub.status.idle":"2025-01-17T10:15:55.767190Z","shell.execute_reply.started":"2025-01-17T10:15:41.559992Z","shell.execute_reply":"2025-01-17T10:15:55.765991Z"}},"outputs":[{"name":"stdout","text":"Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data 242.88025282580364\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"## TODO #8 - Kneser-ney on \"News\"\n\n<!-- Reimplement equation 4.33 in SLP textbook (https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) -->\n\nImplement Bigram Knerser-ney LM. The result perplexity should be around 65.81, 93.21 on train and test data. Be careful not to mix up vocab from the above section!\n","metadata":{"id":"XNPEhD7WuXBV"}},{"cell_type":"code","source":"from collections import defaultdict\n\nclass BigramKneserNey:\n    def __init__(self, data, vocab, d=0.75):\n        self.unigram_count = defaultdict(float)\n        self.bigram_count = defaultdict(float)\n        self.start_with_unique_count = defaultdict(set)\n        self.end_with_unique_count = defaultdict(set)\n        \n        self.vocab = vocab\n        self.d = d\n\n        for sentence in data:\n            for w1, w2 in sentence:\n                self.bigram_count[(w1, w2)] += 1.0\n                self.unigram_count[w1] += 1.0\n                self.start_with_unique_count[w1].add(w2)\n                self.end_with_unique_count[w2].add(w1)\n\n            self.unigram_count[w2] += 1.0\n\n        self.bigram_size = len(self.bigram_count)\n\n    def __getitem__(self, bigram):\n        w1, w2 = bigram\n        \n        bigram_count = self.bigram_count[(w1, w2)]\n        unigram_count = self.unigram_count[w1]\n        unique_continuations = len(self.start_with_unique_count[w1])\n        unique_preceding = len(self.end_with_unique_count[w2])\n\n        discounted_prob = max(bigram_count - self.d, 0) / unigram_count\n        \n        backoff_weight = (self.d / unigram_count) * unique_continuations\n        \n        continuation_prob = unique_preceding / self.bigram_size\n\n        return discounted_prob + backoff_weight * continuation_prob\n\n\nmodel = BigramKneserNey(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(train_bigrams))[:1000]],model)) #can be used to compare with nltk\nprint(perplexity([list(flatten(test_bigrams))[:1000]], model)) #can be used to compare with nltk\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"Y_8xFf7tBqpc","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:16:54.054797Z","iopub.execute_input":"2025-01-17T10:16:54.055250Z","iopub.status.idle":"2025-01-17T10:17:00.650134Z","shell.execute_reply.started":"2025-01-17T10:16:54.055221Z","shell.execute_reply":"2025-01-17T10:17:00.648541Z"}},"outputs":[{"name":"stdout","text":"65.80901349887752\n50.40541747450013\n90.29909158711666\n93.21479572741401\n","output_type":"stream"}],"execution_count":77},{"cell_type":"markdown","source":"## Q5 MCV","metadata":{"id":"ULDScRw-g8Yn"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"eSZ1Pb9WvfWC","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:17:07.034860Z","iopub.execute_input":"2025-01-17T10:17:07.035241Z","iopub.status.idle":"2025-01-17T10:17:07.724286Z","shell.execute_reply.started":"2025-01-17T10:17:07.035212Z","shell.execute_reply":"2025-01-17T10:17:07.723057Z"}},"outputs":[{"name":"stdout","text":"266.582022413098\n","output_type":"stream"}],"execution_count":78}]}