{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subword Tokenization\n\nIn this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n\n## Ref:\nhttps://github.com/google/sentencepiece/blob/master/python","metadata":{"id":"iU5fRQwhEdJy"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"pI9gRZlUE80g"}},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:20.066631Z","iopub.execute_input":"2025-01-19T14:24:20.066926Z","iopub.status.idle":"2025-01-19T14:24:25.355873Z","shell.execute_reply.started":"2025-01-19T14:24:20.066903Z","shell.execute_reply":"2025-01-19T14:24:25.354977Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.10)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.5.0.post0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl","metadata":{"id":"1pOsV-jaW975","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:25.356932Z","iopub.execute_input":"2025-01-19T14:24:25.357136Z","iopub.status.idle":"2025-01-19T14:24:27.836000Z","shell.execute_reply.started":"2025-01-19T14:24:25.357117Z","shell.execute_reply":"2025-01-19T14:24:27.834863Z"}},"outputs":[{"name":"stdout","text":"--2025-01-19 14:24:25--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n--2025-01-19 14:24:26--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3231076 (3.1M) [application/octet-stream]\nSaving to: ‘pra-apai-manee-ch1-50.txt’\n\npra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.07s   \n\n2025-01-19 14:24:26 (47.2 MB/s) - ‘pra-apai-manee-ch1-50.txt’ saved [3231076/3231076]\n\n--2025-01-19 14:24:26--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl [following]\n--2025-01-19 14:24:27--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2968483 (2.8M) [text/plain]\nSaving to: ‘kratoo-40000000-40002000.jsonl’\n\nkratoo-40000000-400 100%[===================>]   2.83M  --.-KB/s    in 0.07s   \n\n2025-01-19 14:24:27 (43.1 MB/s) - ‘kratoo-40000000-40002000.jsonl’ saved [2968483/2968483]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Code","metadata":{"id":"CSiDpG9WE-cT"}},{"cell_type":"code","source":"import sentencepiece as spm\nimport io\nimport json","metadata":{"id":"OQd7M6gLWPLN","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.837902Z","iopub.execute_input":"2025-01-19T14:24:27.838266Z","iopub.status.idle":"2025-01-19T14:24:27.871133Z","shell.execute_reply.started":"2025-01-19T14:24:27.838230Z","shell.execute_reply":"2025-01-19T14:24:27.870192Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Load data","metadata":{"id":"OifbmMIstzs8"}},{"cell_type":"code","source":"pantip_text = []\nwith open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n    json_list = list(json_file)\n    for json_str in json_list:\n        result = json.loads(json_str)\n        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\nsum([len(t) for t in pantip_text])","metadata":{"id":"-FnIDvb1lMuh","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.872633Z","iopub.execute_input":"2025-01-19T14:24:27.872938Z","iopub.status.idle":"2025-01-19T14:24:27.910944Z","shell.execute_reply.started":"2025-01-19T14:24:27.872907Z","shell.execute_reply":"2025-01-19T14:24:27.910093Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"1060318"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"with open(\"pra-apai-manee-ch1-50.txt\") as f:\n  pra_apai_manee_data = f.readlines()","metadata":{"id":"yaaQVXZ8A0j1","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.911870Z","iopub.execute_input":"2025-01-19T14:24:27.912178Z","iopub.status.idle":"2025-01-19T14:24:27.932403Z","shell.execute_reply.started":"2025-01-19T14:24:27.912148Z","shell.execute_reply":"2025-01-19T14:24:27.931781Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"sum([len(t) for t in pra_apai_manee_data])","metadata":{"id":"LksJKc9MA5F_","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.933518Z","iopub.execute_input":"2025-01-19T14:24:27.933837Z","iopub.status.idle":"2025-01-19T14:24:27.942647Z","shell.execute_reply.started":"2025-01-19T14:24:27.933805Z","shell.execute_reply":"2025-01-19T14:24:27.941923Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1100605"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\npantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n\npam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\npam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]","metadata":{"id":"RbdfkF-vAoie","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.943429Z","iopub.execute_input":"2025-01-19T14:24:27.943725Z","iopub.status.idle":"2025-01-19T14:24:27.955607Z","shell.execute_reply.started":"2025-01-19T14:24:27.943695Z","shell.execute_reply":"2025-01-19T14:24:27.954924Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Run tokenizer training\n\nThe Python wrapper provides multiple APIs for training our tokenizers\n\n1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n  <br><br>\n2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n<br><br>\n3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n<br> Same as no.1\n\n\n","metadata":{"id":"BhwcH0Aot1XI"}},{"cell_type":"markdown","source":"### Unigram tokenizer\n\nWe are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000.","metadata":{"id":"c3XeFFYw-T_0"}},{"cell_type":"code","source":"## Train\nmodel_pam_uni = io.BytesIO()\nspm.SentencePieceTrainer.train(sentence_iterator=iter(pam_train_text), model_writer=model_pam_uni, vocab_size=1000)","metadata":{"id":"bFCfHphd15g9","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:27.957976Z","iopub.execute_input":"2025-01-19T14:24:27.958181Z","iopub.status.idle":"2025-01-19T14:24:39.253927Z","shell.execute_reply.started":"2025-01-19T14:24:27.958164Z","shell.execute_reply":"2025-01-19T14:24:39.252821Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Q1 MCV\n\nHow many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'","metadata":{"id":"gdXPaoW3_v2T"}},{"cell_type":"code","source":"sp_pam = spm.SentencePieceProcessor(model_proto=model_pam_uni.getvalue())\nlen(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))","metadata":{"id":"J1bO3s-z-PLb","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:39.256050Z","iopub.execute_input":"2025-01-19T14:24:39.256373Z","iopub.status.idle":"2025-01-19T14:24:39.272929Z","shell.execute_reply.started":"2025-01-19T14:24:39.256342Z","shell.execute_reply":"2025-01-19T14:24:39.272075Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"29"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### BPE Tokenizer\n\nNow try training a BPE tokenizer.","metadata":{"id":"tKkc1D-hAFxl"}},{"cell_type":"code","source":"model_pam_bpe = io.BytesIO()\nspm.SentencePieceTrainer.train(sentence_iterator=iter(pam_train_text), model_writer=model_pam_bpe, vocab_size=1000, model_type='bpe')","metadata":{"id":"AiXj57rh-PIv","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:39.273820Z","iopub.execute_input":"2025-01-19T14:24:39.274034Z","iopub.status.idle":"2025-01-19T14:24:41.784848Z","shell.execute_reply.started":"2025-01-19T14:24:39.274015Z","shell.execute_reply":"2025-01-19T14:24:41.783957Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Q2 MCV\n\nHow many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'","metadata":{"id":"nrQwGmL5AMXc"}},{"cell_type":"code","source":"bpe = spm.SentencePieceProcessor(model_proto=model_pam_bpe.getvalue())\nlen(bpe.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))","metadata":{"id":"0AXuzyaN-PEr","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:41.785748Z","iopub.execute_input":"2025-01-19T14:24:41.785969Z","iopub.status.idle":"2025-01-19T14:24:41.802186Z","shell.execute_reply.started":"2025-01-19T14:24:41.785950Z","shell.execute_reply":"2025-01-19T14:24:41.801485Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"28"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources.","metadata":{"id":"rbb6C6-IS_Ly"}},{"cell_type":"code","source":"unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n\" | \".join(unigram_vocabs[:500])","metadata":{"id":"Aa9j6XrTKjyA","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:41.802879Z","iopub.execute_input":"2025-01-19T14:24:41.803162Z","iopub.status.idle":"2025-01-19T14:24:41.818860Z","shell.execute_reply.started":"2025-01-19T14:24:41.803135Z","shell.execute_reply":"2025-01-19T14:24:41.818175Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<unk> | <s> | </s> | ▁ | า | เ | น | ม | ย | ก | ร | ว | ด | ส | ง | บ | ค | มา | อ | ล | จะ | ท | ให้ | ห | ไป | ไม่ | แ | ว่า | พ | ุ | ี | ๏ | ฯ | ข | ช | เป็น | พระ | โ | ที่ | ใจ | ▁จะ | จ | ะ | ิ | ต | ก็ | อยู่ | ป | ได้ | ่ | ไ | เข้า | ู | ▁พระ | ้า | ตาม | ใน | ้ | ▁แล้ว | เหมือน | รา | ศ | เจ้า | เห็น | ลา | กัน | ั | หา | นาง | ทรง | ประ | ์ | ยา | ัก | ํา | ซ | าน | ัง | ฉ | องค์ | ัด | แล้ว | อน | ดู | ถ | ด้วย | มี | ▁จึง | นี้ | ่า | ผ | น้อง | แต่ | ทํา | ▁นาง | ▁ให้ | รัก | พี่ | คิด | ลูก | พา | รู้ | การ | กับ | ัน | หน้า | กระ | วน | ออก | ่อ | เขา | ถึง | ระ | ข้า | ับ | พล | นั่ง | ทั้ง | หน | รับ | ษ | กล | วง | ลง | ฝ | กร | พร | ความ | เสีย | ดี | ขึ้น | อง | ่ง | ธ | ▁แต่ | คน | กลับ | ▁ฝ่าย | ้น | อด | ภ | หรือ | ตร | ือ | ฟัง | แม่ | ▁ไม่ | ไว้ | ยัง | ▁เห็น | นา | ขอ | มิ | น้ํา | หล | ดัง | ▁พอ | ▁ทั้ง | ช่วย | สม | นั้น | ริ | ทัพ | ต้อง | วัน | อา | น้อย | รบ | ิน | อย่า | เอา | จน | เรา | สุด | เสียง | ข้าง | หลัง | ตี | ตัว | ละ | สุ | วัง | ทุก | ่น | ึก | นึก | เฝ้า | นาย | ฝรั่ง | ทูล | เส | วิ | ปล | ▁ถึง | ตาย | ใคร | อก | อั | ตา | เรือ | จึง | แล | ี่ | ั่ง | แสน | สอง | ของ | ็ | ลี | ี้ | จิต | หมาย | ้ม | แจ้ง | ั่น | สั่ง | ราช | พิ | เห | หาย | ้อง | เมือง | เหลือ | กลาง | กษัตริย์ | ยิ่ง | ตรัส | ึง | เลย | เล่า | ทาง | ุด | ศรี | เคย | ไหน | สาม | หนี | ณ | มัน | ื้อ | ค่อย | ชาย | พราหมณ์ | ▁อย่า | ญ | ที | นิ | น่า | สิ้น | ฉัน | กาย | ลังกา | ▁ด้วย | คอย | บอก | สิ | ฟ | สงสาร | พ่อ | ยง | จริง | ชาว | ถาม | ไร | ทหาร | ตั้ง | ▁อัน | เที่ยว | ปร | ผู้ | พวก | สาร | ชม | ศึก | คํา | ▁เป็น | ทอง | อบ | ใหญ่ | ถือ | สาว | พระอภัย | จง | สา | จับ | ั้น | พลาง | ▁มา | ยก | ▁บ้าง | ไพร่ | ลม | ล้วน | ▁ต่าง | ร้อย | พบ | งาม | แกล้ง | อาย | จะได้ | เคียง | อย่าง | เครื่อง | กลัว | ลาย | จํา | ต่าง | สินสมุทร | ▁พวก | ม้า | ลํา | นี่ | ผา | แก้ว | เพราะ | ▁ครั้น | ▁จน | ▁แม้น | สาย | พัน | พระองค์ | พร้อม | วาย | ชิง | ห้อง | ร้อง | สู้ | ▁จง | ลิ | ราย | ล่อ | จาก | ้ว | ท่าน | รอง | เดิน | เรียก | ขัด | เหล่า | กุมาร | ผล | ป่า | ู่ | คู่ | รูป | กิน | พอ | ร่ํา | โฉม | ▁ถ้า | คง | ่าย | ใช้ | ตอบ | หลง | ไล่ | จัด | ดับ | ▁เมื่อ | บน | อ่อน | แสง | คืน | ใส่ | แค้น | รถ | ตรง | แต่ง | แน่ | เชิญ | ชื่น | ถวาย | โห | จร | มิได้ | นอน | ุก | ชวน | เมีย | อาลัย | ้อม | ลับ | ไหว | ▁แม้ | บิดา | หญิง | หลับ | ดอก | กล้า | ขาด | จัก | ไม่มี | บาท | เสนา | ย์ | ช่าง | โศก | วาง | ติด | เสร็จ | ร้อน | คุณ | ผัว | นัก | ความตาม | พักตร์ | หน่อ | ้อย | ▁ซึ่ง | ตะ | ห้าม | พราย | ฟ้า | ไฉน | ใ | ตก | เมื่อ | ยศ | ชล | ดํา | หนึ่ง | ผัน | ใด | สัก | ร้าย | วิ่ง | แก้ | ยาม | ศรีสุวรรณ | ปืน | ฆ่า | ขับ | ขวา | ไฟ | พูด | หมอง | ก็ไม่ | กําลัง | รักษา | เช่น | ุ่ม | ผี | หาญ | เล่น | เนื้อ | รีบ | ถูก | ชัย | บุตรี | ฟัน | บ้าง | เอ๋ย | สงสัย | ผิด | นิ่ง | ชื่อ | เถิด | ผ่อน | หลาน | สี่ | ชาติ | อี | ปาก | ช้า | ึ | แตก | ตรา | รณ | ลอง | ปี | หมอ | เจ้าพราหมณ์ | พี่เลี้ยง | ต่อ | พลอย | โฉมยง | เนตร | หัก | กอด | เชย | ทั้งสอง | ยิ้ม | ค่ํา | นอก | ขวัญ | ซ้ํา | อารมณ์ | ทุกข์ | แขก | เย็น | หนักหนา | ั้ง | ปิด | โปรด | ้ง | กําปั่น | เรียง | แรง | สิ่ง | เศร้า'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n\" | \".join(bpe_vocabs[:500])","metadata":{"id":"2TsXA0UqN5LN","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:41.819650Z","iopub.execute_input":"2025-01-19T14:24:41.819887Z","iopub.status.idle":"2025-01-19T14:24:41.835887Z","shell.execute_reply.started":"2025-01-19T14:24:41.819860Z","shell.execute_reply":"2025-01-19T14:24:41.835038Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'<unk> | <s> | </s> | ้า | ่า | อง | ระ | ํา | รา | อย | ่ง | มา | จะ | ัง | ัน | ▁เ | าย | ้ว | ับ | ี่ | ม่ | อน | ให | าม | ้น | ็น | พระ | ีย | าง | กล | ้ง | ัก | หน | ให้ | ไม่ | หล | ่น | ึง | ▁แ | ทั | ตร | าร | ้อง | ไป | ิด | ข้า | ว่า | หม | คร | ือ | ล้ว | เป | เส | ประ | าน | ั่ง | ▁๏ | ▁ฯ | ที่ | อก | เล | ิน | ได | พล | ทร | ัด | นาง | ึก | ได้ | ู่ | ▁จะ | ค์ | ี้ | พร | เป็น | สุ | ทั้ง | อม | ัย | เร | ห็น | ▁จ | ▁พระ | ก็ | ใจ | อา | ื่ | ่าง | ต่ | กร | ิง | วง | วน | ือน | เจ | ู้ | ียง | อยู่ | รร | ตาม | ▁พ | ้วย | าว | ถึง | คล | ั้น | รี | เข | ด้วย | สม | องค์ | สน | าก | ▁แล้ว | เช | ัว | ย์ | ใน | คว | น้ | หมือน | ▁ส | ูก | อบ | กระ | เจ้า | ทรง | ลา | กัน | มี | ่าย | พรา | ิ่ง | เข้า | เห็น | ิต | สง | อด | ณ์ | วย | ้ม | คิด | เม | เก | เด | ▁นาง | วา | ุก | ▁ให้ | ดู | หา | ▁อ | ▁จึง | ทํา | ลง | รัก | เค | แล้ว | ่าน | พี่ | เหมือน | ั่น | ความ | ยง | อย่า | หร | มิ | ืน | ช่ | การ | ัญ | ▁ไม่ | ฝ่าย | ศรี | ้าง | วก | ้อม | ือง | น้อง | ยว | พา | แก | กํา | ่อน | ื่น | หน้า | ยา | ดี | ั้ง | ▁ทั้ง | ปรา | คน | เน | หว | รับ | แต่ | ้าย | ัส | เหล | ดา | สํา | นี้ | สาร | กับ | ลูก | ละ | ▁ต | รู้ | ื่อ | ▁ฝ่าย | ึ่ง | ลัง | าด | ื้ | กา | ขึ | นั่ง | เท | ▁เห็น | ฟัง | ้อย | ไร | ขึ้น | เสีย | ▁แต่ | บุ | สา | ไว | ทุก | กลับ | สุด | ัต | ใคร | น้ํา | ชา | ุด | ทัพ | วัน | สอง | นา | หย | ตา | รบ | ▁มา | ่อ | หรือ | ทู | ยัง | รง | จร | ปร | ▁บ | ไว้ | ดัง | วิ | ช่วย | ปล | ออก | ัตร | เพ | สิ | แจ | แล | ็จ | ิย์ | ▁พอ | มาร | ค่ | วรร | หมณ์ | คํา | เขา | นั้น | กษ | เย | ข้าง | หมา | เว | ไพร | หลัง | จิต | พราหมณ์ | ้ํา | ▁ถึง | ขอ | ทูล | สาม | ื้อ | วาย | อภ | ทาง | ▁แม | วัง | โฉ | ่ม | จน | ▁เป | ัตริย์ | ื่อง | สั่ง | แม่ | ▁ช | ฝ้า | โฉม | ราช | ฝร | ▁ถ | ฝรั่ง | ิ์ | ลม | แต | ▁เป็น | หาร | ื้น | เห | ้อน | ตาย | ุ่ง | ตัว | อย่าง | ลี | ผู้ | น้อย | ฉัน | ตรี | กุ | ษา | ุทร | ถาม | ของ | พร้อม | ชี | สร | เอ | ุง | พลาง | ตี | สมุทร | หาย | ที | วรรณ | เลี้ | นึก | จึง | หมาย | ▁ด้วย | ขว | ียน | ศึก | ่อง | ต้อง | ลัย | บา | พิ | อุ | สุวรรณ | โย | เรา | กลาง | เฝ้า | กษัตริย์ | สะ | แท | สัย | แจ้ง | หญ | ▁อย่า | รํา | ตรัส | อภัย | ผล | เลย | ียว | ไหน | ้าว | แน | ิดา | ริ | สาว | ิ้ม | เมือง | เล่า | ขัด | ค่อย | ภา | โอ | ่ํา | มัน | ชม | ห์ | ชาย | ัล | นาย | ▁เจ | เสียง | ยิ่ง | รู | ๋ย | เปล | เอา | ▁เส | คง | ตรา | ห้า | ินสมุทร | คอย | หญิง | หนี | ้าน | ญา | คุ | บรร | ▁ประ | กาย | ทหาร | ▁อัน | สิ้น | ทธ | ทอง | ักษ | ลังกา | นิ | พู | ศ์ | ่ว | จา | ใหญ | ที่ยว | มน | ไล | จริง | ▁เจ้า | จํา | ▁บ้าง | บอก | ▁ต่าง | ติ | ▁เข้า | ไม | ศร | อั | เคย | เลี้ยง | กรา | แสน | ▁จน | จับ | พบ | ครั้น | จง | พวก | สี | ไข | ษฐ | เกล | คา | รม | พัก | พัน | ซึ่ง | หนัก | นี | ่าว | กรุง | กล้ง | ▁เหมือน | ครา | เคร | ท้าว | ใส | ▁พวก | ตั้ง | หลง | ล้วน | ▁ไป | ผี | ลํา | นัก | ร้อง | ▁จง | ทรา | หนา | ▁ก็ | กลัว | ▁ที่ | เคียง | อาย | เรือ | ▁แม้น | เต | แค | ยก | พราะ | ใหญ่ | ▁ครั้น | ▁น | แก้ว | ถือ | ▁ได้ | เหลือ'"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### User-defined symbols\n\nAnother important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n\nRefer to the documentation for ways to add these special tokens to your tokenizer.\n\nhttps://github.com/google/sentencepiece/blob/master/python","metadata":{"id":"eu6QnnRfQyFj"}},{"cell_type":"markdown","source":"## Train another tokenizer on another domain\n\nNow try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier.","metadata":{"id":"QEFOj62ZEdzT"}},{"cell_type":"code","source":"## Train\nmodel_pantip = io.BytesIO()\nspm.SentencePieceTrainer.train(sentence_iterator=iter(pantip_train_text), model_writer=model_pantip, vocab_size=1000)\n\nsp_pantip = spm.SentencePieceProcessor(model_proto=model_pantip.getvalue())\nprint(len(sp_pantip.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str)))","metadata":{"id":"O7-QkA1eMZFf","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:41.836817Z","iopub.execute_input":"2025-01-19T14:24:41.837104Z","iopub.status.idle":"2025-01-19T14:24:47.056944Z","shell.execute_reply.started":"2025-01-19T14:24:41.837075Z","shell.execute_reply":"2025-01-19T14:24:47.055955Z"}},"outputs":[{"name":"stdout","text":"27\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Analyse top tokens on different datasets\n\nUse your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens.","metadata":{"id":"R5WOVMbONnYv"}},{"cell_type":"code","source":"word_count_pam = {}\nfor line in pam_test_text:\n    for word in sp_pam.encode(line, out_type=str):\n        if word not in word_count_pam:\n            word_count_pam[word] = 0\n        word_count_pam[word] += 1\n\nfor i in word_count_pam:\n    if len(i) > 1 and word_count_pam[i] > 300:\n        print(i, word_count_pam[i])","metadata":{"id":"wbfkGcsUrPYS","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:47.057939Z","iopub.execute_input":"2025-01-19T14:24:47.058208Z","iopub.status.idle":"2025-01-19T14:24:47.191286Z","shell.execute_reply.started":"2025-01-19T14:24:47.058186Z","shell.execute_reply":"2025-01-19T14:24:47.190631Z"}},"outputs":[{"name":"stdout","text":"ว่า 520\nพระ 462\nจะ 622\nมา 666\nที่ 417\nให้ 650\nไป 657\nอยู่ 314\nไม่ 536\nยา 321\n▁จะ 421\nเป็น 387\nใจ 313\nรา 326\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### To answer\nWhat are some notable differences you see between the two vocabs?\n\nWrite your answer below.\n\nส่วนใหญ่คล้ายๆ กัน\npam มีคำว่า พระ และคำอื่นๆ ที่ใช้เล่าเรื่อง\npantip มีคำภาษาพูดที่พบเจอได้บ่อย อาจเพราะคนใช้จริง","metadata":{"id":"Qz0GdZ-5YYM9"}},{"cell_type":"code","source":"word_count_pantip = {}\nfor line in pantip_test_text:\n    for word in sp_pantip.encode(line, out_type=str):\n        if word not in word_count_pantip:\n            word_count_pantip[word] = 0\n        word_count_pantip[word] += 1\n\nfor i in word_count_pantip:\n    if len(i) > 1 and word_count_pantip[i] > 300:\n        print(i, word_count_pantip[i])","metadata":{"id":"QxxYr0QLbDoU","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:47.192112Z","iopub.execute_input":"2025-01-19T14:24:47.192371Z","iopub.status.idle":"2025-01-19T14:24:47.302218Z","shell.execute_reply.started":"2025-01-19T14:24:47.192348Z","shell.execute_reply":"2025-01-19T14:24:47.301573Z"}},"outputs":[{"name":"stdout","text":"ของ 375\nจะ 677\nได้ 579\nเป็น 525\nที่ 1145\nนี้ 329\nค่ะ 318\nไป 654\nให้ 405\n้า 411\nแล้ว 366\nเรา 576\nมี 536\nก็ 535\nว่า 555\nมา 651\nไม่ 475\nเลย 375\nใน 436\nการ 445\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Using tokenizer across domains\n\nOne problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n\nNext you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa.","metadata":{"id":"ipjO87HPYl4N"}},{"cell_type":"markdown","source":"### Q3 MCV\n\nWhat percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี.","metadata":{"id":"I4_6JG_l5BXh"}},{"cell_type":"code","source":"train_pam_test_pam_count = 0\ntrain_pantip_test_pam_count = 0\n\nfor line in pra_apai_manee_data:\n    train_pam_test_pam_count += len(sp_pam.encode(line, out_type=str))\n    train_pantip_test_pam_count += len(sp_pantip.encode(line, out_type=str))\n\nprint((train_pantip_test_pam_count - train_pam_test_pam_count) / train_pam_test_pam_count * 100)","metadata":{"id":"3tCh1RaZrTAM","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:47.302949Z","iopub.execute_input":"2025-01-19T14:24:47.303151Z","iopub.status.idle":"2025-01-19T14:24:48.302287Z","shell.execute_reply.started":"2025-01-19T14:24:47.303133Z","shell.execute_reply":"2025-01-19T14:24:48.301559Z"}},"outputs":[{"name":"stdout","text":"43.42710745209495\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Q4 MCV\n\nWhat percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip.","metadata":{"id":"duaCJRO96SX1"}},{"cell_type":"code","source":"train_pantip_test_pantip_count = 0\ntrain_pam_test_pantip_count = 0\n\nfor line in pantip_text:\n    train_pantip_test_pantip_count += len(sp_pantip.encode(line, out_type=str))\n    train_pam_test_pantip_count += len(sp_pam.encode(line, out_type=str))\n\nprint((train_pam_test_pantip_count - train_pantip_test_pantip_count) / train_pantip_test_pantip_count * 100)","metadata":{"id":"axk9gOIgrTYd","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:48.303083Z","iopub.execute_input":"2025-01-19T14:24:48.303295Z","iopub.status.idle":"2025-01-19T14:24:49.145573Z","shell.execute_reply.started":"2025-01-19T14:24:48.303276Z","shell.execute_reply":"2025-01-19T14:24:49.144683Z"}},"outputs":[{"name":"stdout","text":"12.855248847916101\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### To answer\nWhy do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)","metadata":{"id":"yZYKuamv7-wI"}},{"cell_type":"markdown","source":"Because the general tokenizer does not know the old words in pam data, the tokens from the tokenizer are higher than those from the pam tokenizer.","metadata":{}},{"cell_type":"markdown","source":"## The effect on language models\n\nNext, we will see the effect of using \"cross-domain\" tokenizers on Language models.","metadata":{"id":"O7j_Cc0p9-5S"}},{"cell_type":"markdown","source":"### Setup\nWe are going to reuse the code from the last assignment","metadata":{"id":"KiWztANvohhn"}},{"cell_type":"code","source":"import itertools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport lightning as L\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"id":"JMt5GzLrW4x3","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:49.146525Z","iopub.execute_input":"2025-01-19T14:24:49.146850Z","iopub.status.idle":"2025-01-19T14:24:58.017343Z","shell.execute_reply.started":"2025-01-19T14:24:49.146816Z","shell.execute_reply":"2025-01-19T14:24:58.016660Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class TextDataset(Dataset):\n  def __init__(self, data, tokenizer, seq_len = 128):\n\n    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n    flatten_token_ids = list(itertools.chain(*token_ids))\n    encoded = torch.LongTensor(flatten_token_ids)\n\n    left_over = len(encoded) % seq_len\n    encoded = encoded[:len(encoded)-left_over]\n    self.encoded = encoded.view(-1, seq_len)\n\n  def __getitem__(self, idx):\n    return self.encoded[idx]\n\n  def __len__(self):\n    return len(self.encoded)","metadata":{"id":"0OIs_VS_oo1M","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:58.018200Z","iopub.execute_input":"2025-01-19T14:24:58.018699Z","iopub.status.idle":"2025-01-19T14:24:58.023955Z","shell.execute_reply.started":"2025-01-19T14:24:58.018672Z","shell.execute_reply":"2025-01-19T14:24:58.022984Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class LSTM(L.LightningModule):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.vocab_size=vocab_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.learning_rate = learning_rate\n        self.criterion = criterion\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        embedded = self.dropout(embedded)\n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = self.dropout(lstm_out)\n        output = self.fc(lstm_out)\n\n        return output\n\n    def training_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        prediction = self(src)\n        prediction = prediction.reshape(-1, self.vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        with torch.no_grad():\n          prediction = self(src)\n        prediction = prediction.reshape(-1, self.vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"id":"hk6vEPiMq34n","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:58.024848Z","iopub.execute_input":"2025-01-19T14:24:58.025146Z","iopub.status.idle":"2025-01-19T14:24:58.065925Z","shell.execute_reply.started":"2025-01-19T14:24:58.025115Z","shell.execute_reply":"2025-01-19T14:24:58.065233Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"vocab_size = sp_pam.get_piece_size()\nembedding_dim = 200\nhidden_dim = 512\nnum_layers = 3\ndropout_rate = 0.2\nlr = 1e-3\ncriterion = nn.CrossEntropyLoss()\ntrain_batch_size = 64\ntest_batch_size = 128","metadata":{"id":"oKhuOygixndB","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:58.066902Z","iopub.execute_input":"2025-01-19T14:24:58.067203Z","iopub.status.idle":"2025-01-19T14:24:58.083412Z","shell.execute_reply.started":"2025-01-19T14:24:58.067172Z","shell.execute_reply":"2025-01-19T14:24:58.082582Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Training","metadata":{"id":"kOtOE7mr-heY"}},{"cell_type":"markdown","source":"<a name=\"no1\"></a>\n#### 1. Training on Pantip data with Pantip tokenizer","metadata":{"id":"g8-x9HiPDcpE"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pantip)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pantip)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pantip_train_loader)","metadata":{"id":"oUv_A4MTx0Ob","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:24:58.086287Z","iopub.execute_input":"2025-01-19T14:24:58.086553Z","iopub.status.idle":"2025-01-19T14:26:06.991901Z","shell.execute_reply.started":"2025-01-19T14:24:58.086532Z","shell.execute_reply":"2025-01-19T14:26:06.990883Z"}},"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (45) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046d29c45b504f0faa67c9f1596c01ac"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"1e-Y1_GYy65g","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:26:06.993232Z","iopub.execute_input":"2025-01-19T14:26:06.993626Z","iopub.status.idle":"2025-01-19T14:26:12.915771Z","shell.execute_reply.started":"2025-01-19T14:26:06.993581Z","shell.execute_reply":"2025-01-19T14:26:12.915015Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b48381d568433498dc1bfcb9c395d9"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t65.48542148800519\nPerplexity on Pra apai manee train set is:\t107.82675078924876\nPerplexity on Pantip test set is:\t101.5747773544641\nPerplexity on Pra apai manee test set is:\t109.55010732666676\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"<a name=\"no2\"></a>\n#### 2. Training on Pantip data with Pra apai manee tokenizer","metadata":{"id":"7s3AmE4nDjmL"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pam)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pam)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pantip_train_loader)","metadata":{"id":"vfRdW3m1Dmj_","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:26:12.916685Z","iopub.execute_input":"2025-01-19T14:26:12.916999Z","iopub.status.idle":"2025-01-19T14:27:26.289679Z","shell.execute_reply.started":"2025-01-19T14:26:12.916966Z","shell.execute_reply":"2025-01-19T14:27:26.288645Z"}},"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677d74ef527c43dba238f53ac893fee4"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"xwLN1IarD3g9","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:27:26.290636Z","iopub.execute_input":"2025-01-19T14:27:26.291243Z","iopub.status.idle":"2025-01-19T14:27:32.183292Z","shell.execute_reply.started":"2025-01-19T14:27:26.291209Z","shell.execute_reply":"2025-01-19T14:27:32.182628Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0762bec284014cde8de249435af82da4"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t36.50440184431368\nPerplexity on Pra apai manee train set is:\t444.1728629346219\nPerplexity on Pantip test set is:\t46.5575833246705\nPerplexity on Pra apai manee test set is:\t418.9724141541434\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"#### To answer\n\nThe perplexity numbers should indicate that:\n1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n\nTry and come up with some reasons for the results above. <br>\nHint:\n1. think about \"general\" vocabs and domain-specific vocabs.\n2. what do you think happens to the model when the token ids become longer.","metadata":{"id":"NB8zqptTWcA6"}},{"cell_type":"markdown","source":"The vocabulary of the Pra Apai Manee tokenizer is less general and modern compared to Pantip's. Longer tokens in Pra Apai Manee require more specific subwords.\n\nThis results in the model's attention being distributed across a wider variety of tokens, making it more challenging to generalize and learn dependencies between tokens—an issue specific to the Pra Apai Manee tokenizer.","metadata":{}},{"cell_type":"markdown","source":"\n<a name=\"no3\"></a>\n#### 3. Training on Pra apai manee data with Pantip tokenizer\n","metadata":{"id":"y8VPMm7pLdSl"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pantip)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pantip)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pam_train_loader)","metadata":{"id":"oR5fp-YCLnnU","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:27:32.184158Z","iopub.execute_input":"2025-01-19T14:27:32.184501Z","iopub.status.idle":"2025-01-19T14:29:06.997717Z","shell.execute_reply.started":"2025-01-19T14:27:32.184468Z","shell.execute_reply":"2025-01-19T14:29:06.996792Z"}},"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fe63567f2e4c1e806bcaf74301fd9c"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"f_LhF7w7Lxwo","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:29:06.998745Z","iopub.execute_input":"2025-01-19T14:29:06.999028Z","iopub.status.idle":"2025-01-19T14:29:13.595572Z","shell.execute_reply.started":"2025-01-19T14:29:06.998992Z","shell.execute_reply":"2025-01-19T14:29:13.594604Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff934277e7fb4de39b2020130d5281f2"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t3413.385523520331\nPerplexity on Pra apai manee train set is:\t34.574579492259765\nPerplexity on Pantip test set is:\t3082.149172075294\nPerplexity on Pra apai manee test set is:\t36.969036797853505\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"<a name=\"no4\"></a>\n#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n\n\n","metadata":{"id":"apk9crJjMLoW"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pam)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pam)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pam_train_loader)","metadata":{"id":"_G7GMBIKLzGK","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:29:13.596655Z","iopub.execute_input":"2025-01-19T14:29:13.597017Z","iopub.status.idle":"2025-01-19T14:30:24.927889Z","shell.execute_reply.started":"2025-01-19T14:29:13.596981Z","shell.execute_reply":"2025-01-19T14:30:24.926956Z"}},"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4156658ab3f0459cae96336641ee13ee"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"9H753o_JMRFw","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:30:24.928756Z","iopub.execute_input":"2025-01-19T14:30:24.928988Z","iopub.status.idle":"2025-01-19T14:30:30.865887Z","shell.execute_reply.started":"2025-01-19T14:30:24.928968Z","shell.execute_reply":"2025-01-19T14:30:30.865211Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b15f3fa5bbd4cb38bc4dd9b2a8a9a60"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t536.2838958106731\nPerplexity on Pra apai manee train set is:\t73.70037004377242\nPerplexity on Pantip test set is:\t517.8680983402234\nPerplexity on Pra apai manee test set is:\t82.3355785992015\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"#### To answer\n\nThe perplexity numbers should indicate that:\n1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n\nTry and come up with some reasons for the results above. <br>","metadata":{"id":"en9Lmmj4dZ-1"}},{"cell_type":"markdown","source":"The tokenizer's vocabulary aligns well with the training data, unlike in case no.3. The Pra Apai Manee tokenizer is specifically designed for the linguistic patterns found in Pra Apai Manee data (poems, which differ from the way people communicate in Pantip data). \n\nThis alignment enables the model to learn better representations for these tokens, allowing it to generalize slightly better to unseen domains, such as Pantip data.","metadata":{}}]}