{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc","timestamp":1612276273852},{"file_id":"1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw","timestamp":1612057948373}],"toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Language Modeling","metadata":{"collapsed":true,"id":"15QfB7RAuXAc","jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n\n","metadata":{"id":"gucid6KNuXAe"}},{"cell_type":"markdown","source":"## setup","metadata":{"id":"yL_M2zf4myYa"}},{"cell_type":"code","source":"# #download corpus\n!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n!unzip BEST2010.zip","metadata":{"id":"MRRrn78ZjL54","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:09.903264Z","iopub.execute_input":"2025-01-19T13:25:09.903609Z","iopub.status.idle":"2025-01-19T13:25:11.264294Z","shell.execute_reply.started":"2025-01-19T13:25:09.903582Z","shell.execute_reply":"2025-01-19T13:25:11.263252Z"}},"outputs":[{"name":"stdout","text":"--2025-01-19 13:25:09--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\nResolving github.com (github.com)... 140.82.121.4\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n--2025-01-19 13:25:10--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7423530 (7.1M) [application/zip]\nSaving to: ‘BEST2010.zip’\n\nBEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.03s   \n\n2025-01-19 13:25:10 (208 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n\nArchive:  BEST2010.zip\n   creating: BEST2010/\n  inflating: BEST2010/article.txt    \n  inflating: BEST2010/encyclopedia.txt  \n  inflating: BEST2010/news.txt       \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install lightning","metadata":{"id":"SGmYebp38OUl","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:11.265784Z","iopub.execute_input":"2025-01-19T13:25:11.266091Z","iopub.status.idle":"2025-01-19T13:25:16.498448Z","shell.execute_reply.started":"2025-01-19T13:25:11.266061Z","shell.execute_reply":"2025-01-19T13:25:16.497451Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.10)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.5.0.post0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## code","metadata":{"id":"IR4HK5jQm17K"}},{"cell_type":"code","source":"total_word_count = 0\nbest2010 = []\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n  for i,line in enumerate(f):\n    line=line.strip()[:-1] #remove the trailing |\n    total_word_count += len(line.split(\"|\"))\n    best2010.append(line)\n\ntrain = best2010[:int(len(best2010)*0.7)]\ntest = best2010[int(len(best2010)*0.7):]\n#Training data\ntrain_word_count =0\nfor line in train:\n    for word in line.split('|'):\n        train_word_count+=1\nprint ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\nprint ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))","metadata":{"id":"oPE1RqKOrWJ0","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:16.500308Z","iopub.execute_input":"2025-01-19T13:25:16.500578Z","iopub.status.idle":"2025-01-19T13:25:16.874069Z","shell.execute_reply.started":"2025-01-19T13:25:16.500556Z","shell.execute_reply":"2025-01-19T13:25:16.873381Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news training dataset :\t21678\nTotal word counts in BEST2010 news training dataset :\t1042797\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer).","metadata":{"id":"SQBjqe5arHGX"}},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import CharDelimiterSplit\nfrom tokenizers.trainers import WordLevelTrainer\n\n#Basically, we just use the new tokenizer as our vocab building tool.\n#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\ntokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\ntrainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\ntokenizer.train_from_iterator(train, trainer=trainer)","metadata":{"id":"elwE0gh2rE3C","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:16.875282Z","iopub.execute_input":"2025-01-19T13:25:16.875540Z","iopub.status.idle":"2025-01-19T13:25:17.398854Z","shell.execute_reply.started":"2025-01-19T13:25:16.875521Z","shell.execute_reply":"2025-01-19T13:25:17.397894Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"len(tokenizer.get_vocab()) #same as nltk","metadata":{"id":"TrKtjv4PJpg2","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:17.400012Z","iopub.execute_input":"2025-01-19T13:25:17.400267Z","iopub.status.idle":"2025-01-19T13:25:17.412077Z","shell.execute_reply.started":"2025-01-19T13:25:17.400245Z","shell.execute_reply":"2025-01-19T13:25:17.411390Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"9062"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]","metadata":{"id":"WqM_jrZwrJpB","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:17.412952Z","iopub.execute_input":"2025-01-19T13:25:17.413211Z","iopub.status.idle":"2025-01-19T13:25:17.428302Z","shell.execute_reply.started":"2025-01-19T13:25:17.413190Z","shell.execute_reply":"2025-01-19T13:25:17.427552Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['กฎหมาย', 'กับ', 'การ', 'เบียดบัง', 'คน', 'จน', '[UNK]']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM","metadata":{"id":"1r1pJ1B_sp9j","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:17.429034Z","iopub.execute_input":"2025-01-19T13:25:17.429287Z","iopub.status.idle":"2025-01-19T13:25:17.442655Z","shell.execute_reply.started":"2025-01-19T13:25:17.429267Z","shell.execute_reply":"2025-01-19T13:25:17.441892Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[242, 28, 5, 8883, 22, 190, 0]"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import itertools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport lightning as L\nfrom tqdm import tqdm","metadata":{"id":"Fkx6CSoXWXmG","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:17.445138Z","iopub.execute_input":"2025-01-19T13:25:17.445375Z","iopub.status.idle":"2025-01-19T13:25:26.615450Z","shell.execute_reply.started":"2025-01-19T13:25:17.445317Z","shell.execute_reply":"2025-01-19T13:25:26.614795Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"L.seed_everything(42, workers=True)","metadata":{"id":"3XHJsP8_898x","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:26.617092Z","iopub.execute_input":"2025-01-19T13:25:26.617652Z","iopub.status.idle":"2025-01-19T13:25:26.630301Z","shell.execute_reply.started":"2025-01-19T13:25:26.617626Z","shell.execute_reply":"2025-01-19T13:25:26.629406Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"class TextDataset(Dataset):\n  def __init__(self, data, seq_len = 128):\n    #  data is currently a list of sentences\n    #  [sent1,\n    #   sent2,\n    #   ...,\n    #  ]\n\n    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n    encoded = torch.LongTensor(flatten_token_ids)\n\n    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n    left_over = len(encoded) % seq_len\n    encoded = encoded[:len(encoded)-left_over]\n    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n    ## now data looks like this\n    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n    ##   [1,2,3, ... , 128]\n    ##   [1,2,3, ... , 128]\n    ## ]\n\n  def __getitem__(self, idx):\n    return self.encoded[idx]\n\n  def __len__(self):\n    return len(self.encoded)","metadata":{"id":"-r_kyrrrDHZq","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:26.631498Z","iopub.execute_input":"2025-01-19T13:25:26.631794Z","iopub.status.idle":"2025-01-19T13:25:26.682484Z","shell.execute_reply.started":"2025-01-19T13:25:26.631765Z","shell.execute_reply":"2025-01-19T13:25:26.681620Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_batch_size = 64\ntest_batch_size = 128\ntrain_dataset = TextDataset(train)\ntrain_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n\ntest_dataset = TextDataset(test)\ntest_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)","metadata":{"id":"YmW-K0XBZ4Dq","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:26.683378Z","iopub.execute_input":"2025-01-19T13:25:26.683614Z","iopub.status.idle":"2025-01-19T13:25:28.412372Z","shell.execute_reply.started":"2025-01-19T13:25:26.683593Z","shell.execute_reply":"2025-01-19T13:25:28.411414Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Model : Implement the forward function here","metadata":{"id":"ElhZcB94MUtC"}},{"cell_type":"code","source":"class LSTM(L.LightningModule):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n        self.learning_rate = learning_rate\n        self.criterion = criterion\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        embedded = self.dropout(embedded)\n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = self.dropout(lstm_out)\n        output = self.fc(lstm_out)\n\n        return output\n\n    def training_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        prediction = self(src) # run the sequence through the model (the forward method)\n        prediction = prediction.reshape(-1, vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n        with torch.no_grad(): #disable gradient calculation for faster inference\n          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n        loss = self.criterion(prediction, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"id":"nKNJAolug-1I","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.413207Z","iopub.execute_input":"2025-01-19T13:25:28.413470Z","iopub.status.idle":"2025-01-19T13:25:28.421632Z","shell.execute_reply.started":"2025-01-19T13:25:28.413450Z","shell.execute_reply":"2025-01-19T13:25:28.420382Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nembedding_dim = 200\nhidden_dim = 512\nnum_layers = 3\ndropout_rate = 0.2\nlr = 1e-3","metadata":{"id":"jBnYCh-miOEr","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.422565Z","iopub.execute_input":"2025-01-19T13:25:28.422833Z","iopub.status.idle":"2025-01-19T13:25:28.440959Z","shell.execute_reply.started":"2025-01-19T13:25:28.422801Z","shell.execute_reply":"2025-01-19T13:25:28.440244Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)","metadata":{"id":"HHWXaPsvigPq","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.441763Z","iopub.execute_input":"2025-01-19T13:25:28.442011Z","iopub.status.idle":"2025-01-19T13:25:28.566993Z","shell.execute_reply.started":"2025-01-19T13:25:28.441991Z","shell.execute_reply":"2025-01-19T13:25:28.566319Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from lightning.pytorch.loggers import CSVLogger\ncsv_logger = CSVLogger(\"log\")","metadata":{"id":"_yNEZ4jwXumR","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.567806Z","iopub.execute_input":"2025-01-19T13:25:28.568077Z","iopub.status.idle":"2025-01-19T13:25:28.571876Z","shell.execute_reply.started":"2025-01-19T13:25:28.568055Z","shell.execute_reply":"2025-01-19T13:25:28.571060Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Training","metadata":{"id":"eZwqhWicMdH0"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=20,\n    logger=csv_logger,\n    deterministic=True\n)","metadata":{"id":"kr0zdeMAjD1U","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.572673Z","iopub.execute_input":"2025-01-19T13:25:28.572861Z","iopub.status.idle":"2025-01-19T13:25:28.663725Z","shell.execute_reply.started":"2025-01-19T13:25:28.572844Z","shell.execute_reply":"2025-01-19T13:25:28.663003Z"}},"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins","metadata":{"id":"A9qcwNA0mN6J","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:25:28.664531Z","iopub.execute_input":"2025-01-19T13:25:28.664813Z","iopub.status.idle":"2025-01-19T13:33:50.059463Z","shell.execute_reply.started":"2025-01-19T13:25:28.664777Z","shell.execute_reply":"2025-01-19T13:33:50.058823Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 1.8 M  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 4.6 M  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n12.1 M    Trainable params\n0         Non-trainable params\n12.1 M    Total params\n48.504    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dadfe340d1c488fbfbaff774a87bebc"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Testing","metadata":{"id":"uUfWF_V6Me9H"}},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=test_loader)","metadata":{"id":"WXVj9ewNqweZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:50.060175Z","iopub.execute_input":"2025-01-19T13:33:50.060376Z","iopub.status.idle":"2025-01-19T13:33:55.108213Z","shell.execute_reply.started":"2025-01-19T13:33:50.060358Z","shell.execute_reply":"2025-01-19T13:33:55.107624Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8093ff51dbb747b89420391724d1af1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   4.1082844734191895    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    4.1082844734191895     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import numpy as np","metadata":{"id":"4pVjEyYDtnc-","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:55.108924Z","iopub.execute_input":"2025-01-19T13:33:55.109142Z","iopub.status.idle":"2025-01-19T13:33:55.112507Z","shell.execute_reply.started":"2025-01-19T13:33:55.109122Z","shell.execute_reply":"2025-01-19T13:33:55.111769Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")","metadata":{"id":"uuIPToGQs-ZG","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:55.113141Z","iopub.execute_input":"2025-01-19T13:33:55.113380Z","iopub.status.idle":"2025-01-19T13:33:55.188221Z","shell.execute_reply.started":"2025-01-19T13:33:55.113322Z","shell.execute_reply":"2025-01-19T13:33:55.187262Z"}},"outputs":[{"name":"stdout","text":"Perplexity : 60.84225148840851\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.eval() #disable dropout","metadata":{"id":"pAZwiRqsnOPe","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:55.189228Z","iopub.execute_input":"2025-01-19T13:33:55.189479Z","iopub.status.idle":"2025-01-19T13:33:55.204180Z","shell.execute_reply.started":"2025-01-19T13:33:55.189459Z","shell.execute_reply":"2025-01-19T13:33:55.203412Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"LSTM(\n  (embedding): Embedding(9062, 200)\n  (lstm): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc): Linear(in_features=512, out_features=9062, bias=True)\n  (criterion): CrossEntropyLoss()\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"unk_token_id = tokenizer.encode(\"[UNK]\").ids\neos_token_id = tokenizer.encode(\"</s>\").ids","metadata":{"id":"VFtebDAmVh_T","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:55.205004Z","iopub.execute_input":"2025-01-19T13:33:55.205289Z","iopub.status.idle":"2025-01-19T13:33:55.218214Z","shell.execute_reply.started":"2025-01-19T13:33:55.205266Z","shell.execute_reply":"2025-01-19T13:33:55.217423Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def generate_seq(context, max_new_token = 10):\n  encoded = tokenizer.encode(context).ids\n  with torch.no_grad():\n      for i in range(max_new_token):\n          src = torch.LongTensor([encoded]).to(model.device)\n          prediction = model(src)\n          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n          prediction = torch.multinomial(probs, num_samples=1).item()\n\n          while prediction == unk_token_id:\n              prediction = torch.multinomial(probs, num_samples=1).item()\n\n          if prediction == eos_token_id:\n              breakx\n\n          encoded.append(prediction)\n\n  return tokenizer.decode(encoded)","metadata":{"id":"hj-V4OsDqpBO","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:33:55.220649Z","iopub.execute_input":"2025-01-19T13:33:55.220841Z","iopub.status.idle":"2025-01-19T13:33:55.231246Z","shell.execute_reply.started":"2025-01-19T13:33:55.220824Z","shell.execute_reply":"2025-01-19T13:33:55.230573Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"context = \"<s>|วัน|นี้\"\ngenerate_seq(context, 50)","metadata":{"id":"u20r9w8zvJi4","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:36:06.115620Z","iopub.execute_input":"2025-01-19T13:36:06.115983Z","iopub.status.idle":"2025-01-19T13:36:06.999460Z","shell.execute_reply.started":"2025-01-19T13:36:06.115950Z","shell.execute_reply":"2025-01-19T13:36:06.998525Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'วัน นี้   เมื่อ ถาม ว่า จะ พูด กับ ทุก ฝ่าย   เพราะ จะ ไม่ มี การ ติดต่อ ชั่วคราว ด้วย   โดย นี่ ถึง บท ของ อาจารย์เสรี จำนวน   4 , 000   แสน บาท   หาก เป็น นโยบาย สำคัญ ของ นัก งาน   ด้าน   นายก เทศมนตรี   จคป.'"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Questions: Answer the following in MyCourseville\n\n1. What is the perplexity of the neural LM you trained?\n2. Paste your favorite sentence generated with the LM.","metadata":{"id":"1fr536NVvGX3"}}]}