{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"provenance":[{"file_id":"1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc","timestamp":1612276273852},{"file_id":"1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw","timestamp":1612057948373}],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"15QfB7RAuXAc"},"source":["# Neural Language Modeling"]},{"cell_type":"markdown","metadata":{"id":"gucid6KNuXAe"},"source":["In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n","\n"]},{"cell_type":"markdown","source":["## setup"],"metadata":{"id":"yL_M2zf4myYa"}},{"cell_type":"code","metadata":{"id":"MRRrn78ZjL54"},"source":["# #download corpus\n","!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n","!unzip BEST2010.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install lightning"],"metadata":{"id":"SGmYebp38OUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## code"],"metadata":{"id":"IR4HK5jQm17K"}},{"cell_type":"code","source":["total_word_count = 0\n","best2010 = []\n","with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n","  for i,line in enumerate(f):\n","    line=line.strip()[:-1] #remove the trailing |\n","    total_word_count += len(line.split(\"|\"))\n","    best2010.append(line)\n","\n","train = best2010[:int(len(best2010)*0.7)]\n","test = best2010[int(len(best2010)*0.7):]\n","#Training data\n","train_word_count =0\n","for line in train:\n","    for word in line.split('|'):\n","        train_word_count+=1\n","print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n","print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"],"metadata":{"id":"oPE1RqKOrWJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer)."],"metadata":{"id":"SQBjqe5arHGX"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.pre_tokenizers import CharDelimiterSplit\n","from tokenizers.trainers import WordLevelTrainer\n","\n","#Basically, we just use the new tokenizer as our vocab building tool.\n","#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\n","tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\n","trainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n","                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\n","tokenizer.train_from_iterator(train, trainer=trainer)"],"metadata":{"id":"elwE0gh2rE3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(tokenizer.get_vocab()) #same as nltk"],"metadata":{"id":"TrKtjv4PJpg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]"],"metadata":{"id":"WqM_jrZwrJpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM"],"metadata":{"id":"1r1pJ1B_sp9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import lightning as L\n","from tqdm import tqdm"],"metadata":{"id":"Fkx6CSoXWXmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["L.seed_everything(42, workers=True)"],"metadata":{"id":"3XHJsP8_898x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","  def __init__(self, data, seq_len = 128):\n","    #  data is currently a list of sentences\n","    #  [sent1,\n","    #   sent2,\n","    #   ...,\n","    #  ]\n","\n","    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n","    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n","    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n","    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n","    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n","    encoded = torch.LongTensor(flatten_token_ids)\n","\n","    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n","    left_over = len(encoded) % seq_len\n","    encoded = encoded[:len(encoded)-left_over]\n","    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n","    ## now data looks like this\n","    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n","    ##   [1,2,3, ... , 128]\n","    ##   [1,2,3, ... , 128]\n","    ## ]\n","\n","  def __getitem__(self, idx):\n","    return self.encoded[idx]\n","\n","  def __len__(self):\n","    return len(self.encoded)"],"metadata":{"id":"-r_kyrrrDHZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_batch_size = 64\n","test_batch_size = 128\n","train_dataset = TextDataset(train)\n","train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n","\n","test_dataset = TextDataset(test)\n","test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"],"metadata":{"id":"YmW-K0XBZ4Dq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model : Implement the forward function here"],"metadata":{"id":"ElhZcB94MUtC"}},{"cell_type":"code","source":["class LSTM(L.LightningModule):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n","\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n","                    dropout=dropout_rate, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n","        self.learning_rate = learning_rate\n","        self.criterion = criterion\n","\n","    def forward(self, src):\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","\n","        src = batch[:, :-1]\n","        target = batch[:, 1:]\n","        prediction = self(src) # run the sequence through the model (the forward method)\n","        prediction = prediction.reshape(-1, vocab_size)\n","        target = target.reshape(-1)\n","        loss = self.criterion(prediction, target)\n","        self.log(\"train_loss\", loss)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","\n","        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n","        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n","        with torch.no_grad(): #disable gradient calculation for faster inference\n","          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n","        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n","        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n","        loss = self.criterion(prediction, target)\n","        self.log(\"test_loss\", loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), lr=self.learning_rate)"],"metadata":{"id":"nKNJAolug-1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = tokenizer.get_vocab_size()\n","embedding_dim = 200\n","hidden_dim = 512\n","num_layers = 3\n","dropout_rate = 0.2\n","lr = 1e-3"],"metadata":{"id":"jBnYCh-miOEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)"],"metadata":{"id":"HHWXaPsvigPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightning.pytorch.loggers import CSVLogger\n","csv_logger = CSVLogger(\"log\")"],"metadata":{"id":"_yNEZ4jwXumR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"eZwqhWicMdH0"}},{"cell_type":"code","source":["trainer = L.Trainer(\n","    max_epochs=20,\n","    logger=csv_logger,\n","    deterministic=True\n",")"],"metadata":{"id":"kr0zdeMAjD1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins"],"metadata":{"id":"A9qcwNA0mN6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing"],"metadata":{"id":"uUfWF_V6Me9H"}},{"cell_type":"code","source":["test_result = trainer.test(model, dataloaders=test_loader)"],"metadata":{"id":"WXVj9ewNqweZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"4pVjEyYDtnc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")"],"metadata":{"id":"uuIPToGQs-ZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval() #disable dropout"],"metadata":{"id":"pAZwiRqsnOPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unk_token_id = tokenizer.encode(\"[UNK]\").ids\n","eos_token_id = tokenizer.encode(\"</s>\").ids"],"metadata":{"id":"VFtebDAmVh_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_seq(context, max_new_token = 10):\n","  encoded = tokenizer.encode(context).ids\n","  with torch.no_grad():\n","      for i in range(max_new_token):\n","          src = torch.LongTensor([encoded]).to(model.device)\n","          prediction = model(src)\n","          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n","          prediction = torch.multinomial(probs, num_samples=1).item()\n","\n","          while prediction == unk_token_id:\n","              prediction = torch.multinomial(probs, num_samples=1).item()\n","\n","          if prediction == eos_token_id:\n","              break\n","\n","          encoded.append(prediction)\n","\n","  return tokenizer.decode(encoded)"],"metadata":{"id":"hj-V4OsDqpBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = \"<s>|วัน|จันทร์\"\n","generate_seq(context, 50)"],"metadata":{"id":"u20r9w8zvJi4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Questions: Answer the following in MyCourseville\n","\n","1. What is the perplexity of the neural LM you trained?\n","2. Paste your favorite sentence generated with the LM."],"metadata":{"id":"1fr536NVvGX3"}}]}