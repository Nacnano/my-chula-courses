{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HOMEWORK 6: TEXT CLASSIFICATION\nIn this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n\nWe will focus only on the Object Classification task for this homework.\n\nIn this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n\nYou will need to build 3 different models.\n\n1. A model based on tf-idf\n2. A model based on MUSE\n3. A model based on wangchanBERTa\n\n**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n\nThis homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification.","metadata":{"id":"VQ8FRFIYMc5X"}},{"cell_type":"code","source":"!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv","metadata":{"id":"kHqkFSyaNvOt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"879b17f1-0fb2-455c-ca37-b5a4aecd7b1c","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:20.567593Z","iopub.execute_input":"2025-02-15T17:31:20.567931Z","iopub.status.idle":"2025-02-15T17:31:21.896023Z","shell.execute_reply.started":"2025-02-15T17:31:20.567884Z","shell.execute_reply":"2025-02-15T17:31:21.895158Z"}},"outputs":[{"name":"stdout","text":"--2025-02-15 17:31:20--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n--2025-02-15 17:31:20--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com/cd/0/inline/CkKZqgsnCeK_cKn_F_r3AHWaZekPiUFbXPjkJ4ZJHHgogAQO1mChEOUYHrrhbXMe9R9gmHHmEniUmay7iiBTFp3FHu2zmEi0mfm4AFLanWxwsxFcNGH4IxP-8iqNVDU-Hgw/file# [following]\n--2025-02-15 17:31:21--  https://uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com/cd/0/inline/CkKZqgsnCeK_cKn_F_r3AHWaZekPiUFbXPjkJ4ZJHHgogAQO1mChEOUYHrrhbXMe9R9gmHHmEniUmay7iiBTFp3FHu2zmEi0mfm4AFLanWxwsxFcNGH4IxP-8iqNVDU-Hgw/file\nResolving uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com (uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\nConnecting to uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com (uc776968fa7b8fef321a00b88019.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2518977 (2.4M) [text/plain]\nSaving to: ‘clean-phone-data-for-students.csv’\n\nclean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.03s   \n\n2025-02-15 17:31:21 (82.2 MB/s) - ‘clean-phone-data-for-students.csv’ saved [2518977/2518977]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pythainlp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRlx5Mb5zkXw","outputId":"18d913e0-aa6d-435b-931d-591386cb4ba8","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:21.897245Z","iopub.execute_input":"2025-02-15T17:31:21.897623Z","iopub.status.idle":"2025-02-15T17:31:28.180995Z","shell.execute_reply.started":"2025-02-15T17:31:21.897585Z","shell.execute_reply":"2025-02-15T17:31:28.180010Z"}},"outputs":[{"name":"stdout","text":"Collecting pythainlp\n  Downloading pythainlp-5.0.5-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\nDownloading pythainlp-5.0.5-py3-none-any.whl (17.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pythainlp\nSuccessfully installed pythainlp-5.0.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Import Libs","metadata":{"id":"2YprqbOPMc5a"}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom torch.utils.data import Dataset\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"heICP79cMc5e","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:28.181916Z","iopub.execute_input":"2025-02-15T17:31:28.182148Z","iopub.status.idle":"2025-02-15T17:31:32.548653Z","shell.execute_reply.started":"2025-02-15T17:31:28.182128Z","shell.execute_reply":"2025-02-15T17:31:32.547919Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_df = pd.read_csv('clean-phone-data-for-students.csv')","metadata":{"id":"JhZ2eBAWMc5l","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:32.550509Z","iopub.execute_input":"2025-02-15T17:31:32.550914Z","iopub.status.idle":"2025-02-15T17:31:32.613443Z","shell.execute_reply.started":"2025-02-15T17:31:32.550889Z","shell.execute_reply":"2025-02-15T17:31:32.612826Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def clean_data(df):\n    \"\"\"Cleans the dataset by selecting relevant columns, normalizing labels, \n    trimming whitespace, and removing duplicates.\"\"\"\n    \n    # Select and rename columns\n    df = df[[\"Sentence Utterance\", \"Object\"]].rename(columns={\"Sentence Utterance\": \"input\", \"Object\": \"raw_label\"})\n\n    # Normalize label (lowercase)\n    df[\"clean_label\"] = df[\"raw_label\"].str.lower()\n\n    # Trim white spaces in input column\n    df[\"input\"] = df[\"input\"].str.strip()\n\n    # Remove duplicates based on input\n    df = df.drop_duplicates(subset=\"input\", keep=\"first\")\n\n    # Drop the raw label column\n    df.drop(columns=[\"raw_label\"], inplace=True)\n\n    return df\n\n# Apply cleaning function\ndata_df = clean_data(data_df)\n\n# Display summary\ndisplay(data_df.describe())\ndisplay(data_df[\"clean_label\"].unique())\n","metadata":{"id":"19onNNUZMc54","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:32.614604Z","iopub.execute_input":"2025-02-15T17:31:32.614961Z","iopub.status.idle":"2025-02-15T17:31:32.680314Z","shell.execute_reply.started":"2025-02-15T17:31:32.614928Z","shell.execute_reply":"2025-02-15T17:31:32.679400Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       input clean_label\ncount                                  13367       13367\nunique                                 13367          26\ntop     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\nfreq                                       1        2108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>13367</td>\n      <td>13367</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13367</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>2108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n      dtype=object)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Mapping and Trimming\ndata = data_df.to_numpy()\nunique_label = data_df.clean_label.unique()\n\nlabel_2_num_map = dict(zip(unique_label, range(len(unique_label))))\nnum_2_label_map = dict(zip(range(len(unique_label)), unique_label))\n\ndata[:,1] = np.vectorize(label_2_num_map.get)(data[:,1]) \n\ndef strip_str(string):\n    return string.strip()\ndata[:,0] = np.vectorize(strip_str)(data[:,0])\n\ndisplay(data)","metadata":{"id":"EYzMrvb7nYR2","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:32.681106Z","iopub.execute_input":"2025-02-15T17:31:32.681419Z","iopub.status.idle":"2025-02-15T17:31:32.723201Z","shell.execute_reply.started":"2025-02-15T17:31:32.681398Z","shell.execute_reply":"2025-02-15T17:31:32.722508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"array([['<PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter Services เค้าเช็ต 3276.25 บาท เมื่อวานที่ผมเช็คที่ศูนย์บอกมียอด 3057.79 บาท',\n        0],\n       ['internet ยังความเร็วอยุ่เท่าไหร ครับ', 1],\n       ['ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ', 2],\n       ...,\n       ['ยอดเงินเหลือเท่าไหร่ค่ะ', 7],\n       ['ยอดเงินในระบบ', 7],\n       ['สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ', 1]], dtype=object)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Split\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nSEED = 42\nMIN_INSTANCES = 10  # Minimum instances per class\n\n\ndef filter_data(data_df, min_instances=MIN_INSTANCES):\n    \"\"\"\n    Filters classes with fewer than `min_instances` occurrences.\n    Returns filtered input (X) and labels (y).\n    \"\"\"\n    class_counts = data_df[\"clean_label\"].value_counts()\n    valid_classes = class_counts[class_counts >= min_instances].index\n\n    filtered_data = data_df[data_df[\"clean_label\"].isin(valid_classes)]\n    return filtered_data[\"input\"], filtered_data[\"clean_label\"].astype(int)\n\ndef split_data(data_df, random_state=SEED, min_instances=MIN_INSTANCES):\n    \"\"\"\n    Splits data into train (80%), validation (10%), and test (10%) sets.\n    Ensures stratification and filtering of rare classes.\n    \"\"\"\n    # Filter classes\n    X, y = filter_data(data_df, min_instances)\n\n    # Split 80% Train, 20% Temp\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        X, y, test_size=0.20, stratify=y, random_state=random_state\n    )\n\n    # Split 10% Validation, 10% Test\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state\n    )\n\n    print(f\"Train size: {len(X_train)}\")\n    print(f\"Validation size: {len(X_val)}\")\n    print(f\"Test size: {len(X_test)}\")\n\n    return (\n        np.array(X_train), np.array(X_val), np.array(X_test),\n        np.array(y_train), np.array(y_val), np.array(y_test)\n    )\n\n# Convert to DataFrame\ndf = pd.DataFrame(data, columns=['input', 'clean_label'])\n\n# Split dataset\nX_train, X_val, X_test, y_train, y_val, y_test = split_data(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:32.723982Z","iopub.execute_input":"2025-02-15T17:31:32.724285Z","iopub.status.idle":"2025-02-15T17:31:32.771217Z","shell.execute_reply.started":"2025-02-15T17:31:32.724255Z","shell.execute_reply":"2025-02-15T17:31:32.770495Z"}},"outputs":[{"name":"stdout","text":"Train size: 10690\nValidation size: 1336\nTest size: 1337\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#Model 1 TF-IDF\n\nBuild a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n\nFor this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.","metadata":{"id":"Nx6gllzrnVVU"}},{"cell_type":"code","source":"print(\"TfidfVectorizer + Logistic Regression\")\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom pythainlp.corpus import thai_stopwords\nimport time\n\nthai_stopwords_list = list(thai_stopwords())\n\nvectorizer = TfidfVectorizer(\n    tokenizer=None,  \n    stop_words=thai_stopwords_list,  \n    max_features=5000  \n)\n\nmodel = LogisticRegression(random_state=SEED)\n\ntext_clf = Pipeline([\n    ('tfidf', vectorizer),\n    ('clf', model)\n])\n\nstart_time = time.time()\ntext_clf.fit(X_train, y_train) \nend_time = time.time()\nprint(f\"Training time: {end_time - start_time:.4f} seconds\")\n\ny_pred_train = text_clf.predict(X_train)\ny_pred_val = text_clf.predict(X_val)\ny_pred_test = text_clf.predict(X_test)\n\ntrain_acc = np.mean(y_train.astype(int) == y_pred_train)\nval_acc = np.mean(y_val.astype(int) == y_pred_val)\ntest_acc = np.mean(y_test.astype(int) == y_pred_test)\n\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"id":"9vOqTqmfufsT","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:32.772118Z","iopub.execute_input":"2025-02-15T17:31:32.772401Z","iopub.status.idle":"2025-02-15T17:31:37.864696Z","shell.execute_reply.started":"2025-02-15T17:31:32.772365Z","shell.execute_reply":"2025-02-15T17:31:37.863738Z"}},"outputs":[{"name":"stdout","text":"TfidfVectorizer + Logistic Regression\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กคน', 'กคร', 'กครา', 'กคราว', 'กจะ', 'กช', 'กต', 'กท', 'กทาง', 'กน', 'กระท', 'กระน', 'กระไร', 'กล', 'กว', 'กส', 'กหน', 'กอ', 'กอย', 'กำล', 'กเม', 'กแห', 'กๆ', 'ขณะท', 'ขณะน', 'ขณะหน', 'ขณะเด', 'คงอย', 'คร', 'ครบคร', 'ครบถ', 'คราท', 'คราน', 'คราวก', 'คราวท', 'คราวน', 'คราวหน', 'คราวหล', 'คราวโน', 'คราหน', 'คล', 'งก', 'งกระน', 'งกล', 'งกว', 'งข', 'งคง', 'งคน', 'งครา', 'งคราว', 'งง', 'งจ', 'งจน', 'งจะ', 'งจาก', 'งต', 'งท', 'งน', 'งบ', 'งปวง', 'งมวล', 'งละ', 'งว', 'งส', 'งหน', 'งหมด', 'งหมาย', 'งหล', 'งหลาย', 'งอย', 'งเก', 'งเคย', 'งเน', 'งเป', 'งเม', 'งแก', 'งแต', 'งแม', 'งแล', 'งโง', 'งโน', 'งใด', 'งใหญ', 'งไง', 'งได', 'งไหน', 'งๆ', 'งๆจ', 'จก', 'จจ', 'จนกระท', 'จนกว', 'จนขณะน', 'จนถ', 'จนท', 'จนบ', 'จนเม', 'จนแม', 'จร', 'จรดก', 'จวนเจ', 'จวบก', 'จส', 'จสมบ', 'จะได', 'จากน', 'จำเป', 'จแล', 'ฉะน', 'ซะก', 'ซะจนกระท', 'ซะจนถ', 'ณๆ', 'ดการ', 'ดงาน', 'ดดล', 'ดต', 'ดทำ', 'ดน', 'ดว', 'ดหน', 'ดหา', 'ดเด', 'ดเผย', 'ดแจง', 'ดให', 'ดไป', 'ดๆ', 'ตลอดถ', 'ตลอดท', 'ตลอดป', 'ตลอดว', 'ตามด', 'ตามท', 'ตามแต', 'ทว', 'ทำให', 'นก', 'นการ', 'นกาลนาน', 'นควร', 'นจะ', 'นด', 'นต', 'นท', 'นน', 'นนะ', 'นนาน', 'นมา', 'นมาก', 'นย', 'นยง', 'นยาว', 'นละ', 'นว', 'นวาน', 'นอ', 'นอกจากท', 'นอกจากน', 'นอกจากว', 'นอกน', 'นอกเหน', 'นอาท', 'นา', 'นเคย', 'นเด', 'นเถอะ', 'นเน', 'นเป', 'นเพ', 'นเพราะ', 'นเพราะว', 'นเม', 'นเอง', 'นแก', 'นแต', 'นและก', 'นแหละ', 'นใด', 'นใดน', 'นไง', 'นได', 'นไป', 'นไร', 'นไว', 'นไหน', 'นไหม', 'นๆ', 'บจากน', 'บต', 'บรอง', 'บว', 'บอกว', 'บอกแล', 'บางกว', 'บางคร', 'บางท', 'บางแห', 'บเน', 'บแต', 'ปฏ', 'ปร', 'ประการฉะน', 'ประการหน', 'ปรากฏว', 'พบว', 'พร', 'พวกก', 'พวกค', 'พวกฉ', 'พวกท', 'พวกน', 'พวกม', 'พวกโน', 'พอก', 'พอด', 'พอต', 'พอท', 'พอเพ', 'พอแล', 'ภายภาคหน', 'ภายหน', 'ภายหล', 'ภายใต', 'มก', 'มองว', 'มากกว', 'มเต', 'มไปด', 'มไปหมด', 'มๆ', 'ยกให', 'ยง', 'ยงพอ', 'ยงว', 'ยงเพ', 'ยงเพราะ', 'ยงแค', 'ยงแต', 'ยงใด', 'ยงไร', 'ยงไหน', 'ยจน', 'ยจนกระท', 'ยจนถ', 'ยด', 'ยน', 'ยนะ', 'ยนแปลง', 'ยบ', 'ยย', 'ยล', 'ยว', 'ยวก', 'ยวข', 'ยวน', 'ยวเน', 'ยวๆ', 'ยอมร', 'ยเน', 'ยเอง', 'ยแล', 'ยโน', 'รณ', 'รวดเร', 'รวมก', 'รวมด', 'รวมถ', 'รวมท', 'ระหว', 'วก', 'วง', 'วงก', 'วงต', 'วงถ', 'วงท', 'วงน', 'วงระหว', 'วงหน', 'วงหล', 'วงแรก', 'วงๆ', 'วถ', 'วท', 'วน', 'วนจน', 'วนด', 'วนท', 'วนน', 'วนมาก', 'วนเก', 'วนแต', 'วนใด', 'วนใหญ', 'วม', 'วมก', 'วมด', 'วมม', 'วย', 'วยก', 'วยท', 'วยประการฉะน', 'วยว', 'วยเช', 'วยเพราะ', 'วยเหต', 'วยเหม', 'วเสร', 'วแต', 'วๆ', 'สม', 'สำค', 'หมดก', 'หมดส', 'หร', 'หล', 'หากว', 'หากแม', 'หาร', 'หาใช', 'อก', 'อค', 'อคร', 'อคราว', 'อคราวก', 'อคราวท', 'อง', 'องจาก', 'องมาจาก', 'อจะ', 'อจาก', 'อด', 'อถ', 'อท', 'อน', 'อนก', 'อนข', 'อนมาทาง', 'อนว', 'อนหน', 'อนๆ', 'อบ', 'อบจะ', 'อบๆ', 'อม', 'อมก', 'อมด', 'อมท', 'อมเพ', 'อย', 'อยกว', 'อยคร', 'อยจะ', 'อยเป', 'อยไปทาง', 'อยๆ', 'อว', 'อวาน', 'อาจเป', 'อเก', 'อเช', 'อเปล', 'อเม', 'อเย', 'อใด', 'อให', 'อไง', 'อไป', 'อไม', 'อไร', 'อไหร', 'าก', 'าง', 'างก', 'างขวาง', 'างจะ', 'างด', 'างต', 'างท', 'างน', 'างบน', 'างมาก', 'างย', 'างล', 'างละ', 'างหน', 'างหาก', 'างเค', 'างเช', 'างเด', 'างโน', 'างใด', 'างไร', 'างไรก', 'างไรเส', 'างไหน', 'างๆ', 'าจะ', 'าท', 'าน', 'านาน', 'านๆ', 'าพเจ', 'าย', 'ายก', 'ายว', 'ายใด', 'ายๆ', 'าว', 'าวค', 'าส', 'าหร', 'าหาก', 'าฯ', 'าใจ', 'าใด', 'าให', 'าไร', 'าไหร', 'าๆ', 'เก', 'เข', 'เฉกเช', 'เช', 'เด', 'เต', 'เถ', 'เท', 'เน', 'เป', 'เปล', 'เผ', 'เพ', 'เพราะฉะน', 'เพราะว', 'เม', 'เร', 'เล', 'เส', 'เสม', 'เสร', 'เห', 'เหต', 'เหล', 'เอ', 'แค', 'แด', 'แต', 'แท', 'แน', 'แม', 'แล', 'แสดงว', 'แห', 'แหล', 'ใกล', 'ใช', 'ใด', 'ใต', 'ในช', 'ในท', 'ในระหว', 'ในเม', 'ให', 'ใหญ', 'ใหม', 'ไข', 'ได', 'ไม', 'ไว', 'ไหม'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training time: 2.2299 seconds\nTrain Accuracy: 0.7574\nValidation Accuracy: 0.6257\nTest Accuracy: 0.6178\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"TfidfVectorizer + Logistic Regression + pythainlp.word_tokenize\")\nfrom pythainlp.tokenize import word_tokenize\n\nthai_stopwords_list = list(thai_stopwords())\n\n\ndef thai_tokenizer(text):\n    return word_tokenize(text, keep_whitespace=False)\n\n\nvectorizer = TfidfVectorizer(\n    tokenizer=thai_tokenizer,  \n    stop_words=thai_stopwords_list,\n    max_features=5000,\n)\n\nmodel = LogisticRegression(random_state=SEED)\n\ntext_clf = Pipeline([(\"tfidf\", vectorizer), (\"clf\", model)])\n\nstart_time = time.time()\ntext_clf.fit(X_train, y_train)\nend_time = time.time()\nprint(f\"Training time: {end_time - start_time:.4f} seconds\")\n\ny_pred_train = text_clf.predict(X_train)\ny_pred_val = text_clf.predict(X_val)\ny_pred_test = text_clf.predict(X_test)\n\ntrain_acc = np.mean(y_train.astype(int) == y_pred_train)\nval_acc = np.mean(y_val.astype(int) == y_pred_val)\ntest_acc = np.mean(y_test.astype(int) == y_pred_test)\n\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:37.865845Z","iopub.execute_input":"2025-02-15T17:31:37.866137Z","iopub.status.idle":"2025-02-15T17:31:42.349913Z","shell.execute_reply.started":"2025-02-15T17:31:37.866115Z","shell.execute_reply":"2025-02-15T17:31:42.349060Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กระไร', 'กาลนาน', 'ชิ้น', 'ดังที่', 'ดี', 'ดีกว่า', 'ด้อย', 'ตัว', 'ต่อไป', 'ถัดไป', 'ทั่วถึง', 'ทำ', 'ที่จะ', 'ท่าน', 'ท้าย', 'นา', 'บอ', 'บัด', 'ระยะเวลา', 'ล่ะ', 'วันวาน', 'สม', 'สมบูรณ์', 'สํา', 'หน้า', 'หรับ', 'หา', 'อย', 'เกี่ยว', 'เก่า', 'เดี๋ยวนี้', 'เย็น', 'เล่า', 'เสมือน', 'เหมือนกัน', 'แด่', 'แม้น', 'แหล่', 'โง้น', 'โน้น', 'ใด', 'ไว', 'ไหม', '\\ufeff'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"TfidfVectorizer + Logistic Regression + pythainlp.word_tokenize\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Training time: 2.8185 seconds\nTrain Accuracy: 0.7675\nValidation Accuracy: 0.6886\nTest Accuracy: 0.6933\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def calculate_oov_words(X_train, X_test):\n    \"\"\"\n    Calculates the number of out-of-vocabulary (OOV) words in the test set \n    compared to the training set.\n    \n    Parameters:\n        X_train (list): List of training texts.\n        X_test (list): List of test texts.\n    \n    Returns:\n        int: Number of OOV words.\n    \"\"\"\n    train_words = {word for text in X_train for word in text.split()}\n    test_words = {word for text in X_test for word in text.split()}\n\n    oov_words = test_words - train_words\n    oov_count = len(oov_words)\n\n    return oov_count\n\n# Compute OOV words\noov_count = calculate_oov_words(X_train, X_test)\nprint(oov_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:42.350859Z","iopub.execute_input":"2025-02-15T17:31:42.351190Z","iopub.status.idle":"2025-02-15T17:31:42.369705Z","shell.execute_reply.started":"2025-02-15T17:31:42.351158Z","shell.execute_reply":"2025-02-15T17:31:42.368806Z"}},"outputs":[{"name":"stdout","text":"2245\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Below are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n\n\nWhat tokenizer will you use and why?\n\nA: I will use pythainlp.word_tokenize because it is specifically designed for processing Thai text. The results show that it provides:\n- Higher accuracy across all datasets.\n- Better generalization, as indicated by improved validation accuracy (val_acc).\n\nWill you remove stopwords (e.g., \"a,\" \"an,\" \"the,\" \"to\" in English) in your TF-IDF process? Is it necessary?\n\nA: Yes, I will remove Thai stopwords using pythainlp.thai_stopwords(). Eliminating these common but non-informative words helps enhance model efficiency by reducing noise and improving classification performance.\n\nThe dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n\nA: 2245","metadata":{}},{"cell_type":"markdown","source":"# Comparison\n\nAfter you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville.","metadata":{"id":"Qr9_0DnMBcFZ"}}]}