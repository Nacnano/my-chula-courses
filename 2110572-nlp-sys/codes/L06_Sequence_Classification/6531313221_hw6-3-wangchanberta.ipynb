{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HOMEWORK 6: TEXT CLASSIFICATION\nIn this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n\nWe will focus only on the Object Classification task for this homework.\n\nIn this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n\nYou will need to build 3 different models.\n\n1. A model based on tf-idf\n2. A model based on MUSE\n3. A model based on wangchanBERTa\n\n**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n\nThis homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification.","metadata":{"id":"VQ8FRFIYMc5X"}},{"cell_type":"code","source":"!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv","metadata":{"id":"kHqkFSyaNvOt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"879b17f1-0fb2-455c-ca37-b5a4aecd7b1c","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:01.467049Z","iopub.execute_input":"2025-02-15T17:48:01.467419Z","iopub.status.idle":"2025-02-15T17:48:02.802367Z","shell.execute_reply.started":"2025-02-15T17:48:01.467390Z","shell.execute_reply":"2025-02-15T17:48:02.801351Z"}},"outputs":[{"name":"stdout","text":"--2025-02-15 17:48:01--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n--2025-02-15 17:48:01--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com/cd/0/inline/CkLXTGTTsPIvZrDlmR9Omr_SLvsQ8MPpi0L6OgiT8lVd90KrH9gwzq7iswv2Whu7T0FnU9D4Ma4nqkK6-YeJ_dUKI_BG_wnrlqRXjoGs6PI8u_YSHuB72Dq87BlEE13c_S8/file# [following]\n--2025-02-15 17:48:02--  https://ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com/cd/0/inline/CkLXTGTTsPIvZrDlmR9Omr_SLvsQ8MPpi0L6OgiT8lVd90KrH9gwzq7iswv2Whu7T0FnU9D4Ma4nqkK6-YeJ_dUKI_BG_wnrlqRXjoGs6PI8u_YSHuB72Dq87BlEE13c_S8/file\nResolving ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com (ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com (ucf7cfebc37547e840b4907af352.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2518977 (2.4M) [text/plain]\nSaving to: ‘clean-phone-data-for-students.csv.1’\n\nclean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.04s   \n\n2025-02-15 17:48:02 (53.9 MB/s) - ‘clean-phone-data-for-students.csv.1’ saved [2518977/2518977]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pythainlp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRlx5Mb5zkXw","outputId":"18d913e0-aa6d-435b-931d-591386cb4ba8","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:02.803777Z","iopub.execute_input":"2025-02-15T17:48:02.804042Z","iopub.status.idle":"2025-02-15T17:48:06.039895Z","shell.execute_reply.started":"2025-02-15T17:48:02.804020Z","shell.execute_reply":"2025-02-15T17:48:06.038879Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pythainlp in /usr/local/lib/python3.10/dist-packages (5.0.5)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Import Libs","metadata":{"id":"2YprqbOPMc5a"}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom torch.utils.data import Dataset\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"heICP79cMc5e","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:06.041639Z","iopub.execute_input":"2025-02-15T17:48:06.041916Z","iopub.status.idle":"2025-02-15T17:48:06.047734Z","shell.execute_reply.started":"2025-02-15T17:48:06.041882Z","shell.execute_reply":"2025-02-15T17:48:06.047061Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb_api_key\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nimport wandb\nwandb.login(key=secret_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:06.048915Z","iopub.execute_input":"2025-02-15T17:48:06.049198Z","iopub.status.idle":"2025-02-15T17:48:14.580230Z","shell.execute_reply.started":"2025-02-15T17:48:06.049167Z","shell.execute_reply":"2025-02-15T17:48:14.579514Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnacnano\u001b[0m (\u001b[33mnacnano2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data_df = pd.read_csv('clean-phone-data-for-students.csv')","metadata":{"id":"JhZ2eBAWMc5l","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:14.581040Z","iopub.execute_input":"2025-02-15T17:48:14.581425Z","iopub.status.idle":"2025-02-15T17:48:14.640752Z","shell.execute_reply.started":"2025-02-15T17:48:14.581403Z","shell.execute_reply":"2025-02-15T17:48:14.639921Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def clean_data(df):\n    \"\"\"Cleans the dataset by selecting relevant columns, normalizing labels, \n    trimming whitespace, and removing duplicates.\"\"\"\n    \n    # Select and rename columns\n    df = df[[\"Sentence Utterance\", \"Object\"]].rename(columns={\"Sentence Utterance\": \"input\", \"Object\": \"raw_label\"})\n\n    # Normalize label (lowercase)\n    df[\"clean_label\"] = df[\"raw_label\"].str.lower()\n\n    # Trim white spaces in input column\n    df[\"input\"] = df[\"input\"].str.strip()\n\n    # Remove duplicates based on input\n    df = df.drop_duplicates(subset=\"input\", keep=\"first\")\n\n    # Drop the raw label column\n    df.drop(columns=[\"raw_label\"], inplace=True)\n\n    return df\n\n# Apply cleaning function\ndata_df = clean_data(data_df)\n\n# Display summary\ndisplay(data_df.describe())\ndisplay(data_df[\"clean_label\"].unique())\n","metadata":{"id":"19onNNUZMc54","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:14.641441Z","iopub.execute_input":"2025-02-15T17:48:14.641687Z","iopub.status.idle":"2025-02-15T17:48:14.700238Z","shell.execute_reply.started":"2025-02-15T17:48:14.641666Z","shell.execute_reply":"2025-02-15T17:48:14.699637Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       input clean_label\ncount                                  13367       13367\nunique                                 13367          26\ntop     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\nfreq                                       1        2108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>13367</td>\n      <td>13367</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13367</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>2108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n      dtype=object)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Mapping and Trimming\ndata = data_df.to_numpy()\nunique_label = data_df.clean_label.unique()\n\nlabel_2_num_map = dict(zip(unique_label, range(len(unique_label))))\nnum_2_label_map = dict(zip(range(len(unique_label)), unique_label))\n\ndata[:,1] = np.vectorize(label_2_num_map.get)(data[:,1]) \n\ndef strip_str(string):\n    return string.strip()\ndata[:,0] = np.vectorize(strip_str)(data[:,0])\n\ndisplay(data)","metadata":{"id":"EYzMrvb7nYR2","execution":{"iopub.status.busy":"2025-02-15T17:48:14.700897Z","iopub.execute_input":"2025-02-15T17:48:14.701087Z","iopub.status.idle":"2025-02-15T17:48:14.742079Z","shell.execute_reply.started":"2025-02-15T17:48:14.701070Z","shell.execute_reply":"2025-02-15T17:48:14.741428Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"array([['<PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter Services เค้าเช็ต 3276.25 บาท เมื่อวานที่ผมเช็คที่ศูนย์บอกมียอด 3057.79 บาท',\n        0],\n       ['internet ยังความเร็วอยุ่เท่าไหร ครับ', 1],\n       ['ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ', 2],\n       ...,\n       ['ยอดเงินเหลือเท่าไหร่ค่ะ', 7],\n       ['ยอดเงินในระบบ', 7],\n       ['สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ', 1]], dtype=object)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Split\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nSEED = 42\nMIN_INSTANCES = 10  # Minimum instances per class\n\n\ndef filter_data(data_df, min_instances=MIN_INSTANCES):\n    \"\"\"\n    Filters classes with fewer than `min_instances` occurrences.\n    Returns filtered input (X) and labels (y).\n    \"\"\"\n    class_counts = data_df[\"clean_label\"].value_counts()\n    valid_classes = class_counts[class_counts >= min_instances].index\n\n    filtered_data = data_df[data_df[\"clean_label\"].isin(valid_classes)]\n    return filtered_data[\"input\"], filtered_data[\"clean_label\"].astype(int)\n\ndef split_data(data_df, random_state=SEED, min_instances=MIN_INSTANCES):\n    \"\"\"\n    Splits data into train (80%), validation (10%), and test (10%) sets.\n    Ensures stratification and filtering of rare classes.\n    \"\"\"\n    # Filter classes\n    X, y = filter_data(data_df, min_instances)\n\n    # Split 80% Train, 20% Temp\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        X, y, test_size=0.20, stratify=y, random_state=random_state\n    )\n\n    # Split 10% Validation, 10% Test\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state\n    )\n\n    print(f\"Train size: {len(X_train)}\")\n    print(f\"Validation size: {len(X_val)}\")\n    print(f\"Test size: {len(X_test)}\")\n\n    return (\n        np.array(X_train), np.array(X_val), np.array(X_test),\n        np.array(y_train), np.array(y_val), np.array(y_test)\n    )\n\n# Convert to DataFrame\ndf = pd.DataFrame(data, columns=['input', 'clean_label'])\n\n# Split dataset\nX_train, X_val, X_test, y_train, y_val, y_test = split_data(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:14.742758Z","iopub.execute_input":"2025-02-15T17:48:14.742967Z","iopub.status.idle":"2025-02-15T17:48:14.776807Z","shell.execute_reply.started":"2025-02-15T17:48:14.742949Z","shell.execute_reply":"2025-02-15T17:48:14.776137Z"}},"outputs":[{"name":"stdout","text":"Train size: 10690\nValidation size: 1336\nTest size: 1337\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Model 3 WangchanBERTa\n\nWe ask you to train a WangchanBERTa-based model.\n\nWe recommend you use the thaixtransformers fork (which we used in the PoS homework).\nhttps://github.com/PyThaiNLP/thaixtransformers\n\nThe structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n\nWhich WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n\n**Ans:**  I will use airesearch/wangchanberta-base-att-spm-uncased because:\n\n- It is specifically trained for Thai text, making it well-suited for Thai NLP tasks.\n- It uses SentencePiece tokenization, which is more effective for Thai than space-based tokenization.\n- It achieves state-of-the-art performance for Thai text classification tasks.\n","metadata":{"id":"ZDHfX377rnp_"}},{"cell_type":"code","source":"for i in range(len(data)):\n    data[i][0] = data[i][0].replace('ํา', \"ำ\")","metadata":{"id":"ZI8SvILyub0m","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:14.778884Z","iopub.execute_input":"2025-02-15T17:48:14.779097Z","iopub.status.idle":"2025-02-15T17:48:14.790997Z","shell.execute_reply.started":"2025-02-15T17:48:14.779079Z","shell.execute_reply":"2025-02-15T17:48:14.790128Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = \"cuda\"\n\nimport time\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score\n\nstart_time = time.time()\n\nMODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True) \n\ntrain_data = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train.tolist()})\nval_data = Dataset.from_dict({\"text\": X_val.tolist(), \"label\": y_val.tolist()})\ntest_data = Dataset.from_dict({\"text\": X_test.tolist(), \"label\": y_test.tolist()})\n\ntrain_dataset = train_data.map(tokenize_function, batched=True)\nval_dataset = val_data.map(tokenize_function, batched=True)\ntest_dataset = test_data.map(tokenize_function, batched=True)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n                                                           num_labels=len(num_2_label_map),\n                                                           id2label=num_2_label_map,\n                                                           label2id=label_2_num_map)\nmodel.to(device)\n\nEPOCHS = 5\nBATCH_SIZE = 32\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    logging_dir=\"./logs\",\n    logging_steps=500,\n)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\"accuracy\": accuracy_score(labels, preds)}\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator  \n)\n\ntrainer.train()\n\ntrain_results = trainer.evaluate(train_dataset)\nval_results = trainer.evaluate(val_dataset)\ntest_results = trainer.evaluate(test_dataset)\n\nprint(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")\nprint(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\nend_time = time.time()\nprint(f\"Total Time: {end_time - start_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:48:14.791931Z","iopub.execute_input":"2025-02-15T17:48:14.792132Z","iopub.status.idle":"2025-02-15T17:54:44.184479Z","shell.execute_reply.started":"2025-02-15T17:48:14.792100Z","shell.execute_reply":"2025-02-15T17:54:44.183753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ea3540a727436787ee34d3c404f852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa773fd42be8403282f7eb69ef3c09b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660dcd1b021d4449a05a3490f554e038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10690 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a55f96b3aa24c37a958355319b5757a"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1336 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837981bd023a4fddb829abe30d4c802f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1337 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843d62ab99544e34b3a5977520d59360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba07c9372475420184a2970124361f76"}},"metadata":{}},{"name":"stderr","text":"Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250215_174839-o66qevg6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nacnano2/huggingface/runs/o66qevg6' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nacnano2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nacnano2/huggingface' target=\"_blank\">https://wandb.ai/nacnano2/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nacnano2/huggingface/runs/o66qevg6' target=\"_blank\">https://wandb.ai/nacnano2/huggingface/runs/o66qevg6</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [840/840 05:31, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.363287</td>\n      <td>0.324102</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.466080</td>\n      <td>0.568862</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.025600</td>\n      <td>1.029604</td>\n      <td>0.706587</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.025600</td>\n      <td>0.891187</td>\n      <td>0.750749</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.025600</td>\n      <td>0.859678</td>\n      <td>0.754491</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='420' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [168/168 00:49]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 0.7821\nValidation Accuracy: 0.7545\nTest Accuracy: 0.7472\nTotal Time: 369.9648 seconds\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_results = trainer.evaluate(train_dataset)\nval_results = trainer.evaluate(val_dataset)\ntest_results = trainer.evaluate(test_dataset)\n\nprint(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")\nprint(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:54:44.185273Z","iopub.execute_input":"2025-02-15T17:54:44.185579Z","iopub.status.idle":"2025-02-15T17:55:09.114675Z","shell.execute_reply.started":"2025-02-15T17:54:44.185550Z","shell.execute_reply":"2025-02-15T17:55:09.113825Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 0.7821\nValidation Accuracy: 0.7545\nTest Accuracy: 0.7472\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Comparison\n\nAfter you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville.\n\n## TF-IDF\n\nTraining time: 2.8185 seconds\nTrain Accuracy: 0.7675\nValidation Accuracy: 0.6886\nTest Accuracy: 0.6933\n\n## MUSE\n\nEncoding Time: 21.6718 seconds\nTraining Time: 2.2648 seconds\nTrain Accuracy: 0.7373\nValidation Accuracy: 0.7073\nTest Accuracy: 0.7023\nTotal Time: 38.5585 seconds\n\n## wangchanberta\n\nTrain Accuracy: 0.7821\nValidation Accuracy: 0.7545\nTest Accuracy: 0.7472\nTotal Time: 369.9648 seconds\n\nQ: Based on the performance of the three models, which one do you think is best for this use case (Callcenter Chatbot)?\n\nA: WangchanBERTa\n\n- Highest accuracy: Call center questions are often repetitive, so accuracy is a priority.\n- Better generalization: It can handle a variety of customer queries effectively.\n- Deep contextual understanding: Helps interpret nuances and variations in Thai language.\n- Inference speed trade-off: While inference is slower than traditional models, it can be optimized with increased computation, and real-time speed is not as critical as accuracy in this case.","metadata":{"id":"Qr9_0DnMBcFZ"}}]}