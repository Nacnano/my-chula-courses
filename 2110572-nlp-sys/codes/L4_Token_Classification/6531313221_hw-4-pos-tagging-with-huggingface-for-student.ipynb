{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW 4 - POS Tagging with Hugging Face\n\nIn this exercise, you will create a part-of-speech (POS) tagging system for Thai text using NECTEC’s ORCHID corpus. Instead of building your own deep learning architecture from scratch, you will leverage a pretrained tokenizer and a pretrained token classification model from Hugging Face.\n\nWe have provided some starter code for data cleaning and preprocessing in this notebook, but feel free to modify those parts to suit your needs. You are welcome to use additional libraries (e.g., scikit-learn) as long as you incorporate the pretrained Hugging Face model. Specifically, you will need to:\n\n1. Load a pretrained tokenizer and token classification model.\n2. Fine-tune it on the ORCHID corpus for POS tagging.\n3. Evaluate and report the performance of your model on the test data.\n\n### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###","metadata":{"id":"UxOTS6n1ikVL"}},{"cell_type":"markdown","source":"## 1. Setup and Preprocessing","metadata":{"id":"bQ1Uqldlj81G"}},{"cell_type":"code","source":"# Install transformers and thai2transformers\n!pip install wandb\n!pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n!pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n!pip install peft==0.10.0","metadata":{"id":"kyb4FhsEEeH8","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:11.451699Z","iopub.execute_input":"2025-02-02T03:32:11.452037Z","iopub.status.idle":"2025-02-02T03:32:24.969675Z","shell.execute_reply.started":"2025-02-02T03:32:11.452008Z","shell.execute_reply":"2025-02-02T03:32:24.968643Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.30.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.27.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2024.11.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (0.13.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.10.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft==0.10.0) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.12.14)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Setup\n\n1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n\n2. `wandb login` and copy paste the API key when prompt","metadata":{"id":"fTgw8WW3BxWZ"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb_api_key\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\n!wandb login $secret_value","metadata":{"id":"4FjMIqhTBfOX","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:24.970932Z","iopub.execute_input":"2025-02-02T03:32:24.971280Z","iopub.status.idle":"2025-02-02T03:32:27.298165Z","shell.execute_reply.started":"2025-02-02T03:32:24.971245Z","shell.execute_reply":"2025-02-02T03:32:27.297176Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import wandb","metadata":{"id":"4qSh7MXZB74z","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:27.299727Z","iopub.execute_input":"2025-02-02T03:32:27.299963Z","iopub.status.idle":"2025-02-02T03:32:27.303613Z","shell.execute_reply.started":"2025-02-02T03:32:27.299942Z","shell.execute_reply":"2025-02-02T03:32:27.302805Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"We encourage you to login to your `Hugging Face` account so you can upload and share your model with the community. When prompted, enter your token to login","metadata":{"id":"i-BR7danGv6W"}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"n7h8NENllZK2","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:27.304735Z","iopub.execute_input":"2025-02-02T03:32:27.305027Z","iopub.status.idle":"2025-02-02T03:32:27.336483Z","shell.execute_reply.started":"2025-02-02T03:32:27.304999Z","shell.execute_reply":"2025-02-02T03:32:27.335598Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4bd14c670164d70a8d47389b940c8a2"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Download the dataset from Hugging Face","metadata":{"id":"XqkDkseilv19"}},{"cell_type":"code","source":"from datasets import load_dataset\n\norchid = load_dataset(\"Thichow/orchid_corpus\")","metadata":{"id":"kRksERXFEngl","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:27.337500Z","iopub.execute_input":"2025-02-02T03:32:27.337829Z","iopub.status.idle":"2025-02-02T03:32:36.211611Z","shell.execute_reply.started":"2025-02-02T03:32:27.337799Z","shell.execute_reply":"2025-02-02T03:32:36.210950Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/79.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed29ab498cbf420c9fdfefb9001cb7e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"orchid_corpus.py:   0%|          | 0.00/7.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60e8e345ce94ebfb7d5185394608748"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for Thichow/orchid_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Thichow/orchid_corpus.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef5fd3b2acc4386a3e254a75fad4660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d324d1d1fdf94daa9f82c1f1b03d06c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591755d0528748c99af42376e41b0925"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1361c21d3c40849034ecfc3ea70971"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"orchid","metadata":{"id":"T_AWd4d5lCYd","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.212355Z","iopub.execute_input":"2025-02-02T03:32:36.212587Z","iopub.status.idle":"2025-02-02T03:32:36.217765Z","shell.execute_reply.started":"2025-02-02T03:32:36.212557Z","shell.execute_reply":"2025-02-02T03:32:36.216916Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"orchid['train'][0]","metadata":{"id":"QNmIqSo0FkAx","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.218732Z","iopub.execute_input":"2025-02-02T03:32:36.219050Z","iopub.status.idle":"2025-02-02T03:32:36.240505Z","shell.execute_reply.started":"2025-02-02T03:32:36.219018Z","shell.execute_reply":"2025-02-02T03:32:36.239761Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1'}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"orchid['train'][0][\"sentence\"]","metadata":{"id":"hUuz3dLGlI_S","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.242762Z","iopub.execute_input":"2025-02-02T03:32:36.242956Z","iopub.status.idle":"2025-02-02T03:32:36.260760Z","shell.execute_reply.started":"2025-02-02T03:32:36.242939Z","shell.execute_reply":"2025-02-02T03:32:36.260042Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'การประชุมทางวิชาการ ครั้งที่ 1'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"''.join(orchid['train'][0]['label_tokens'])","metadata":{"id":"bh7fX19zI85W","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.261931Z","iopub.execute_input":"2025-02-02T03:32:36.262213Z","iopub.status.idle":"2025-02-02T03:32:36.279582Z","shell.execute_reply.started":"2025-02-02T03:32:36.262192Z","shell.execute_reply":"2025-02-02T03:32:36.279002Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'การประชุมทางวิชาการ ครั้งที่ 1'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"label_list = orchid[\"train\"].features[f\"pos_tags\"].feature.names\nprint('total type of pos_tags :', len(label_list))\nprint(label_list)","metadata":{"id":"38jM9YcSFmjV","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.280165Z","iopub.execute_input":"2025-02-02T03:32:36.280398Z","iopub.status.idle":"2025-02-02T03:32:36.298489Z","shell.execute_reply.started":"2025-02-02T03:32:36.280378Z","shell.execute_reply":"2025-02-02T03:32:36.297871Z"}},"outputs":[{"name":"stdout","text":"total type of pos_tags : 47\n['ADVI', 'ADVN', 'ADVP', 'ADVS', 'CFQC', 'CLTV', 'CMTR', 'CMTR@PUNC', 'CNIT', 'CVBL', 'DCNM', 'DDAC', 'DDAN', 'DDAQ', 'DDBQ', 'DIAC', 'DIAQ', 'DIBQ', 'DONM', 'EAFF', 'EITT', 'FIXN', 'FIXV', 'JCMP', 'JCRG', 'JSBR', 'NCMN', 'NCNM', 'NEG', 'NLBL', 'NONM', 'NPRP', 'NTTL', 'PDMN', 'PNTR', 'PPRS', 'PREL', 'PUNC', 'RPRE', 'VACT', 'VATT', 'VSTA', 'XVAE', 'XVAM', 'XVBB', 'XVBM', 'XVMM']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nimport numpy.random\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\n#transformers\nfrom transformers import (\n    CamembertTokenizer,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForMaskedLM,\n    AutoModelForSequenceClassification,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n)\n\n#thaixtransformers\nfrom thaixtransformers import Tokenizer\nfrom thaixtransformers.preprocess import process_transformers","metadata":{"id":"Uf_NDWg7F6z_","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:32:36.299236Z","iopub.execute_input":"2025-02-02T03:32:36.299455Z","iopub.status.idle":"2025-02-02T03:33:06.852456Z","shell.execute_reply.started":"2025-02-02T03:32:36.299437Z","shell.execute_reply":"2025-02-02T03:33:06.851492Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e336a17b87445e1b2fd97a4d722bfb5"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"Next, we load a pretrained tokenizer from Hugging Face. In this work, we utilize WangchanBERTa, a Thai-specific pretrained model, as the tokenizer.","metadata":{"id":"T1suScqmntBW"}},{"cell_type":"markdown","source":"# Choose Pretrained Model","metadata":{"id":"jt3ASYUVm54n"}},{"cell_type":"markdown","source":"In this notebook, you can choose from 5 versions of WangchanBERTa, XLMR and mBERT to perform downstream tasks on Thai datasets. The datasets are:\n\n* `wangchanberta-base-att-spm-uncased` (recommended) - Largest WangchanBERTa trained on 78.5GB of Assorted Thai Texts with subword tokenizer SentencePiece\n* `xlm-roberta-base` - Facebook's [XLMR](https://arxiv.org/abs/1911.02116) trained on 100 languages\n* `bert-base-multilingual-cased` - Google's [mBERT](https://arxiv.org/abs/1911.03310) trained on 104 languages\n* `wangchanberta-base-wiki-newmm` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's word-level tokenizer  `newmm`\n* `wangchanberta-base-wiki-syllable` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's syllabel-level tokenizer `syllable`\n* `wangchanberta-base-wiki-sefr` - WangchanBERTa trained on Thai Wikipedia Dump with word-level tokenizer  `SEFR`\n* `wangchanberta-base-wiki-spm` - WangchanBERTa trained on Thai Wikipedia Dump with subword-level tokenizer SentencePiece\n\nIn the first part, we require you to select the wangchanberta-base-att-spm-uncased.","metadata":{"id":"sFBKLqbIm23-"}},{"cell_type":"markdown","source":"<b> Learn more about using wangchanberta at [wangchanberta_getting_started_ai_reseach](https://colab.research.google.com/github/PyThaiNLP/thaixtransformers/blob/main/notebooks/wangchanberta_getting_started_aireseach.ipynb?fbclid=IwY2xjawH61XZleHRuA2FlbQIxMAABHZUaAmHobzmCMHpX0EgdLdjDAEwSX0bjqpo5xPUSIx9b4O_dsIvvG8KVNA_aem_IyKkvzy-VPf9k2pYAFf6Nw#scrollTo=n5IaCot9b3cF) <b>\n\n\n\n*   You need to set the transformers version to transformers==4.30.1.\n\n","metadata":{"id":"6HbZo_TZDn17"}},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"kl2NposVIh9-"}},{"cell_type":"code","source":"model_names = [\n    'airesearch/wangchanberta-base-att-spm-uncased',\n    'airesearch/wangchanberta-base-wiki-newmm',\n    'airesearch/wangchanberta-base-wiki-ssg',\n    'airesearch/wangchanberta-base-wiki-sefr',\n    'airesearch/wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n\n#create tokenizer\ntokenizer = Tokenizer(model_name).from_pretrained(\n                f'{model_name}',\n                revision='main',\n                model_max_length=416,)\n","metadata":{"id":"n5IaCot9b3cF","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:06.853472Z","iopub.execute_input":"2025-02-02T03:33:06.854130Z","iopub.status.idle":"2025-02-02T03:33:07.675080Z","shell.execute_reply.started":"2025-02-02T03:33:06.854103Z","shell.execute_reply":"2025-02-02T03:33:07.674224Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f70a474d4541d196bc3af65aca45fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff4e5776ba14a99a0b7fbef469ca04f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80c6f965cf542fcb4ca227525377aa6"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Let's try using a pretrained tokenizer.","metadata":{"id":"LzdbERHLwd0X"}},{"cell_type":"code","source":"text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\nprint('text :', text)\ntokens = []\nfor i in tokenizer([text], is_split_into_words=True)['input_ids']:\n  tokens.append(tokenizer.decode(i))\nprint('tokens :', tokens)","metadata":{"id":"qwrwXsHFwl-G","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.675998Z","iopub.execute_input":"2025-02-02T03:33:07.676303Z","iopub.status.idle":"2025-02-02T03:33:07.682689Z","shell.execute_reply.started":"2025-02-02T03:33:07.676271Z","shell.execute_reply":"2025-02-02T03:33:07.681813Z"}},"outputs":[{"name":"stdout","text":"text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\ntokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"model : * `wangchanberta-base-att-spm-uncased`\n\nFirst, we print examples of label tokens from our dataset for inspection.","metadata":{"id":"dEQAVqO8pDhK"}},{"cell_type":"code","source":"example = orchid[\"train\"][0]\nfor i in example :\n    print(i, ':', example[i])","metadata":{"id":"Vw_GdRdlpAhu","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.683589Z","iopub.execute_input":"2025-02-02T03:33:07.683907Z","iopub.status.idle":"2025-02-02T03:33:07.701846Z","shell.execute_reply.started":"2025-02-02T03:33:07.683871Z","shell.execute_reply":"2025-02-02T03:33:07.701099Z"}},"outputs":[{"name":"stdout","text":"id : 0\nlabel_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\npos_tags : [21, 39, 26, 26, 37, 4, 18]\nsentence : การประชุมทางวิชาการ ครั้งที่ 1\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Then, we use the sentence 'การประชุมทางวิชาการ<space>ครั้งที่ 1' to be tokenized by the pretrained tokenizer model.","metadata":{"id":"yTuiwEWkppdA"}},{"cell_type":"code","source":"text = 'การประชุมทางวิชาการ ครั้งที่ 1'\ntokenizer(text)","metadata":{"id":"BRCxMtHToN16","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.702803Z","iopub.execute_input":"2025-02-02T03:33:07.703119Z","iopub.status.idle":"2025-02-02T03:33:07.719460Z","shell.execute_reply.started":"2025-02-02T03:33:07.703088Z","shell.execute_reply":"2025-02-02T03:33:07.718654Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"These are already mapped into discrete values. We can uncover the original token text from the tokens by.","metadata":{"id":"kxi8WqZnGa5F"}},{"cell_type":"code","source":"for i in tokenizer(text)['input_ids']:\n  print(tokenizer.convert_ids_to_tokens(i))","metadata":{"id":"optGK_eco3K6","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.720150Z","iopub.execute_input":"2025-02-02T03:33:07.720361Z","iopub.status.idle":"2025-02-02T03:33:07.741921Z","shell.execute_reply.started":"2025-02-02T03:33:07.720341Z","shell.execute_reply":"2025-02-02T03:33:07.741173Z"}},"outputs":[{"name":"stdout","text":"<s>\n▁\nการประชุม\nทางวิชาการ\n<_>\n▁\nครั้งที่\n<_>\n▁\n1\n</s>\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Now let's look at another example.","metadata":{"id":"T3l13UKnwK-d"}},{"cell_type":"code","source":"example = orchid[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"UyfIR3BowU84","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.742778Z","iopub.execute_input":"2025-02-02T03:33:07.743064Z","iopub.status.idle":"2025-02-02T03:33:07.760829Z","shell.execute_reply.started":"2025-02-02T03:33:07.743032Z","shell.execute_reply":"2025-02-02T03:33:07.760268Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (Bilingual transfer dictionary)\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', '<unk>', 'i', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'Bilingual transfer dictionary', ')']\nlabel pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Notice how `B` becomes an ``<unk>`` token. This is because this is an uncased model, meaning it only handles small English characters.","metadata":{"id":"cmV6M-vAwew5"}},{"cell_type":"markdown","source":"# #TODO 0\n\nConvert the dataset to lowercase.","metadata":{"id":"WniJR47ww7a0"}},{"cell_type":"code","source":"# Create a lowercase dataset for uncased BERT\ndef lower_case_sentences(examples):\n  # fill code here to lower case the \"sentence\" and \"label_tokens\"\n  lower_cased_examples = examples\n  lower_cased_examples['sentence'] = lower_cased_examples['sentence'].lower()\n  lower_cased_examples['label_tokens'] = [t.lower() for t in lower_cased_examples['label_tokens']]\n\n  return lower_cased_examples","metadata":{"id":"RQWm_iWBxFQ8","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.761680Z","iopub.execute_input":"2025-02-02T03:33:07.762030Z","iopub.status.idle":"2025-02-02T03:33:07.781389Z","shell.execute_reply.started":"2025-02-02T03:33:07.761999Z","shell.execute_reply":"2025-02-02T03:33:07.780604Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"orchidl = orchid.map(lower_case_sentences)","metadata":{"id":"ndBIqEpWuqBP","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:07.782304Z","iopub.execute_input":"2025-02-02T03:33:07.782584Z","iopub.status.idle":"2025-02-02T03:33:09.661045Z","shell.execute_reply.started":"2025-02-02T03:33:07.782555Z","shell.execute_reply":"2025-02-02T03:33:09.660203Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a01b80e65e4a058ed0128dffe5246d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1d1ca0422a4449ad3678936bc03a56"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"orchidl","metadata":{"id":"z8xpcCqTrqbc","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.662023Z","iopub.execute_input":"2025-02-02T03:33:09.662368Z","iopub.status.idle":"2025-02-02T03:33:09.667223Z","shell.execute_reply.started":"2025-02-02T03:33:09.662322Z","shell.execute_reply":"2025-02-02T03:33:09.666435Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"orchidl[\"train\"][1899]","metadata":{"id":"ecpDHyTPv2py","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.667937Z","iopub.execute_input":"2025-02-02T03:33:09.668172Z","iopub.status.idle":"2025-02-02T03:33:09.708101Z","shell.execute_reply.started":"2025-02-02T03:33:09.668152Z","shell.execute_reply":"2025-02-02T03:33:09.707289Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'id': '1899',\n 'label_tokens': ['โดย',\n  'พิจารณา',\n  'จาก',\n  'พจนานุกรม',\n  'ภาษา',\n  'คู่',\n  ' ',\n  '(',\n  'bilingual transfer dictionary',\n  ')'],\n 'pos_tags': [25, 39, 38, 26, 26, 5, 37, 37, 26, 37],\n 'sentence': 'โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"Now let's examine the labels again.","metadata":{"id":"rgV4ohz2xTY9"}},{"cell_type":"code","source":"example = orchidl[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"DoUDQzM7q265","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.708773Z","iopub.execute_input":"2025-02-02T03:33:09.708958Z","iopub.status.idle":"2025-02-02T03:33:09.730283Z","shell.execute_reply.started":"2025-02-02T03:33:09.708942Z","shell.execute_reply":"2025-02-02T03:33:09.729618Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\nlabel pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"example = orchidl[\"train\"][0]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"aEHgBeX7fQFt","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.734440Z","iopub.execute_input":"2025-02-02T03:33:09.734657Z","iopub.status.idle":"2025-02-02T03:33:09.755039Z","shell.execute_reply.started":"2025-02-02T03:33:09.734638Z","shell.execute_reply":"2025-02-02T03:33:09.754166Z"}},"outputs":[{"name":"stdout","text":"sentence : การประชุมทางวิชาการ ครั้งที่ 1\ntokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\nlabel tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\nlabel pos : [21, 39, 26, 26, 37, 4, 18]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"In the example above, tokens refer to those tokenized using the pretrained tokenizer, while label tokens refer to tokens tokenized from our dataset.","metadata":{"id":"5dVcLxYbrl4E"}},{"cell_type":"markdown","source":"**Do you see something?**\n\nYes, the tokens from the two tokenizers do not match.\n\n- sentence : `การประชุมทางวิชาการ ครั้งที่ 1`\n\n---\n\n- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n\n\n---\n\n\n- label tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n- label pos : `[21, 39, 26, 26, 37, 4, 18]`\n\nYou can see that in our label tokens, 'การ' has a POS tag of 21, and 'ประชุม' has a POS tag of 39. However, when we tokenize the sentence using WangchanBERTa, we get the token 'การประชุม'. What POS tag should we assign to this new token?\n\n**What should we do ?**\n\nBased on this example, we found that the tokens from the WangchanBERTa do not directly align with our label tokens. This means we cannot directly use the label POS tags. Therefore, we need to reassign POS tags to the tokens produced by WangchanBERTa tokenization. The method we will use is majority voting:\n- If a token from the WangchanBERTa matches a label token exactly, we will directly assign the POS tag from the label POS.\n- If the token generated overlaps or combines multiple label tokens, we assign the POS tag based on the number of characters in each token: If the token contains the most characters from any label token, we assign the POS tag from that label token.\n\n**Example :**\n\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.","metadata":{"id":"r1inxbOYuBpB"}},{"cell_type":"markdown","source":"# #TODO 1","metadata":{"id":"jTkgye8K8sd8"}},{"cell_type":"markdown","source":"`**Warning: Please be careful of <unk>, an unknown word token.**`\n\n`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`\n\nAssigning the label -100 to the special tokens `[<s>]` and `[</s>]` and `[_]`  so they’re ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): ignore_index)","metadata":{"id":"lgU8Nudh2rUJ"}},{"cell_type":"code","source":"def majority_vote_pos(examples):\n\n    ####################################################################################################################\n    # TO DO: Since the tokens from the output of the pretrained tokenizer\n    # do not match the tokens in the label tokens of the dataset,\n    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n    # the POS tag from the segment with the greater number of characters should be assigned.\n    #\n    # Example :\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n    #\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n\n    # tokenize word by pretrained tokenizer\n    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n\n    # FILL CODE HERE\n    label_tokens = examples[\"label_tokens\"]\n    pos_tags = examples[\"pos_tags\"]\n    new_pos_result = []\n    \n    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n    \n    label_idx, i = 0, 0\n    \n    for t in new_tokens:\n        if t in {\"<s>\", \"</s>\", \"▁\"}:\n            new_pos_result.append(-100)\n            continue\n        \n        buffer = \"\"\n        weights = {}\n    \n        # Normalize token\n        t = t.replace('ํา', \"ำ\").replace(\"<_>\", \" \")\n        t = t[1:] if t.startswith(\"▁\") else t\n    \n        # Find the correct label index\n        while label_tokens[label_idx][i] != t[0]:\n            i += 1\n            if i == len(label_tokens[label_idx]):\n                label_idx += 1\n                i = 0\n    \n        # Aggregate POS tags\n        while buffer != t:\n            buffer += label_tokens[label_idx][i]\n            weights[pos_tags[label_idx]] = weights.get(pos_tags[label_idx], 0) + 1\n            i += 1\n            if i == len(label_tokens[label_idx]):\n                label_idx += 1\n                i = 0\n    \n        new_pos_result.append(max(weights, key=weights.get))\n\n    tokenized_inputs['tokens'] = new_tokens\n    tokenized_inputs['labels'] = new_pos_result\n\n    return tokenized_inputs\n    ####################################################################################################################","metadata":{"id":"bdxxiUU69lDx","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.756561Z","iopub.execute_input":"2025-02-02T03:33:09.756772Z","iopub.status.idle":"2025-02-02T03:33:09.765629Z","shell.execute_reply.started":"2025-02-02T03:33:09.756753Z","shell.execute_reply":"2025-02-02T03:33:09.765034Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"tokenized_orchid = orchidl.map(majority_vote_pos)","metadata":{"id":"doFKOhpbGf9N","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:09.766299Z","iopub.execute_input":"2025-02-02T03:33:09.766530Z","iopub.status.idle":"2025-02-02T03:33:21.591025Z","shell.execute_reply.started":"2025-02-02T03:33:09.766500Z","shell.execute_reply":"2025-02-02T03:33:21.590067Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d02d617b95b4d9bae1ed79b5d839164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9151406448494aaa5af45521b356e8"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"tokenized_orchid","metadata":{"id":"uvdDnWeOJYpv","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.591924Z","iopub.execute_input":"2025-02-02T03:33:21.592194Z","iopub.status.idle":"2025-02-02T03:33:21.597511Z","shell.execute_reply.started":"2025-02-02T03:33:21.592171Z","shell.execute_reply":"2025-02-02T03:33:21.596422Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"tokenized_orchid['train'][0]","metadata":{"id":"ojrRF85dJbwf","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.598402Z","iopub.execute_input":"2025-02-02T03:33:21.598599Z","iopub.status.idle":"2025-02-02T03:33:21.641226Z","shell.execute_reply.started":"2025-02-02T03:33:21.598578Z","shell.execute_reply":"2025-02-02T03:33:21.640485Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1',\n 'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'tokens': ['<s>',\n  '▁',\n  'การประชุม',\n  'ทางวิชาการ',\n  '<_>',\n  '▁',\n  'ครั้งที่',\n  '<_>',\n  '▁',\n  '1',\n  '</s>'],\n 'labels': [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]}"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"example = tokenized_orchid[\"train\"][0]\nfor i in example :\n    print(i, \":\", example[i])","metadata":{"id":"KMfzFnjSdCGI","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.642099Z","iopub.execute_input":"2025-02-02T03:33:21.642421Z","iopub.status.idle":"2025-02-02T03:33:21.662450Z","shell.execute_reply.started":"2025-02-02T03:33:21.642389Z","shell.execute_reply":"2025-02-02T03:33:21.661798Z"}},"outputs":[{"name":"stdout","text":"id : 0\nlabel_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\npos_tags : [21, 39, 26, 26, 37, 4, 18]\nsentence : การประชุมทางวิชาการ ครั้งที่ 1\ninput_ids : [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\nlabels : [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"This is the result after we realigned the POS based on the majority vote.\n- label_tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n- pos_tags : `[21, 39, 26, 26, 37, 4, 18]`\n- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n- labels : `[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]`\n\n`['<s>', '▁', '</s>'] : -100`\n\n**Check :**\n\n> \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n\n\n> \"การ\" has a POS tag of 21,\n\n> and \"ประชุม\" has a POS tag of 39.\n\n> Therefore, the POS tag for \"การประชุม\" is 39,\n\n> as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n\n\n\n","metadata":{"id":"9lhsQcdL6H3J"}},{"cell_type":"code","source":"# hard test case\nexample = tokenized_orchid[\"train\"][1899]\nfor i in example :\n    print(i, \":\", example[i])","metadata":{"id":"iOE5CEgZdO9c","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.663101Z","iopub.execute_input":"2025-02-02T03:33:21.663305Z","iopub.status.idle":"2025-02-02T03:33:21.682101Z","shell.execute_reply.started":"2025-02-02T03:33:21.663286Z","shell.execute_reply":"2025-02-02T03:33:21.681050Z"}},"outputs":[{"name":"stdout","text":"id : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"Expected output\n\n\n```\nid : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n```","metadata":{"id":"Fv_NkVQ6qsJe"}},{"cell_type":"markdown","source":"# Train and Evaluate model","metadata":{"id":"0pwADd1a85bn"}},{"cell_type":"markdown","source":"We will create a batch of examples using [DataCollatorWithPadding.](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding)  \n\nData collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n\nDataCollatorWithPadding will help us pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. This allows for efficient computation during each batch.\n\n*   DataCollatorForTokenClassification : `padding (bool, str or PaddingStrategy, optional, defaults to True)`\n*   `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single sequence is provided).\n\n","metadata":{"id":"TsnlIUJvYEy2"}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"id":"CcAY4-E2J6e5","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.682887Z","iopub.execute_input":"2025-02-02T03:33:21.683175Z","iopub.status.idle":"2025-02-02T03:33:21.698388Z","shell.execute_reply.started":"2025-02-02T03:33:21.683155Z","shell.execute_reply":"2025-02-02T03:33:21.697738Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"For evaluating your model’s performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy.","metadata":{"id":"jg4v14KcElbY"}},{"cell_type":"code","source":"import evaluate\n\nseqeval = evaluate.load(\"seqeval\")","metadata":{"id":"cZk3PjndK-Q8","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:21.699124Z","iopub.execute_input":"2025-02-02T03:33:21.699413Z","iopub.status.idle":"2025-02-02T03:33:22.188379Z","shell.execute_reply.started":"2025-02-02T03:33:21.699384Z","shell.execute_reply":"2025-02-02T03:33:22.187757Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced8e63872b84ec889bad8b3d69b8577"}},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"Huggingface requires us to write a ``compute_metrics()`` function. This will be invoked when huggingface evalutes a model.\n\nNote that we ignore to evaluate on -100 labels.","metadata":{"id":"FllGDNO5RUIA"}},{"cell_type":"code","source":"import numpy as np\nimport warnings\n\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"id":"vDwNPItNLTM1","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:22.189088Z","iopub.execute_input":"2025-02-02T03:33:22.189291Z","iopub.status.idle":"2025-02-02T03:33:22.194811Z","shell.execute_reply.started":"2025-02-02T03:33:22.189272Z","shell.execute_reply":"2025-02-02T03:33:22.194048Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"The total number of labels in our POS tag set.","metadata":{"id":"kt13vTldvTw4"}},{"cell_type":"code","source":"id2label = {\n    0: 'ADVI',\n    1: 'ADVN',\n    2: 'ADVP',\n    3: 'ADVS',\n    4: 'CFQC',\n    5: 'CLTV',\n    6: 'CMTR',\n    7: 'CMTR@PUNC',\n    8: 'CNIT',\n    9: 'CVBL',\n    10: 'DCNM',\n    11: 'DDAC',\n    12: 'DDAN',\n    13: 'DDAQ',\n    14: 'DDBQ',\n    15: 'DIAC',\n    16: 'DIAQ',\n    17: 'DIBQ',\n    18: 'DONM',\n    19: 'EAFF',\n    20: 'EITT',\n    21: 'FIXN',\n    22: 'FIXV',\n    23: 'JCMP',\n    24: 'JCRG',\n    25: 'JSBR',\n    26: 'NCMN',\n    27: 'NCNM',\n    28: 'NEG',\n    29: 'NLBL',\n    30: 'NONM',\n    31: 'NPRP',\n    32: 'NTTL',\n    33: 'PDMN',\n    34: 'PNTR',\n    35: 'PPRS',\n    36: 'PREL',\n    37: 'PUNC',\n    38: 'RPRE',\n    39: 'VACT',\n    40: 'VATT',\n    41: 'VSTA',\n    42: 'XVAE',\n    43: 'XVAM',\n    44: 'XVBB',\n    45: 'XVBM',\n    46: 'XVMM',\n    # 47: 'O'\n}\nlabel2id = {}\nfor k, v in id2label.items() :\n    label2id[v] = k\n\nlabel2id","metadata":{"id":"JD84B79-Lxwf","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:22.195596Z","iopub.execute_input":"2025-02-02T03:33:22.195875Z","iopub.status.idle":"2025-02-02T03:33:22.220738Z","shell.execute_reply.started":"2025-02-02T03:33:22.195836Z","shell.execute_reply":"2025-02-02T03:33:22.219957Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'ADVI': 0,\n 'ADVN': 1,\n 'ADVP': 2,\n 'ADVS': 3,\n 'CFQC': 4,\n 'CLTV': 5,\n 'CMTR': 6,\n 'CMTR@PUNC': 7,\n 'CNIT': 8,\n 'CVBL': 9,\n 'DCNM': 10,\n 'DDAC': 11,\n 'DDAN': 12,\n 'DDAQ': 13,\n 'DDBQ': 14,\n 'DIAC': 15,\n 'DIAQ': 16,\n 'DIBQ': 17,\n 'DONM': 18,\n 'EAFF': 19,\n 'EITT': 20,\n 'FIXN': 21,\n 'FIXV': 22,\n 'JCMP': 23,\n 'JCRG': 24,\n 'JSBR': 25,\n 'NCMN': 26,\n 'NCNM': 27,\n 'NEG': 28,\n 'NLBL': 29,\n 'NONM': 30,\n 'NPRP': 31,\n 'NTTL': 32,\n 'PDMN': 33,\n 'PNTR': 34,\n 'PPRS': 35,\n 'PREL': 36,\n 'PUNC': 37,\n 'RPRE': 38,\n 'VACT': 39,\n 'VATT': 40,\n 'VSTA': 41,\n 'XVAE': 42,\n 'XVAM': 43,\n 'XVBB': 44,\n 'XVBM': 45,\n 'XVMM': 46}"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"labels = [i for i in id2label.values()]\nlabels","metadata":{"id":"mQtGN8QQQLME","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:22.221557Z","iopub.execute_input":"2025-02-02T03:33:22.221802Z","iopub.status.idle":"2025-02-02T03:33:22.237194Z","shell.execute_reply.started":"2025-02-02T03:33:22.221782Z","shell.execute_reply":"2025-02-02T03:33:22.236511Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['ADVI',\n 'ADVN',\n 'ADVP',\n 'ADVS',\n 'CFQC',\n 'CLTV',\n 'CMTR',\n 'CMTR@PUNC',\n 'CNIT',\n 'CVBL',\n 'DCNM',\n 'DDAC',\n 'DDAN',\n 'DDAQ',\n 'DDBQ',\n 'DIAC',\n 'DIAQ',\n 'DIBQ',\n 'DONM',\n 'EAFF',\n 'EITT',\n 'FIXN',\n 'FIXV',\n 'JCMP',\n 'JCRG',\n 'JSBR',\n 'NCMN',\n 'NCNM',\n 'NEG',\n 'NLBL',\n 'NONM',\n 'NPRP',\n 'NTTL',\n 'PDMN',\n 'PNTR',\n 'PPRS',\n 'PREL',\n 'PUNC',\n 'RPRE',\n 'VACT',\n 'VATT',\n 'VSTA',\n 'XVAE',\n 'XVAM',\n 'XVBB',\n 'XVBM',\n 'XVMM']"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"## Load pretrained model","metadata":{"id":"Nu7Z3QH_BJe4"}},{"cell_type":"markdown","source":"Select a pretrained model for fine-tuning to develop a POS Tagger model using the Orchid corpus dataset.\n\n\n\n*   model : `wangchanberta-base-att-spm-uncased`\n*   Don't forget to update the num_labels.\n\nYou’re ready to start training your model now! Load pretrained model with AutoModelForTokenClassification along with the number of expected labels, and the label mappings:\n\n\n","metadata":{"id":"cBYlcF-gDZcF"}},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"6OOu8s-mO_Fw"}},{"cell_type":"code","source":"model_names = [\n    'wangchanberta-base-att-spm-uncased',\n    'wangchanberta-base-wiki-newmm',\n    'wangchanberta-base-wiki-ssg',\n    'wangchanberta-base-wiki-sefr',\n    'wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"wangchanberta-base-att-spm-uncased\"\n\n#create model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    f\"airesearch/{model_name}\",\n    revision='main',\n    num_labels=47, id2label=id2label, label2id=label2id\n)\n","metadata":{"id":"OOsnubHyDMmA","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:22.237901Z","iopub.execute_input":"2025-02-02T03:33:22.238210Z","iopub.status.idle":"2025-02-02T03:33:25.993641Z","shell.execute_reply.started":"2025-02-02T03:33:22.238181Z","shell.execute_reply":"2025-02-02T03:33:25.993042Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d62e5a68ad944ed922d73b133b40f78"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### #TODO 2","metadata":{"id":"-H2OExQrCAfX"}},{"cell_type":"markdown","source":"* Configure your training hyperparameters using `**TrainingArguments**`. The only required parameter is is `output_dir`, which determines the directory where your model will be saved. To upload the model to the Hugging Face Hub, set push_to_hub=True (note: you must be logged into Hugging Face for this). During training, the Trainer will compute seqeval metrics at the end of each epoch and store the training checkpoint.\n* Provide the `**Trainer**` with the training arguments, as well as the model, dataset, tokenizer, data collator, and compute_metrics function.\n* Use `**train()**` to fine-tune the model.\n\n\nRead [huggingface's tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification) for more details.","metadata":{"id":"FBZKrz8nFXyT"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    #########################\n    output_dir=\"pos-spm-uncased\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    push_to_hub=True\n    #########################\n)\n\ntrainer = Trainer(\n    #########################\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_orchid[\"train\"],\n    eval_dataset=tokenized_orchid[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    ########################\n)\n\ntrainer.train()","metadata":{"id":"gMUkNHNrCwsl","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:33:25.994390Z","iopub.execute_input":"2025-02-02T03:33:25.994934Z","iopub.status.idle":"2025-02-02T03:38:36.633672Z","shell.execute_reply.started":"2025-02-02T03:33:25.994910Z","shell.execute_reply":"2025-02-02T03:38:36.632357Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\nFor more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n  warnings.warn(warning_message, FutureWarning)\nCloning https://huggingface.co/Nacnano/pos-spm-uncased into local empty directory.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnacnano\u001b[0m (\u001b[33mnacnano2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250202_033339-iv0yj4w6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nacnano2/huggingface/runs/iv0yj4w6' target=\"_blank\">clean-gorge-2</a></strong> to <a href='https://wandb.ai/nacnano2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nacnano2/huggingface' target=\"_blank\">https://wandb.ai/nacnano2/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nacnano2/huggingface/runs/iv0yj4w6' target=\"_blank\">https://wandb.ai/nacnano2/huggingface/runs/iv0yj4w6</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='580' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [580/580 04:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.808900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=580, training_loss=0.7498709218255405, metrics={'train_runtime': 302.8881, 'train_samples_per_second': 122.157, 'train_steps_per_second': 1.915, 'total_flos': 1416038773777320.0, 'train_loss': 0.7498709218255405, 'epoch': 2.0})"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"XiXUG6aakihd"}},{"cell_type":"markdown","source":"With your model fine-tuned, you can now perform inference.","metadata":{"id":"2OPVknoQk4wL"}},{"cell_type":"code","source":"text = \"การประชุมทางวิชาการ ครั้งที่ 1\"","metadata":{"id":"mMyq6I9CkZ1R","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:38:36.635094Z","iopub.execute_input":"2025-02-02T03:38:36.635413Z","iopub.status.idle":"2025-02-02T03:38:36.640203Z","shell.execute_reply.started":"2025-02-02T03:38:36.635376Z","shell.execute_reply":"2025-02-02T03:38:36.639385Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"FL5qxD3EPLFt"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load pretrained tokenizer from Hugging Face\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n\ntokenizer = Tokenizer(model_name).from_pretrained(model_name)\ninputs = tokenizer(text, return_tensors=\"pt\")","metadata":{"id":"sKgM-EaGfxA4","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:38:36.641191Z","iopub.execute_input":"2025-02-02T03:38:36.641525Z","iopub.status.idle":"2025-02-02T03:38:36.915618Z","shell.execute_reply.started":"2025-02-02T03:38:36.641495Z","shell.execute_reply":"2025-02-02T03:38:36.914756Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"inputs","metadata":{"id":"PcRf-Q9nf4-t","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:38:36.916529Z","iopub.execute_input":"2025-02-02T03:38:36.916845Z","iopub.status.idle":"2025-02-02T03:38:36.928108Z","shell.execute_reply.started":"2025-02-02T03:38:36.916813Z","shell.execute_reply":"2025-02-02T03:38:36.927284Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   5,   10,  882, 8222,    8,   10, 1014,    8,   10,   59,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\n## Load your fine-tuned model from Hugging Face\nmodel = AutoModelForTokenClassification.from_pretrained(\"nacnano/pos-spm-uncased\") ## your model path from Hugging Face\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"id":"6ADY5OuqkkHb","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:42:10.107810Z","iopub.execute_input":"2025-02-02T03:42:10.108198Z","iopub.status.idle":"2025-02-02T03:42:22.067877Z","shell.execute_reply.started":"2025-02-02T03:42:10.108168Z","shell.execute_reply":"2025-02-02T03:42:22.067071Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee0c12707874cb7be65ebde1004c58a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/419M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafcdc3181c2457790452af98a3d0dca"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"predictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\npredicted_token_class","metadata":{"id":"AACsd7VZgT1E","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:42:35.830589Z","iopub.execute_input":"2025-02-02T03:42:35.831040Z","iopub.status.idle":"2025-02-02T03:42:35.839179Z","shell.execute_reply.started":"2025-02-02T03:42:35.830996Z","shell.execute_reply":"2025-02-02T03:42:35.838321Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"['PUNC',\n 'PUNC',\n 'VACT',\n 'NCMN',\n 'PUNC',\n 'PUNC',\n 'NCMN',\n 'DONM',\n 'DONM',\n 'DONM',\n 'PUNC']"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"id2label","metadata":{"id":"kJKGbf4Rk0c9","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:42:48.347940Z","iopub.execute_input":"2025-02-02T03:42:48.348385Z","iopub.status.idle":"2025-02-02T03:42:48.356844Z","shell.execute_reply.started":"2025-02-02T03:42:48.348357Z","shell.execute_reply":"2025-02-02T03:42:48.356040Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"{0: 'ADVI',\n 1: 'ADVN',\n 2: 'ADVP',\n 3: 'ADVS',\n 4: 'CFQC',\n 5: 'CLTV',\n 6: 'CMTR',\n 7: 'CMTR@PUNC',\n 8: 'CNIT',\n 9: 'CVBL',\n 10: 'DCNM',\n 11: 'DDAC',\n 12: 'DDAN',\n 13: 'DDAQ',\n 14: 'DDBQ',\n 15: 'DIAC',\n 16: 'DIAQ',\n 17: 'DIBQ',\n 18: 'DONM',\n 19: 'EAFF',\n 20: 'EITT',\n 21: 'FIXN',\n 22: 'FIXV',\n 23: 'JCMP',\n 24: 'JCRG',\n 25: 'JSBR',\n 26: 'NCMN',\n 27: 'NCNM',\n 28: 'NEG',\n 29: 'NLBL',\n 30: 'NONM',\n 31: 'NPRP',\n 32: 'NTTL',\n 33: 'PDMN',\n 34: 'PNTR',\n 35: 'PPRS',\n 36: 'PREL',\n 37: 'PUNC',\n 38: 'RPRE',\n 39: 'VACT',\n 40: 'VATT',\n 41: 'VSTA',\n 42: 'XVAE',\n 43: 'XVAM',\n 44: 'XVBB',\n 45: 'XVBM',\n 46: 'XVMM'}"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"# Inference\n# ignore special tokens\ntext = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\ninputs = tokenizer(text, return_tensors=\"pt\")\ntokenized_input = tokenizer([text], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :', tokens)\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\nprint('predict pos :', predicted_token_class)","metadata":{"id":"6OikBqcKTcUY","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:42:55.890378Z","iopub.execute_input":"2025-02-02T03:42:55.890805Z","iopub.status.idle":"2025-02-02T03:42:55.980517Z","shell.execute_reply.started":"2025-02-02T03:42:55.890767Z","shell.execute_reply":"2025-02-02T03:42:55.979661Z"}},"outputs":[{"name":"stdout","text":"tokens : ['<s>', '▁', 'จะว่าไป', 'แล้ว', 'เชิง', 'เทียน', 'ของ', 'ผมก็', 'สวยดี', 'เหมือนกัน', '</s>']\npredict pos : ['PUNC', 'PUNC', 'VACT', 'JSBR', 'NCMN', 'NCMN', 'RPRE', 'XVBM', 'VATT', 'ADVN', 'PUNC']\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"**Evaluate model :**\n\nThe output from the model is a softmax over classes. We choose the maximum class as the answer for evaluation. Again, we will ignore the -100 labels.","metadata":{"id":"D-TtwYTCXs2O"}},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display\n\ndef evaluation_report(y_true, y_pred, get_only_acc=False):\n    # retrieve all tags in y_true\n    tag_set = set()\n    for sent in y_true:\n        for tag in sent:\n            tag_set.add(tag)\n    for sent in y_pred:\n        for tag in sent:\n            tag_set.add(tag)\n    tag_list = sorted(list(tag_set))\n\n    # count correct points\n    tag_info = dict()\n    for tag in tag_list:\n        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n\n    all_correct = 0\n    all_count = sum([len(sent) for sent in y_true])\n    speacial_tag = 0\n    for sent_true, sent_pred in zip(y_true, y_pred):\n        for tag_true, tag_pred in zip(sent_true, sent_pred):\n            # pass special token\n            if tag_true == -100 :\n                speacial_tag += 1\n                pass\n            if tag_true == tag_pred:\n                tag_info[tag_true]['correct_tagged'] += 1\n                all_correct += 1\n            tag_info[tag_true]['y_true'] += 1\n            tag_info[tag_pred]['y_pred'] += 1\n    print('speacial_tag :',speacial_tag) # delete number of special token from all_count\n    accuracy = (all_correct / (all_count-speacial_tag))\n\n    # get only accuracy for testing\n    if get_only_acc:\n      return accuracy\n\n    accuracy *= 100\n\n\n    # summarize and make evaluation result\n    eval_list = list()\n    for tag in tag_list:\n        eval_result = dict()\n        eval_result['tag'] = tag\n        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n        eval_result['precision'] = precision\n        eval_result['recall'] = recall\n        eval_result['f1_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n\n        eval_list.append(eval_result)\n\n    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f1_score': ''})\n\n    df = pd.DataFrame.from_dict(eval_list)\n    df = df[['tag', 'precision', 'recall', 'f1_score', 'correct_count']]\n\n    display(df)\n","metadata":{"id":"Hm_adLrcXyOe","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:43:14.818364Z","iopub.execute_input":"2025-02-02T03:43:14.818687Z","iopub.status.idle":"2025-02-02T03:43:14.828500Z","shell.execute_reply.started":"2025-02-02T03:43:14.818660Z","shell.execute_reply":"2025-02-02T03:43:14.827825Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# prepare test set\ntest_data = tokenized_orchid[\"test\"]","metadata":{"id":"x7Lryj2aYCdn","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:43:20.780142Z","iopub.execute_input":"2025-02-02T03:43:20.780435Z","iopub.status.idle":"2025-02-02T03:43:20.784850Z","shell.execute_reply.started":"2025-02-02T03:43:20.780413Z","shell.execute_reply":"2025-02-02T03:43:20.784016Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# labels for test set\ny_test = []\nfor inputs in test_data:\n  y_test.append(inputs['labels'])","metadata":{"id":"FGXWNs9RY2Zv","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:43:22.727271Z","iopub.execute_input":"2025-02-02T03:43:22.727578Z","iopub.status.idle":"2025-02-02T03:43:23.414017Z","shell.execute_reply.started":"2025-02-02T03:43:22.727555Z","shell.execute_reply":"2025-02-02T03:43:23.413127Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"y_pred = []\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nfor inputs in test_data:\n    text = inputs['sentence']\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        pred = model(**inputs).logits\n        predictions = torch.argmax(pred, dim=2)\n        # Append padded predictions to y_pred\n        y_pred.append(predictions.tolist()[0])","metadata":{"id":"U6__09qnX1DW","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:43:25.380066Z","iopub.execute_input":"2025-02-02T03:43:25.380372Z","iopub.status.idle":"2025-02-02T03:48:06.542324Z","shell.execute_reply.started":"2025-02-02T03:43:25.380347Z","shell.execute_reply":"2025-02-02T03:48:06.541351Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# check our prediction with label\n# -100 is special tokens : [<s>, </s>, _]\nprint(y_pred[0])\nprint(y_test[0])","metadata":{"id":"yX0BhOe7g3eh","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:49:24.909344Z","iopub.execute_input":"2025-02-02T03:49:24.909647Z","iopub.status.idle":"2025-02-02T03:49:24.915661Z","shell.execute_reply.started":"2025-02-02T03:49:24.909623Z","shell.execute_reply":"2025-02-02T03:49:24.914786Z"}},"outputs":[{"name":"stdout","text":"[37, 29, 39, 26, 26, 26, 37, 37, 26, 26, 26, 41, 37, 37, 26, 26, 39, 26, 37]\n[-100, 29, 39, 26, 26, 26, 37, -100, 26, 26, 26, 41, 37, -100, 26, 26, 39, 26, -100]\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"evaluation_report(y_test, y_pred)","metadata":{"id":"Na0L6NUdgLaE","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:49:27.784523Z","iopub.execute_input":"2025-02-02T03:49:27.784847Z","iopub.status.idle":"2025-02-02T03:49:27.912190Z","shell.execute_reply.started":"2025-02-02T03:49:27.784819Z","shell.execute_reply":"2025-02-02T03:49:27.911499Z"}},"outputs":[{"name":"stdout","text":"speacial_tag : 21039\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               tag  precision     recall   f1_score correct_count\n0             -100          -        0.0          -             0\n1                0          -        0.0          -             0\n2                1  58.652246   69.80198  63.743219           705\n3                2          -        0.0          -             0\n4                3          -        0.0          -             0\n5                4          -        0.0          -             0\n6                5       37.5   3.468208   6.349206             6\n7                6  75.628415   96.51325  84.803922           692\n8                7          -        0.0          -             0\n9                8  54.279749  65.989848  59.564719           260\n10              10  90.272374  89.402697  89.835431           928\n11              11  87.344398  92.324561  89.765458           421\n12              12  69.387755  65.384615  67.326733            68\n13              13          -        0.0          -             0\n14              14  78.021978  68.932039  73.195876            71\n15              15     88.125  86.503067  87.306502           282\n16              16          -        0.0          -             0\n17              17  83.783784       87.5  85.601578           217\n18              18  70.663094  96.611722  81.624758          1055\n19              19          -        0.0          -             0\n20              20          -        0.0          -             0\n21              21  84.696756  79.906853  82.232112          1201\n22              22  77.777778  58.333333  66.666667            98\n23              23  95.238095  21.052632  34.482759            20\n24              24  94.143404  95.449501  94.791954          1720\n25              25  81.126126   82.46337  81.789282          1801\n26              26  90.097612  93.764375   91.89443         29352\n27              27  72.929293  58.699187  65.045045           361\n28              28  97.222222  30.172414  46.052632            35\n29              29   97.08589   98.44479  97.760618           633\n30              31  77.820603  85.620663  81.534508          2221\n31              32  97.315436  98.639456  97.972973           145\n32              33  92.592593  27.173913  42.016807            25\n33              34          -        0.0          -             0\n34              35          -        0.0          -             0\n35              36  91.821156  83.531746  87.480519           842\n36              37  38.943267  97.960159  55.731091         12294\n37              38  86.278938  89.435177  87.828711          3056\n38              39  83.121666  87.523402  85.265763          6545\n39              40  60.801394   61.22807  61.013986           698\n40              41  73.662396  72.273567  72.961373          1955\n41              42  79.334677    88.2287  83.545648           787\n42              43  93.459302  94.558824  94.005848           643\n43              45  78.039927  90.146751  83.657588           430\n44              46  88.599349  85.266458  86.900958           272\n45  accuracy=89.55                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-100</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>58.652246</td>\n      <td>69.80198</td>\n      <td>63.743219</td>\n      <td>705</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>37.5</td>\n      <td>3.468208</td>\n      <td>6.349206</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>75.628415</td>\n      <td>96.51325</td>\n      <td>84.803922</td>\n      <td>692</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>54.279749</td>\n      <td>65.989848</td>\n      <td>59.564719</td>\n      <td>260</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>90.272374</td>\n      <td>89.402697</td>\n      <td>89.835431</td>\n      <td>928</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>87.344398</td>\n      <td>92.324561</td>\n      <td>89.765458</td>\n      <td>421</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>69.387755</td>\n      <td>65.384615</td>\n      <td>67.326733</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>78.021978</td>\n      <td>68.932039</td>\n      <td>73.195876</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>88.125</td>\n      <td>86.503067</td>\n      <td>87.306502</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>83.783784</td>\n      <td>87.5</td>\n      <td>85.601578</td>\n      <td>217</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>70.663094</td>\n      <td>96.611722</td>\n      <td>81.624758</td>\n      <td>1055</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>84.696756</td>\n      <td>79.906853</td>\n      <td>82.232112</td>\n      <td>1201</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>77.777778</td>\n      <td>58.333333</td>\n      <td>66.666667</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>95.238095</td>\n      <td>21.052632</td>\n      <td>34.482759</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>94.143404</td>\n      <td>95.449501</td>\n      <td>94.791954</td>\n      <td>1720</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>81.126126</td>\n      <td>82.46337</td>\n      <td>81.789282</td>\n      <td>1801</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>90.097612</td>\n      <td>93.764375</td>\n      <td>91.89443</td>\n      <td>29352</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>72.929293</td>\n      <td>58.699187</td>\n      <td>65.045045</td>\n      <td>361</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>97.222222</td>\n      <td>30.172414</td>\n      <td>46.052632</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>97.08589</td>\n      <td>98.44479</td>\n      <td>97.760618</td>\n      <td>633</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>77.820603</td>\n      <td>85.620663</td>\n      <td>81.534508</td>\n      <td>2221</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>97.315436</td>\n      <td>98.639456</td>\n      <td>97.972973</td>\n      <td>145</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>92.592593</td>\n      <td>27.173913</td>\n      <td>42.016807</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>91.821156</td>\n      <td>83.531746</td>\n      <td>87.480519</td>\n      <td>842</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>38.943267</td>\n      <td>97.960159</td>\n      <td>55.731091</td>\n      <td>12294</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>86.278938</td>\n      <td>89.435177</td>\n      <td>87.828711</td>\n      <td>3056</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>83.121666</td>\n      <td>87.523402</td>\n      <td>85.265763</td>\n      <td>6545</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>60.801394</td>\n      <td>61.22807</td>\n      <td>61.013986</td>\n      <td>698</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>73.662396</td>\n      <td>72.273567</td>\n      <td>72.961373</td>\n      <td>1955</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>79.334677</td>\n      <td>88.2287</td>\n      <td>83.545648</td>\n      <td>787</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>93.459302</td>\n      <td>94.558824</td>\n      <td>94.005848</td>\n      <td>643</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>45</td>\n      <td>78.039927</td>\n      <td>90.146751</td>\n      <td>83.657588</td>\n      <td>430</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>46</td>\n      <td>88.599349</td>\n      <td>85.266458</td>\n      <td>86.900958</td>\n      <td>272</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>accuracy=89.55</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"# Other Pretrained model","metadata":{"id":"F0BZzVgjm4l_"}},{"cell_type":"markdown","source":"In this section, we will experiment by fine-tuning other pretrained models, such as airesearch/wangchanberta-base-wiki-newmm, to see how about their performance.\n\nSince each model uses a different word-tokenization method.\nfor example, **airesearch/wangchanberta-base-wiki-newmm uses newmm**,\nwhile **airesearch/wangchanberta-base-att-spm-uncased uses SentencePiece**.\nplease try fine-tuning and compare the performance of these models.","metadata":{"id":"ssZdIst48AwH"}},{"cell_type":"markdown","source":"### #TODO 3","metadata":{"id":"GUCWuhrl91mj"}},{"cell_type":"code","source":"model_names = [\n    'airesearch/wangchanberta-base-att-spm-uncased',\n    'airesearch/wangchanberta-base-wiki-newmm',\n    'airesearch/wangchanberta-base-wiki-ssg',\n    'airesearch/wangchanberta-base-wiki-sefr',\n    'airesearch/wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-wiki-newmm\" #@param [\"airesearch/wangchanberta-base-att-spm-uncased\", \"airesearch/wangchanberta-base-wiki-newmm\", \"airesearch/wangchanberta-base-wiki-syllable\", \"airesearch/wangchanberta-base-wiki-sefr\", \"airesearch/wangchanberta-base-wiki-spm\"]\n\n#create tokenizer\ntokenizer = Tokenizer(model_name).from_pretrained(\n                f'{model_name}',\n                revision='main',\n                model_max_length=416,)\n","metadata":{"id":"9etT-A_anBfi","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:49:48.310240Z","iopub.execute_input":"2025-02-02T03:49:48.310548Z","iopub.status.idle":"2025-02-02T03:49:50.034123Z","shell.execute_reply.started":"2025-02-02T03:49:48.310526Z","shell.execute_reply":"2025-02-02T03:49:50.033426Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"newmm.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d3b164db584ad585c1a18a3153f6df"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'ThaiWordsNewmmTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'ThaiWordsNewmmTokenizer'.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"example = orchidl[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])","metadata":{"id":"LFXdV8V5nGk2","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:49:52.649875Z","iopub.execute_input":"2025-02-02T03:49:52.650204Z","iopub.status.idle":"2025-02-02T03:49:52.665325Z","shell.execute_reply.started":"2025-02-02T03:49:52.650177Z","shell.execute_reply":"2025-02-02T03:49:52.664516Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ntokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"It's the same problem as above.\n\n`**Warning: Can we use same function as above ?**`\n\n`**Warning: Please beware of <unk>, an unknown word token.**`\n\n`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`","metadata":{"id":"I7QMwcbK5Ibj"}},{"cell_type":"code","source":"def majority_vote_pos(examples):\n\n    ####################################################################################################################\n    # TO DO: Since the tokens from the output of the pretrained tokenizer\n    # do not match the tokens in the label tokens of the dataset,\n    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n    # the POS tag from the segment with the greater number of characters should be assigned.\n    #\n    # Example :\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n    #\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n\n    # FILL CODE HERE\n    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n    label_tokens = examples[\"label_tokens\"]\n    pos_tags = examples[\"pos_tags\"]\n    new_pos_result = []\n    \n    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n    \n    label_idx, char_idx = 0, 0  # Track label and character position\n    \n    for token in new_tokens:\n        if token in {\"<s>\", \"</s>\", \"▁\", \"<unk>\"}:\n            new_pos_result.append(-100)\n            continue\n    \n        # Normalize token\n        token = token.replace('ํา', \"ำ\").replace(\"<_>\", \" \")\n        token = token[1:] if token.startswith(\"▁\") else token\n    \n        buffer = \"\"\n        weights = {}\n    \n        # Align token with label tokens\n        while label_tokens[label_idx][char_idx] != token[0]:\n            char_idx += 1\n            if char_idx == len(label_tokens[label_idx]):\n                label_idx += 1\n                char_idx = 0\n    \n        # Assign POS tag based on the dominant segment\n        char_count = 1\n        for _ in range(30):  # Limit iterations to prevent infinite loops\n            buffer += label_tokens[label_idx][char_idx]\n    \n            if buffer != token[:char_count]:\n                buffer = \"\"\n                char_count = 1\n                continue\n    \n            char_count += 1\n            weights[pos_tags[label_idx]] = weights.get(pos_tags[label_idx], 0) + 1\n            char_idx += 1\n    \n            if char_idx == len(label_tokens[label_idx]):\n                label_idx += 1\n                char_idx = 0\n    \n            if buffer == token:\n                break\n    \n        new_pos_result.append(max(weights, key=weights.get))\n    \n    tokenized_inputs[\"tokens\"] = new_tokens\n    tokenized_inputs[\"labels\"] = new_pos_result\n    \n    return tokenized_inputs\n\n\n    ####################################################################################################################","metadata":{"id":"SI0fUulE5MH3","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:04.813173Z","iopub.execute_input":"2025-02-02T03:50:04.813508Z","iopub.status.idle":"2025-02-02T03:50:04.821807Z","shell.execute_reply.started":"2025-02-02T03:50:04.813481Z","shell.execute_reply":"2025-02-02T03:50:04.821144Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"tokenized_orchid = orchidl.map(majority_vote_pos)","metadata":{"id":"O5n4veYxo3rR","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:08.676505Z","iopub.execute_input":"2025-02-02T03:50:08.676880Z","iopub.status.idle":"2025-02-02T03:50:27.275925Z","shell.execute_reply.started":"2025-02-02T03:50:08.676853Z","shell.execute_reply":"2025-02-02T03:50:27.275052Z"}},"outputs":[{"name":"stderr","text":"Parameter 'function'=<function majority_vote_pos at 0x788e281555a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e91df7f84143a493f9e07c6a0385d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd366ea11c34852b1defea2828678f3"}},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"# hard test case\nexample = tokenized_orchid[\"train\"][1899]\nfor i in example :\n    print(i, \":\", example[i])","metadata":{"id":"ashRh72szWiM","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:36.419980Z","iopub.execute_input":"2025-02-02T03:50:36.420329Z","iopub.status.idle":"2025-02-02T03:50:36.435707Z","shell.execute_reply.started":"2025-02-02T03:50:36.420304Z","shell.execute_reply":"2025-02-02T03:50:36.435042Z"}},"outputs":[{"name":"stdout","text":"id : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [0, 80, 3973, 45, 12252, 3496, 592, 5, 3, 5, 30055, 5, 63190, 178, 2]\ntoken_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\nlabels : [-100, 25, 39, 38, 26, 26, 5, 37, -100, 26, 26, 26, 26, 37, -100]\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model_names = [\n    'wangchanberta-base-att-spm-uncased',\n    'wangchanberta-base-wiki-newmm',\n    'wangchanberta-base-wiki-ssg',\n    'wangchanberta-base-wiki-sefr',\n    'wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"wangchanberta-base-wiki-newmm\" #@param [\"wangchanberta-base-att-spm-uncased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n\n#create model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    f\"airesearch/{model_name}\",\n    revision='main',\n    num_labels=47, id2label=id2label, label2id=label2id\n)\n","metadata":{"id":"7AL0Vqbv7cfb","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:40.349596Z","iopub.execute_input":"2025-02-02T03:50:40.349886Z","iopub.status.idle":"2025-02-02T03:50:42.136510Z","shell.execute_reply.started":"2025-02-02T03:50:40.349864Z","shell.execute_reply":"2025-02-02T03:50:42.135646Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\nSome weights of the model checkpoint at airesearch/wangchanberta-base-wiki-newmm were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-wiki-newmm and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"id":"pWTh0bMfP70g","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:59.207729Z","iopub.execute_input":"2025-02-02T03:50:59.208053Z","iopub.status.idle":"2025-02-02T03:50:59.212699Z","shell.execute_reply.started":"2025-02-02T03:50:59.208028Z","shell.execute_reply":"2025-02-02T03:50:59.211852Z"}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"### #TODO 4","metadata":{"id":"QkhYDS4q7oxK"}},{"cell_type":"markdown","source":"Fine-tuning other pretrained model with our orchid corpus.","metadata":{"id":"XVdITM5E7tQ5"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    #########################\n    output_dir=\"pos-base-wiki-newmm\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    push_to_hub=True\n    #########################\n)\n\ntrainer = Trainer(\n    #########################\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_orchid[\"train\"],\n    eval_dataset=tokenized_orchid[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    ########################\n)\n\ntrainer.train()","metadata":{"id":"hBHlUamr7syk","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:53:16.033709Z","iopub.execute_input":"2025-02-02T03:53:16.034237Z","iopub.status.idle":"2025-02-02T03:57:59.474788Z","shell.execute_reply.started":"2025-02-02T03:53:16.034206Z","shell.execute_reply":"2025-02-02T03:57:59.474067Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\nFor more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n  warnings.warn(warning_message, FutureWarning)\nCloning https://huggingface.co/Nacnano/pos-base-wiki-newmm into local empty directory.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='580' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [580/580 04:36, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.538200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=580, training_loss=0.5066099199755438, metrics={'train_runtime': 277.2197, 'train_samples_per_second': 133.468, 'train_steps_per_second': 2.092, 'total_flos': 1051238545679160.0, 'train_loss': 0.5066099199755438, 'epoch': 2.0})"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"nacnano/pos-base-wiki-newmm\")\n\ntest_data = tokenized_orchid[\"test\"]\n\ny_test = []\nfor inputs in test_data:\n  y_test.append(inputs['labels'])\ny_pred = []\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nfor inputs in test_data:\n    text = inputs['sentence']\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        pred = model(**inputs).logits\n        predictions = torch.argmax(pred, dim=2)\n        y_pred.append(predictions.tolist()[0])\n\n\n######## EVALUATE YOUR MODEL ########\nevaluation_report(y_test, y_pred)","metadata":{"id":"KIdxvgUGCVsm","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:58:34.450712Z","iopub.execute_input":"2025-02-02T03:58:34.451050Z","iopub.status.idle":"2025-02-02T04:03:18.407109Z","shell.execute_reply.started":"2025-02-02T03:58:34.451025Z","shell.execute_reply":"2025-02-02T04:03:18.406045Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b810fbccbb2a48279662186d70c467f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/643M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e4e0d133d04b2284bd72b310c47ff7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"speacial_tag : 11485\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               tag  precision     recall   f1_score correct_count\n0             -100          -        0.0          -             0\n1                0        0.0        0.0          -             0\n2                1  58.270106  69.754768  63.497313           768\n3                2          -        0.0          -             0\n4                3  55.555556  17.857143  27.027027            10\n5                4  88.888889  77.419355  82.758621            48\n6                5  82.608696  43.930636  57.358491            76\n7                6  65.328467  91.094148  76.089267           358\n8                7          -        0.0          -             0\n9                8  62.416107  76.021798  68.550369           279\n10              10  93.198091  88.649262  90.866783           781\n11              11  94.011976  89.543726  91.723466           471\n12              12  59.854015  81.188119  68.907563            82\n13              13          -        0.0          -             0\n14              14  80.952381  86.231884  83.508772           119\n15              15    91.8429  92.682927  92.261002           304\n16              16          -        0.0          -             0\n17              17  97.761194  94.584838  96.146789           262\n18              18  86.299081  75.236708  80.389105          1033\n19              19          -        0.0          -             0\n20              20      100.0  64.705882  78.571429            11\n21              21  79.462285  93.868549  86.066734          2128\n22              22  74.251497  85.517241  79.487179           124\n23              23  81.818182  91.836735  86.538462            90\n24              24  96.744854  97.444552  97.093442          2021\n25              25  79.910045  87.881286  83.706321          2132\n26              26   75.08562   86.54265  80.408069         19074\n27              27  65.902579  52.752294  58.598726           230\n28              28  96.478873  97.508897   96.99115           274\n29              29  97.029703  98.393574  97.706879           490\n30              31  63.012516  84.052965  72.027627          1460\n31              32  79.381443      100.0  88.505747            77\n32              33   50.47619  55.789474       53.0            53\n33              34  64.705882       50.0  56.410256            11\n34              35       89.0  68.461538  77.391304            89\n35              36  94.349442  90.772532  92.526431          1269\n36              37   65.70581  97.711004  78.574297         11739\n37              38  91.870824  94.809423  93.316995          4950\n38              39  83.791771   91.57764  87.511871          7372\n39              40  62.328767  65.677626  63.959391           819\n40              41  82.941531  81.710214  82.321268          2752\n41              42  82.890252  94.044857  88.115942          1216\n42              43  96.932515  98.873592  97.893432           790\n43              45  96.136568  99.074074  97.583219          1070\n44              46  95.428571  93.557423  94.483734           334\n45  accuracy=89.14                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-100</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>58.270106</td>\n      <td>69.754768</td>\n      <td>63.497313</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>55.555556</td>\n      <td>17.857143</td>\n      <td>27.027027</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>88.888889</td>\n      <td>77.419355</td>\n      <td>82.758621</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>82.608696</td>\n      <td>43.930636</td>\n      <td>57.358491</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>65.328467</td>\n      <td>91.094148</td>\n      <td>76.089267</td>\n      <td>358</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>62.416107</td>\n      <td>76.021798</td>\n      <td>68.550369</td>\n      <td>279</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>93.198091</td>\n      <td>88.649262</td>\n      <td>90.866783</td>\n      <td>781</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>94.011976</td>\n      <td>89.543726</td>\n      <td>91.723466</td>\n      <td>471</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>59.854015</td>\n      <td>81.188119</td>\n      <td>68.907563</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>80.952381</td>\n      <td>86.231884</td>\n      <td>83.508772</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>91.8429</td>\n      <td>92.682927</td>\n      <td>92.261002</td>\n      <td>304</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>97.761194</td>\n      <td>94.584838</td>\n      <td>96.146789</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>86.299081</td>\n      <td>75.236708</td>\n      <td>80.389105</td>\n      <td>1033</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>100.0</td>\n      <td>64.705882</td>\n      <td>78.571429</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>79.462285</td>\n      <td>93.868549</td>\n      <td>86.066734</td>\n      <td>2128</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>74.251497</td>\n      <td>85.517241</td>\n      <td>79.487179</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>81.818182</td>\n      <td>91.836735</td>\n      <td>86.538462</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>96.744854</td>\n      <td>97.444552</td>\n      <td>97.093442</td>\n      <td>2021</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>79.910045</td>\n      <td>87.881286</td>\n      <td>83.706321</td>\n      <td>2132</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>75.08562</td>\n      <td>86.54265</td>\n      <td>80.408069</td>\n      <td>19074</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>65.902579</td>\n      <td>52.752294</td>\n      <td>58.598726</td>\n      <td>230</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>96.478873</td>\n      <td>97.508897</td>\n      <td>96.99115</td>\n      <td>274</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>97.029703</td>\n      <td>98.393574</td>\n      <td>97.706879</td>\n      <td>490</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>63.012516</td>\n      <td>84.052965</td>\n      <td>72.027627</td>\n      <td>1460</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>79.381443</td>\n      <td>100.0</td>\n      <td>88.505747</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>50.47619</td>\n      <td>55.789474</td>\n      <td>53.0</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>64.705882</td>\n      <td>50.0</td>\n      <td>56.410256</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>89.0</td>\n      <td>68.461538</td>\n      <td>77.391304</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>94.349442</td>\n      <td>90.772532</td>\n      <td>92.526431</td>\n      <td>1269</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>65.70581</td>\n      <td>97.711004</td>\n      <td>78.574297</td>\n      <td>11739</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>91.870824</td>\n      <td>94.809423</td>\n      <td>93.316995</td>\n      <td>4950</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>83.791771</td>\n      <td>91.57764</td>\n      <td>87.511871</td>\n      <td>7372</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>62.328767</td>\n      <td>65.677626</td>\n      <td>63.959391</td>\n      <td>819</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>82.941531</td>\n      <td>81.710214</td>\n      <td>82.321268</td>\n      <td>2752</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>82.890252</td>\n      <td>94.044857</td>\n      <td>88.115942</td>\n      <td>1216</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>96.932515</td>\n      <td>98.873592</td>\n      <td>97.893432</td>\n      <td>790</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>45</td>\n      <td>96.136568</td>\n      <td>99.074074</td>\n      <td>97.583219</td>\n      <td>1070</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>46</td>\n      <td>95.428571</td>\n      <td>93.557423</td>\n      <td>94.483734</td>\n      <td>334</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>accuracy=89.14</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":68},{"cell_type":"markdown","source":"### #TODO 5\n\nCompare the results between both models. Are they comparable? (Think about the ground truths of both models).\n\nPropose a way to fairly evaluate the models.","metadata":{"id":"AMpXErqPBv2I"}},{"cell_type":"markdown","source":"<b>Write your answer here :</b>\n\nWangchanberta-base-att-spm-uncased got accuracy of 89.55. Wangchanberta-base-wiki-newmm got accuracy of 89.14.\nSo the accuracies are not the significantly different.\n\n1. Models Are Not Directly Comparable\n    - Wangchanberta-base-att-spm-uncased: Trained on assorted Thai texts (social media, news, literature).\n    - Wangchanberta-base-wiki-newmm: Trained on Thai Wikipedia (encyclopedic, formal text).\n\nDifferences in dataset size and domain lead to biased comparisons.\n\n\n2. Proposed Evaluation Methods\n    - Use a Different Test Dataset:\nOrchid dataset favors wiki-newmm (similar to Wikipedia).\nSocial media dataset would provide a fairer test.\nMixed-domain dataset (social media + encyclopedia) is another option.\n    - Ablation Study: Reduce the size of att-spm-uncased dataset to match wiki-newmm for controlled comparisons.\n    - Standardize Tokenization:\nModels use different tokenization methods (word-based vs. subword-based). Re-training wiki-newmm with SentencePiece could help but is costly.","metadata":{"id":"UigMzZVNCDuS"}},{"cell_type":"markdown","source":"A note on preprocessing data.\n\n``process_transformers`` in ``thaixtransformers.preprocess`` also provides a preprocess code that deals with many issues such as casing, text cleaning, and white space replacement with <_>. You can also use this to preprocess your text. Note that space replacement is done automatically without preprocessing in thaixtransformers.\n","metadata":{"id":"bYN0X9HWVuUe"}}]}