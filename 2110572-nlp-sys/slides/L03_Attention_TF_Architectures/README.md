# Lecture03: Attention & Tranformers & Architectures

## Slides

* [Slide3.1: RNN & Attention Mechanisms](L3.1_RNN_Attention_2024.pdf)
* [Slide3.2: Transformer](L3.2_Transformer_2024.pdf)
* [Slide3.3: Transformer family](L3.3_The_transformer_family_2025.pdf)

## Demo codes

* Demo3.1_One-to-Many RNN Date Generation  [![Open In Colab](https://raw.githubusercontent.com/ekapolc/NLP_2025/main/codes/colab-badge.svg)](https://colab.research.google.com/github/ekapolc/NLP_2025/blob/main/codes/L03_Attention_TF_Architectures/Demo3_1_One_to_Many_RNN_Date_Generation.ipynb)
* Demo3.2_RNN Date Translation with Attention  [![Open In Colab](https://raw.githubusercontent.com/ekapolc/NLP_2025/main/codes/colab-badge.svg)](https://colab.research.google.com/github/ekapolc/NLP_2025/blob/main/codes/L03_Attention_TF_Architectures/Demo3_2_RNN_Date_Translation_with_Attention.ipynb)

## HWs

* HW3.1: Key-Value Attention for Thai Karaoke Character-level Machine Translation (answer your questions in MCV)  [![Open In Colab](https://raw.githubusercontent.com/ekapolc/NLP_2025/main/codes/colab-badge.svg)](https://colab.research.google.com/github/ekapolc/NLP_2025/blob/main/codes/L03_Attention_TF_Architectures/HW3_1_Key_Value_Attention_for_Thai_Karaoke_MT_to_student_2024.ipynb)
* HW3.2: Transformer from Scratch (answer your questions in MCV) [![Open In Colab](https://raw.githubusercontent.com/ekapolc/NLP_2025/main/codes/colab-badge.svg)](https://colab.research.google.com/github/ekapolc/NLP_2025/blob/main/codes/L03_Attention_TF_Architectures/HW3_2_Transformer_from_Scratch_to_student.ipynb)
