{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qES4TJDxApaB"
      },
      "source": [
        "# Homework 2 MLE and Naive Bayes\n",
        "\n",
        "## MLE\n",
        "\n",
        "### T1 and OT1\n",
        "\n",
        "see the picture below\n",
        "\n",
        "\n",
        "\n",
        "## Simple Bayes Classifier\n",
        "\n",
        "### T2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_bayes_classifier(mu_1, mu_2, vars_1, vars_2, p_w1, p_w2, intersect_x=None):\n",
        "  x_1 = np.linspace(mu_1 - 4*vars_1, mu_1 + 4*vars_1, 10000)\n",
        "  y_1 = stats.norm.pdf(x_1, mu_1, vars_1) * p_w1\n",
        "  x_2 = np.linspace(mu_2 - 4*vars_2, mu_2 + 4*vars_2, 10000)\n",
        "  y_2 = stats.norm.pdf(x_2, mu_2, vars_2) * p_w2\n",
        "\n",
        "  if(intersect_x is None):\n",
        "    intersect_x = (2*vars_1**2*math.log(p_w2) - 2*vars_2**2*math.log(p_w1) + mu_1**2 - mu_2**2) / (2 * mu_1 - 2 * mu_2)\n",
        "  print(\"intersect: x = \", intersect_x)\n",
        "\n",
        "  plt.plot(x_1, y_1, label='P(x|w1)P(w1)', color='blue')\n",
        "  plt.fill_between(x_1[x_1 < intersect_x], y_1[x_1 < intersect_x], color='blue', alpha=0.3)\n",
        "  plt.plot(x_2, y_2, label='P(x|w2)P(w2)', color='orange')\n",
        "  plt.fill_between(x_2[x_2 > intersect_x], y_2[x_2 > intersect_x], color='orange', alpha=0.3)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "vars = 2\n",
        "mu_1 = 5\n",
        "mu_2 = 0\n",
        "p_w1 = 0.5\n",
        "p_w2 = 0.5\n",
        "plot_bayes_classifier(mu_1, mu_2, vars, vars, p_w1, p_w2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars = 2\n",
        "mu_1 = 5\n",
        "mu_2 = 0\n",
        "p_w1 = 0.75\n",
        "p_w2 = 0.25\n",
        "plot_bayes_classifier(mu_1, mu_2, vars, vars, p_w1, p_w2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OT2\n",
        "\n",
        "As the image below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OT3\n",
        "\n",
        "Find new intersect_x\n",
        "\n",
        "From P(x|w1) = P(x|w2)\n",
        "1/sqrt(2*pi)/2*exp(-1/2*((x-4)^2)/2^2) = 1/sqrt(2*pi)/4*exp(-1/2*((x-0)^2)/4^2) \n",
        "x = -4 / 3 * (-4 + sqrt(4 + log(64)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars1 = 2\n",
        "vars2 = 4\n",
        "mu_1 = 4\n",
        "mu_2 = 0\n",
        "p_w1 = 0.5\n",
        "p_w2 = 0.5\n",
        "intersect_x =  -4 / 3 * (-4 + math.sqrt(4 + math.log(64)))\n",
        "plot_bayes_classifier(mu_1, mu_2, vars1, vars2, p_w1, p_w2, intersect_x=intersect_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Employee Attrition Prediction\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "guR7Z-AoA8wc"
      },
      "source": [
        "### read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2uxs7gfBCUG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('hr-employee-attrition-with-null.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ag-0B8i-BS-x"
      },
      "source": [
        "### Dataset statistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "9IOCh6I1BJ1y",
        "outputId": "6d1e3a16-53aa-462c-bfbf-2f2162000d57"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "ncRpzKlfBb48",
        "outputId": "d03ca374-c2ea-42eb-9922-dfa7d8e4b410"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NApae3ygBbEa"
      },
      "source": [
        "### Feature transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C3vuRArBKEe"
      },
      "outputs": [],
      "source": [
        "df.loc[df[\"Attrition\"] == \"no\", \"Attrition\"] = 0.0\n",
        "df.loc[df[\"Attrition\"] == \"yes\", \"Attrition\"] = 1.0\n",
        "string_categorical_col = ['Department', 'Attrition', 'BusinessTravel', 'EducationField', 'Gender', 'JobRole',\n",
        "                              'MaritalStatus', 'Over18', 'OverTime']\n",
        "\n",
        "# ENCODE STRING COLUMNS TO CATEGORICAL COLUMNS\n",
        "for col in string_categorical_col:\n",
        "    # INSERT CODE HERE\n",
        "    df[col] = pd.Categorical(df[col]).codes\n",
        "\n",
        "# HANDLE NULL NUMBERS\n",
        "# INSERT CODE HERE\n",
        "    \n",
        "\n",
        "df = df.loc[:, ~df.columns.isin(['EmployeeNumber', 'Unnamed: 0', 'EmployeeCount', 'StandardHours', 'Over18'])]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q1sVXHQBGAQJ"
      },
      "source": [
        "###  Spliting data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df['Attrition'], random_state=7)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "edasSXrRHuFm"
      },
      "source": [
        "### Display histogram of each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_histogram(df, feature, n_bin = 40):\n",
        "        \n",
        "        df_dropna = df[feature].dropna()\n",
        "        plt.hist(df_dropna, n_bin)\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "    # INSERT CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6wH3pKLhLKw1"
      },
      "source": [
        "### T4. Observe the histogram for Age, MonthlyIncome and DistanceFromHome. How many bins have zero counts? Do you think this is a good discretization? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_zero_bins(df, col_name, n_bins):\n",
        "    df_drop_na = df[col_name].dropna()\n",
        "    hist, bin__edge = np.histogram(df_drop_na, bins=n_bins)\n",
        "    return np.count_nonzero(hist == 0)\n",
        "\n",
        "features = [\"Age\", \"MonthlyIncome\", \"DistanceFromHome\"]\n",
        "for feature in features:\n",
        "    display_histogram(df, feature)\n",
        "    print(\"Zero bin count of\", feature, count_zero_bins(df, feature, n_bins=40))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "From having no zero bins, the Age and MonthlyIncome features are good discretiztion.\n",
        "\n",
        "While, the DistanceFromHome feature is not."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLY_n3oOlGe"
      },
      "source": [
        "### T5. Can we use a Gaussian to estimate this histogram? Why? What about a Gaussian Mixture Model (GMM)?\n",
        "\n",
        "Only the Age feature one can be estimated as Guassian since the rest don't look like a Normal Distribution.\n",
        "\n",
        "The MonthlyIncome and DistanceFromHome features may be estimated using GMM or other distributions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JMbbl306QrEs"
      },
      "source": [
        "### T6. Now plot the histogram according to the method described above (with 10, 40, and 100 bins) and show 3 plots each for Age, MonthlyIncome, and DistanceFromHome. Which bin size is most sensible for each features? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [10, 40, 100]\n",
        "\n",
        "for bin in bins:\n",
        "    for feature in features:\n",
        "        display_histogram(df, feature, n_bin=bin)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Age, the appropriate bins would be 10 because it can accurately describe the distribution of data. Not too rough and not too detailed.\n",
        "\n",
        "For MonthlyIncome, the optimal bins are 40 because the data is highly distrubuted.\n",
        "\n",
        "For DistanceFromHome, the appropriate bins are 10 because the DistanceFromHome data Will only be in a narrow range, no need to use many bins"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NlpUyIIuN0Hi"
      },
      "source": [
        "### T7. For the rest of the features, which one should be discretized in order to be modeled by histograms? What are the criteria for choosing whether we should discretize a feature or not? Answer this and discretize those features into 10 bins each. In other words, figure out the bin edge for each feature, then use digitize() to convert the features to discrete values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = df.columns.to_list()\n",
        "features.remove(\"Attrition\")\n",
        "for feature in features:\n",
        "    if len(df[feature].unique()) < 10:\n",
        "        features.remove(feature)\n",
        "        continue\n",
        "    display_histogram(df, feature, 10)\n",
        "\n",
        "print(features)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SJHkqbcSPwMW"
      },
      "source": [
        "### T8. What kind of distribution should we use to model histograms? (Answer a distribution name) What is the MLE for the likelihood distribution? (Describe how to do the MLE). Plot the likelihood distributions of MonthlyIncome, JobRole, HourlyRate, and MaritalStatus for different Attrition values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in ['MonthlyIncome', 'JobRole', 'HourlyRate', 'MaritalStatus']:\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    leave_df = df.loc[df['Attrition'] == 1]\n",
        "    stay_df = df.loc[df['Attrition'] == 0]\n",
        "\n",
        "    display_histogram(leave_df, col, 10)\n",
        "\n",
        "    display_histogram(stay_df, col, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "From the distribution, I chose Exponential Distribution.\n",
        "p(x|\\lambda) = \\lambda * exp(-\\lambda x)\n",
        "\n",
        "The process to find MLE is\n",
        "1. find the likelihood for each feature\n",
        "2. find the derivative of sum of log of all likelihoods\n",
        "3. set the derivative to zero and solve for the value that maximize the likelihoods\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c_1EvThSWAwi"
      },
      "source": [
        "### T9. What is the prior distribution of the two classes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_leave = df.loc[df['Attrition'] == 1, 'Attrition'].count() / df.shape[0]\n",
        "p_stay = df.loc[df['Attrition'] == 0, 'Attrition'].count() /df.shape[0]\n",
        "\n",
        "print(\"leave\", p_leave)\n",
        "print(\"stay\", p_stay)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JdEyB5Q7WSqI"
      },
      "source": [
        "### T10. If we use the current Naive Bayes with our current Maximum Likelihood Estimates, we will find that some P (x i |attrition) will be zero and will result in the entire product term to be zero. Propose a method to fix this problem.\n",
        "\n",
        "Use flooring for handling zeros by changing them to small numbers (e.g. 1e-20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "17O4iza4WX1X"
      },
      "source": [
        "### T11. Implement your Naive Bayes classifier. Use the learned distributions to classify the test set. Don’t forget to allow your classifier to handle missing values in the test set. Report the overall Accuracy. Then, report the Precision, Recall, and F score for detecting attrition. See Lecture 1 for the definitions of each metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from SimpleBayesClassifier import SimpleBayesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = df_train.to_numpy()\n",
        "data_test = df_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_size = 0.1\n",
        "\n",
        "X = df_train.drop(columns=['Attrition']).to_numpy()\n",
        "Y = df_train['Attrition'].to_numpy()\n",
        "\n",
        "x_train = X[:-int(len(X) * test_size)]\n",
        "y_train = Y[:-int(len(X) * test_size)]\n",
        "\n",
        "x_test = X[-int(len(X) * test_size):]\n",
        "y_test = Y[-int(len(X) * test_size):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.15785054575986565, 0.8421494542401343)"
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleBayesClassifier(n_pos = (y_train==1).sum(), n_neg = (y_train==0).sum())\n",
        "\n",
        "model.prior_pos, model.prior_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_prior():\n",
        "    \"\"\"\n",
        "    This function designed to test the implementation of the prior probability calculation in a Naive Bayes classifier. \n",
        "    Specifically, it checks if the classifier correctly computes the prior probabilities for the \n",
        "    negative and positive classes based on given input counts.\n",
        "    \"\"\"\n",
        "    \n",
        "    # prior_neg = 5/(5 + 5) = 0.5 and # prior_pos = 5/(5 + 5) = 0.5\n",
        "    assert (SimpleBayesClassifier(5, 5).prior_pos, SimpleBayesClassifier(5, 5).prior_neg) == (0.5, 0.5)\n",
        "\n",
        "    assert (SimpleBayesClassifier(3, 5).prior_pos, SimpleBayesClassifier(3, 5).prior_neg) == (0.375, 0.625)\n",
        "    assert (SimpleBayesClassifier(0, 1).prior_pos, SimpleBayesClassifier(0, 1).prior_neg) == (0, 1)\n",
        "    assert (SimpleBayesClassifier(1, 0).prior_pos, SimpleBayesClassifier(1, 0).prior_neg) == (1, 0)\n",
        "    \n",
        "check_prior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([(array([0.02763819, 0.05653266, 0.15577889, 0.17211055, 0.17839196,\n",
              "          0.15829146, 0.09547739, 0.07035176, 0.05150754, 0.0339196 ]),\n",
              "   array([-inf, 22.2, 26.4, 30.6, 34.8, 39. , 43.2, 47.4, 51.6, 55.8,  inf])),\n",
              "  (array([0.20338983, 0.        , 0.        , 0.08175474, 0.        ,\n",
              "          0.        , 0.13758724, 0.        , 0.        , 0.5772682 ]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.11220715, 0.08138101, 0.0998767 , 0.10850801, 0.08138101,\n",
              "          0.0974106 , 0.0838471 , 0.11960543, 0.11960543, 0.09617756]),\n",
              "   array([  -inf,  244.4,  383.8,  523.2,  662.6,  802. ,  941.4, 1080.8,\n",
              "          1220.2, 1359.6,    inf])),\n",
              "  (array([0.19341974, 0.        , 0.        , 0.03788634, 0.        ,\n",
              "          0.        , 0.54735793, 0.        , 0.        , 0.22133599]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.35903919, 0.13400759, 0.16687737, 0.09987358, 0.02275601,\n",
              "          0.04930468, 0.0505689 , 0.03286979, 0.04045512, 0.04424779]),\n",
              "   array([-inf,  3.8,  6.6,  9.4, 12.2, 15. , 17.8, 20.6, 23.4, 26.2,  inf])),\n",
              "  (array([0.12547529, 0.        , 0.18377693, 0.        , 0.        ,\n",
              "          0.38022814, 0.        , 0.27503169, 0.        , 0.03548796]),\n",
              "   array([-inf,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  inf])),\n",
              "  (array([0.2003988 , 0.01296112, 0.        , 0.33000997, 0.        ,\n",
              "          0.07676969, 0.27018943, 0.        , 0.0448654 , 0.06480558]),\n",
              "   array([-inf, -0.4,  0.2,  0.8,  1.4,  2. ,  2.6,  3.2,  3.8,  4.4,  inf])),\n",
              "  (array([0.18137255, 0.        , 0.        , 0.17279412, 0.        ,\n",
              "          0.        , 0.33088235, 0.        , 0.        , 0.31495098]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.20139581, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.33898305, 0.        , 0.        , 0.        , 0.45962114]),\n",
              "   array([-inf, -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  inf])),\n",
              "  (array([0.07936508, 0.1037851 , 0.08547009, 0.10500611, 0.09401709,\n",
              "          0.09279609, 0.10500611, 0.10744811, 0.10500611, 0.12210012]),\n",
              "   array([-inf,  37.,  44.,  51.,  58.,  65.,  72.,  79.,  86.,  93.,  inf])),\n",
              "  (array([0.03950617, 0.        , 0.        , 0.25185185, 0.        ,\n",
              "          0.        , 0.61234568, 0.        , 0.        , 0.0962963 ]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.32245922, 0.        , 0.38393977, 0.        , 0.        ,\n",
              "          0.15934755, 0.        , 0.08531995, 0.        , 0.0489335 ]),\n",
              "   array([-inf,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  inf])),\n",
              "  (array([0.20338983, 0.08075773, 0.02891326, 0.11565304, 0.05882353,\n",
              "          0.08973081, 0.05383848, 0.16151545, 0.17248255, 0.03489531]),\n",
              "   array([-inf, -0.1,  0.8,  1.7,  2.6,  3.5,  4.4,  5.3,  6.2,  7.1,  inf])),\n",
              "  (array([0.18316832, 0.        , 0.        , 0.18688119, 0.        ,\n",
              "          0.        , 0.29455446, 0.        , 0.        , 0.33539604]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.19441675, 0.        , 0.        , 0.19940179, 0.        ,\n",
              "          0.        , 0.36989033, 0.        , 0.        , 0.23629113]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.21878863, 0.23609394, 0.20148331, 0.07292954, 0.06551298,\n",
              "          0.05438813, 0.04202719, 0.01483313, 0.04449938, 0.04944376]),\n",
              "   array([   -inf,  2929.2,  4807.4,  6685.6,  8563.8, 10442. , 12320.2,\n",
              "          14198.4, 16076.6, 17954.8,     inf])),\n",
              "  (array([0.09440994, 0.11801242, 0.10186335, 0.09192547, 0.10186335,\n",
              "          0.10434783, 0.10186335, 0.09813665, 0.11428571, 0.07329193]),\n",
              "   array([   -inf,  4584.3,  7074.6,  9564.9, 12055.2, 14545.5, 17035.8,\n",
              "          19526.1, 22016.4, 24506.7,     inf])),\n",
              "  (array([0.13676286, 0.36010038, 0.10539523, 0.12672522, 0.09535759,\n",
              "          0.03136763, 0.03513174, 0.03889586, 0.03262233, 0.03764115]),\n",
              "   array([-inf,  0.9,  1.8,  2.7,  3.6,  4.5,  5.4,  6.3,  7.2,  8.1,  inf])),\n",
              "  (array([0.20239282, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.61216351, 0.        , 0.        , 0.        , 0.18544367]),\n",
              "   array([-inf, -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  inf])),\n",
              "  (array([0.26548673, 0.15423515, 0.20101138, 0.05689001, 0.06321113,\n",
              "          0.12262958, 0.02907712, 0.06700379, 0.01896334, 0.02149178]),\n",
              "   array([-inf, 12.4, 13.8, 15.2, 16.6, 18. , 19.4, 20.8, 22.2, 23.6,  inf])),\n",
              "  (array([0.86323714, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.13676286]),\n",
              "   array([-inf,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  inf])),\n",
              "  (array([0.18596059, 0.        , 0.        , 0.21551724, 0.        ,\n",
              "          0.        , 0.29187192, 0.        , 0.        , 0.30665025]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.39454094, 0.        , 0.        , 0.44789082, 0.        ,\n",
              "          0.        , 0.10669975, 0.        , 0.        , 0.05086849]),\n",
              "   array([-inf,  0.3,  0.6,  0.9,  1.2,  1.5,  1.8,  2.1,  2.4,  2.7,  inf])),\n",
              "  (array([0.0931677 , 0.23726708, 0.30434783, 0.07826087, 0.1068323 ,\n",
              "          0.06832298, 0.03726708, 0.03602484, 0.02360248, 0.01490683]),\n",
              "   array([-inf,  3.7,  7.4, 11.1, 14.8, 18.5, 22.2, 25.9, 29.6, 33.3,  inf])),\n",
              "  (array([0.03508772, 0.05137845, 0.        , 0.35213033, 0.        ,\n",
              "          0.33709273, 0.07393484, 0.        , 0.10401003, 0.04636591]),\n",
              "   array([-inf,  0.6,  1.2,  1.8,  2.4,  3. ,  3.6,  4.2,  4.8,  5.4,  inf])),\n",
              "  (array([0.04516939, 0.        , 0.        , 0.23462986, 0.        ,\n",
              "          0.        , 0.61480552, 0.        , 0.        , 0.10539523]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.27985075, 0.34577114, 0.21393035, 0.04477612, 0.039801  ,\n",
              "          0.05223881, 0.00746269, 0.00497512, 0.00621891, 0.00497512]),\n",
              "   array([-inf,  3.7,  7.4, 11.1, 14.8, 18.5, 22.2, 25.9, 29.6, 33.3,  inf])),\n",
              "  (array([0.18532338, 0.34328358, 0.08457711, 0.18532338, 0.06716418,\n",
              "          0.07960199, 0.02238806, 0.01492537, 0.01243781, 0.00497512]),\n",
              "   array([-inf,  1.8,  3.6,  5.4,  7.2,  9. , 10.8, 12.6, 14.4, 16.2,  inf])),\n",
              "  (array([0.61462206, 0.11648079, 0.08798017, 0.03221809, 0.08054523,\n",
              "          0.01115242, 0.01486989, 0.01858736, 0.00991326, 0.01363073]),\n",
              "   array([-inf,  1.5,  3. ,  4.5,  6. ,  7.5,  9. , 10.5, 12. , 13.5,  inf])),\n",
              "  (array([0.20075282, 0.35633626, 0.07904642, 0.01631117, 0.23086575,\n",
              "          0.06649937, 0.01505646, 0.02132999, 0.00878294, 0.00501882]),\n",
              "   array([-inf,  1.7,  3.4,  5.1,  6.8,  8.5, 10.2, 11.9, 13.6, 15.3,  inf]))],\n",
              " [(array([0.12      , 0.09333333, 0.18666667, 0.2       , 0.11333333,\n",
              "          0.08      , 0.06      , 0.04      , 0.04666667, 0.06      ]),\n",
              "   array([-inf,  22.,  26.,  30.,  34.,  38.,  42.,  46.,  50.,  54.,  inf])),\n",
              "  (array([0.19148936, 0.        , 0.        , 0.04255319, 0.        ,\n",
              "          0.        , 0.22340426, 0.        , 0.        , 0.54255319]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.09722222, 0.14583333, 0.07638889, 0.11111111, 0.07638889,\n",
              "          0.11111111, 0.0625    , 0.11111111, 0.10416667, 0.10416667]),\n",
              "   array([  -inf,  242.3,  381.6,  520.9,  660.2,  799.5,  938.8, 1078.1,\n",
              "          1217.4, 1356.7,    inf])),\n",
              "  (array([0.21276596, 0.        , 0.        , 0.04255319, 0.        ,\n",
              "          0.        , 0.46808511, 0.        , 0.        , 0.27659574]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.28025478, 0.10828025, 0.15923567, 0.10828025, 0.03184713,\n",
              "          0.06369427, 0.05732484, 0.05732484, 0.08280255, 0.05095541]),\n",
              "   array([-inf,  3.8,  6.6,  9.4, 12.2, 15. , 17.8, 20.6, 23.4, 26.2,  inf])),\n",
              "  (array([0.13375796, 0.        , 0.20382166, 0.        , 0.        ,\n",
              "          0.40764331, 0.        , 0.22929936, 0.        , 0.02547771]),\n",
              "   array([-inf,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  inf])),\n",
              "  (array([0.21808511, 0.02659574, 0.        , 0.28723404, 0.        ,\n",
              "          0.10106383, 0.20744681, 0.        , 0.04255319, 0.11702128]),\n",
              "   array([-inf, -0.4,  0.2,  0.8,  1.4,  2. ,  2.6,  3.2,  3.8,  4.4,  inf])),\n",
              "  (array([0.26573427, 0.        , 0.        , 0.22377622, 0.        ,\n",
              "          0.        , 0.24475524, 0.        , 0.        , 0.26573427]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.19680851, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.31914894, 0.        , 0.        , 0.        , 0.48404255]),\n",
              "   array([-inf, -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  inf])),\n",
              "  (array([0.12244898, 0.06122449, 0.1292517 , 0.1292517 , 0.07482993,\n",
              "          0.12244898, 0.08163265, 0.0952381 , 0.08163265, 0.10204082]),\n",
              "   array([-inf, 37.9, 44.8, 51.7, 58.6, 65.5, 72.4, 79.3, 86.2, 93.1,  inf])),\n",
              "  (array([0.12337662, 0.        , 0.        , 0.27922078, 0.        ,\n",
              "          0.        , 0.55194805, 0.        , 0.        , 0.04545455]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.59748428, 0.        , 0.25157233, 0.        , 0.        ,\n",
              "          0.10691824, 0.        , 0.01886792, 0.        , 0.02515723]),\n",
              "   array([-inf,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  inf])),\n",
              "  (array([0.19148936, 0.02659574, 0.03723404, 0.21808511, 0.01595745,\n",
              "          0.04255319, 0.0106383 , 0.17553191, 0.18085106, 0.10106383]),\n",
              "   array([-inf, -0.1,  0.8,  1.7,  2.6,  3.5,  4.4,  5.3,  6.2,  7.1,  inf])),\n",
              "  (array([0.20394737, 0.        , 0.        , 0.19078947, 0.        ,\n",
              "          0.        , 0.35526316, 0.        , 0.        , 0.25      ]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.24468085, 0.        , 0.        , 0.10638298, 0.        ,\n",
              "          0.        , 0.25531915, 0.        , 0.        , 0.39361702]),\n",
              "   array([-inf, -0.7, -0.4, -0.1,  0.2,  0.5,  0.8,  1.1,  1.4,  1.7,  inf])),\n",
              "  (array([0.47945205, 0.19178082, 0.12328767, 0.07534247, 0.04794521,\n",
              "          0.03424658, 0.02739726, 0.        , 0.        , 0.02054795]),\n",
              "   array([   -inf,  2862.6,  4716.2,  6569.8,  8423.4, 10277. , 12130.6,\n",
              "          13984.2, 15837.8, 17691.4,     inf])),\n",
              "  (array([0.14666667, 0.06666667, 0.08      , 0.14      , 0.08      ,\n",
              "          0.1       , 0.08666667, 0.08666667, 0.10666667, 0.10666667]),\n",
              "   array([   -inf,  4716.1,  7106.2,  9496.3, 11886.4, 14276.5, 16666.6,\n",
              "          19056.7, 21446.8, 23836.9,     inf])),\n",
              "  (array([0.09868421, 0.375     , 0.06578947, 0.07236842, 0.09210526,\n",
              "          0.06578947, 0.05921053, 0.07894737, 0.03289474, 0.05921053]),\n",
              "   array([-inf,  0.9,  1.8,  2.7,  3.6,  4.5,  5.4,  6.3,  7.2,  8.1,  inf])),\n",
              "  (array([0.22340426, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.36702128, 0.        , 0.        , 0.        , 0.40957447]),\n",
              "   array([-inf, -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  inf])),\n",
              "  (array([0.32278481, 0.14556962, 0.1835443 , 0.05063291, 0.05063291,\n",
              "          0.08227848, 0.03797468, 0.05696203, 0.02531646, 0.0443038 ]),\n",
              "   array([-inf, 12.4, 13.8, 15.2, 16.6, 18. , 19.4, 20.8, 22.2, 23.6,  inf])),\n",
              "  (array([0.82236842, 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.17763158]),\n",
              "   array([-inf,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  inf])),\n",
              "  (array([0.26896552, 0.        , 0.        , 0.2       , 0.        ,\n",
              "          0.        , 0.25517241, 0.        , 0.        , 0.27586207]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.66878981, 0.        , 0.        , 0.21019108, 0.        ,\n",
              "          0.        , 0.07006369, 0.        , 0.        , 0.05095541]),\n",
              "   array([-inf,  0.3,  0.6,  0.9,  1.2,  1.5,  1.8,  2.1,  2.4,  2.7,  inf])),\n",
              "  (array([0.25503356, 0.33557047, 0.19463087, 0.06711409, 0.05369128,\n",
              "          0.04026846, 0.02013423, 0.01342282, 0.00671141, 0.01342282]),\n",
              "   array([-inf,   4.,   8.,  12.,  16.,  20.,  24.,  28.,  32.,  36.,  inf])),\n",
              "  (array([0.06410256, 0.04487179, 0.        , 0.37820513, 0.        ,\n",
              "          0.30769231, 0.12179487, 0.        , 0.06410256, 0.01923077]),\n",
              "   array([-inf,  0.6,  1.2,  1.8,  2.4,  3. ,  3.6,  4.2,  4.8,  5.4,  inf])),\n",
              "  (array([0.12080537, 0.        , 0.        , 0.24832215, 0.        ,\n",
              "          0.        , 0.5033557 , 0.        , 0.        , 0.12751678]),\n",
              "   array([-inf,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8,  3.1,  3.4,  3.7,  inf])),\n",
              "  (array([0.48322148, 0.20805369, 0.1409396 , 0.10067114, 0.01342282,\n",
              "          0.00671141, 0.02684564, 0.00671141, 0.        , 0.01342282]),\n",
              "   array([-inf,  3.3,  6.6,  9.9, 13.2, 16.5, 19.8, 23.1, 26.4, 29.7,  inf])),\n",
              "  (array([0.32876712, 0.30136986, 0.11643836, 0.00684932, 0.14383562,\n",
              "          0.03424658, 0.04794521, 0.        , 0.        , 0.02054795]),\n",
              "   array([-inf,  1.5,  3. ,  4.5,  6. ,  7.5,  9. , 10.5, 12. , 13.5,  inf])),\n",
              "  (array([0.66887417, 0.1192053 , 0.05960265, 0.01324503, 0.08609272,\n",
              "          0.        , 0.01324503, 0.01324503, 0.01324503, 0.01324503]),\n",
              "   array([-inf,  1.5,  3. ,  4.5,  6. ,  7.5,  9. , 10.5, 12. , 13.5,  inf])),\n",
              "  (array([0.41059603, 0.18543046, 0.14569536, 0.00662252, 0.01986755,\n",
              "          0.16556291, 0.03311258, 0.02649007, 0.        , 0.00662252]),\n",
              "   array([-inf,  1.4,  2.8,  4.2,  5.6,  7. ,  8.4,  9.8, 11.2, 12.6,  inf]))])"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit_params(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STAY PARAMETERS\n",
            "Feature : 0\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  0.2  0.4  0.6  0.8  1.   1.2  1.4  1.6  1.8  inf]\n",
            "Feature : 1\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  1.2  1.4  1.6  1.8  2.   2.2  2.4  2.6  2.8  inf]\n",
            "Feature : 2\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  2.2  2.4  2.6  2.8  3.   3.2  3.4  3.6  3.8  inf]\n",
            "Feature : 3\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  3.2  3.4  3.6  3.8  4.   4.2  4.4  4.6  4.8  inf]\n",
            "\n",
            "LEAVE PARAMETERS\n",
            "Feature : 0\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  1.2  1.4  1.6  1.8  2.   2.2  2.4  2.6  2.8  inf]\n",
            "Feature : 1\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  2.2  2.4  2.6  2.8  3.   3.2  3.4  3.6  3.8  inf]\n",
            "Feature : 2\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  3.2  3.4  3.6  3.8  4.   4.2  4.4  4.6  4.8  inf]\n",
            "Feature : 3\n",
            "BINS : [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
            "EDGES : [-inf  4.2  4.4  4.6  4.8  5.   5.2  5.4  5.6  5.8  inf]\n"
          ]
        }
      ],
      "source": [
        "def check_fit_params():\n",
        "\n",
        "    \"\"\"\n",
        "    This function is designed to test the fit_params method of a SimpleBayesClassifier. \n",
        "    This method is presumably responsible for computing parameters for a Naive Bayes classifier \n",
        "    based on the provided training data. The parameters in this context is bins and edges from each histogram.\n",
        "    \"\"\"\n",
        "\n",
        "    T = SimpleBayesClassifier(2, 2)\n",
        "    X_TRAIN_CASE_1 = np.array([\n",
        "        [0, 1, 2, 3],\n",
        "        [1, 2, 3, 4],\n",
        "        [2, 3, 4, 5],\n",
        "        [3, 4, 5, 6]\n",
        "    ])\n",
        "    Y_TRAIN_CASE_1 = np.array([0, 1, 0, 1])\n",
        "    STAY_PARAMS_1, LEAVE_PARAMS_1 = T.fit_params(X_TRAIN_CASE_1, Y_TRAIN_CASE_1)\n",
        "\n",
        "    print(\"STAY PARAMETERS\")\n",
        "    for f_idx in range(len(STAY_PARAMS_1)):\n",
        "        print(f\"Feature : {f_idx}\")\n",
        "        print(f\"BINS : {STAY_PARAMS_1[f_idx][0]}\")\n",
        "        print(f\"EDGES : {STAY_PARAMS_1[f_idx][1]}\")\n",
        "    print(\"\")    \n",
        "    print(\"LEAVE PARAMETERS\")\n",
        "    for f_idx in range(len(STAY_PARAMS_1)):\n",
        "        print(f\"Feature : {f_idx}\")\n",
        "        print(f\"BINS : {LEAVE_PARAMS_1[f_idx][0]}\")\n",
        "        print(f\"EDGES : {LEAVE_PARAMS_1[f_idx][1]}\")\n",
        "\n",
        "check_fit_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "yRoC5CEqopSR"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 10 is out of bounds for axis 0 with size 10",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[257], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Vivobook\\github\\my-chula-courses\\2110573-patt-recog\\HW2\\SimpleBayesClassifier.py:93\u001b[0m, in \u001b[0;36mSimpleBayesClassifier.predict\u001b[1;34m(self, x, thresh)\u001b[0m\n\u001b[0;32m     91\u001b[0m     stay_bin_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(x[i, j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstay_params[j][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     92\u001b[0m     leave_bin_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(x[i, j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleave_params[j][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 93\u001b[0m     h \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstay_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstay_bin_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     94\u001b[0m     h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleave_params[j][\u001b[38;5;241m0\u001b[39m][leave_bin_idx])\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m>\u001b[39m thresh:\n",
            "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(x = x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICDb3zONzk_r"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred, show_result = True):\n",
        "\n",
        "  return accuracy, precision, recall, F1, fpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISYoqxHpF-3n",
        "outputId": "52dcd5c4-c1f2-417d-b34b-f96c66f136ed"
      },
      "outputs": [],
      "source": [
        "evaluate(y_test, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WMAR2VHjFtRY"
      },
      "source": [
        "### T12. Use the learned distributions to classify the test set. Report the results using the same metric as the previous question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc1T-WVrFsyQ"
      },
      "outputs": [],
      "source": [
        "model.fit_gaussian_params(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_fit_gaussian_params():\n",
        "\n",
        "    \"\"\"\n",
        "    This function is designed to test the fit_gaussian_params method of a SimpleBayesClassifier. \n",
        "    This method is presumably responsible for computing parameters for a Naive Bayes classifier \n",
        "    based on the provided training data. The parameters in this context is mean and STD.\n",
        "    \"\"\"\n",
        "\n",
        "    T = SimpleBayesClassifier(2, 2)\n",
        "    X_TRAIN_CASE_1 = np.array([\n",
        "        [0, 1, 2, 3],\n",
        "        [1, 2, 3, 4],\n",
        "        [2, 3, 4, 5],\n",
        "        [3, 4, 5, 6]\n",
        "    ])\n",
        "    Y_TRAIN_CASE_1 = np.array([0, 1, 0, 1])\n",
        "    STAY_PARAMS_1, LEAVE_PARAMS_1 = T.fit_gaussian_params(X_TRAIN_CASE_1, Y_TRAIN_CASE_1)\n",
        "\n",
        "    print(\"STAY PARAMETERS\")\n",
        "    for f_idx in range(len(STAY_PARAMS_1)):\n",
        "        print(f\"Feature : {f_idx}\")\n",
        "        print(f\"Mean : {STAY_PARAMS_1[f_idx][0]}\")\n",
        "        print(f\"STD. : {STAY_PARAMS_1[f_idx][1]}\")\n",
        "    print(\"\")    \n",
        "    print(\"LEAVE PARAMETERS\")\n",
        "    for f_idx in range(len(STAY_PARAMS_1)):\n",
        "        print(f\"Feature : {f_idx}\")\n",
        "        print(f\"Mean : {LEAVE_PARAMS_1[f_idx][0]}\")\n",
        "        print(f\"STD. : {LEAVE_PARAMS_1[f_idx][1]}\")\n",
        "    \n",
        "check_fit_gaussian_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIB77js3Gq_N"
      },
      "outputs": [],
      "source": [
        "y_pred = model.gaussian_predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9klPSVfHROD",
        "outputId": "cda67822-6b24-44eb-fdee-63af89557f08"
      },
      "outputs": [],
      "source": [
        "evaluate(y_test, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qvJd3MoWIR3c"
      },
      "source": [
        "### T13 : The random choice baseline is the accuracy if you make a random guess for each test sample. Give random guess (50% leaving, and 50% staying) to the test samples. Report the overall Accuracy. Then, report the Precision, Recall, and F score for attrition prediction using the random choice baseline."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-vbVLuCDIxNx"
      },
      "source": [
        "### T14. The majority rule is the accuracy if you use the most frequent class from the training set as the classification decision. Report the overall Accuracy. Then, report the Precision, Recall, and F score for attrition prediction using the majority rule baseline.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ6-fMVVJZcH"
      },
      "source": [
        "### T15. Compare the two baselines with your Naive Bayes classifier.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jkX76nMpKVMR"
      },
      "source": [
        "### T16. Use the following threshold values\n",
        "$ t = np.arange(-5,5,0.05) $\n",
        "### find the best accuracy, and F score (and the corresponding thresholds)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "df3wpyLtUMKl"
      },
      "source": [
        "### T17. Plot the RoC of your classifier."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ozvzlJyxZbH3"
      },
      "source": [
        "### T18. Change the number of discretization bins to 5. What happens to the RoC curve? Which discretization is better? The number of discretization bins can be considered as a hyperparameter, and must be chosen by comparing the final performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
