{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 9 : Machine Learning - Part 3 : Image Classification"
      ],
      "metadata": {
        "id": "VHEqg6H916rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will build a machine learning model that's capable of differentiating between different flavors UHT drinkable yoguht, UHT milk, or sparkling water (depending on your room)"
      ],
      "metadata": {
        "id": "lC6E_pw92BqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 : Make a copy of this notebook\n",
        "\n",
        "Before we begin, make a copy of this notebook first."
      ],
      "metadata": {
        "id": "6lMmUjiI2wIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Config Colab for ML usage\n",
        "\n",
        "Config colab for ML usage, as done in the tutorial\n",
        "\n",
        "check if configuration was done correctly by running the cells below"
      ],
      "metadata": {
        "id": "ZM264mUX23sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QqVGy5l027P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d897d98-3fd9-46a5-d51b-ec52390c7c81"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 29 04:10:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    30W /  70W |   2431MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yhdP-NYNAgI3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : Mount Google Drive\n",
        "\n",
        "Mount your Google Drive with this colab VM to use images you've uploaded prior"
      ],
      "metadata": {
        "id": "an0bcdxj3JsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "G9H2RVEl3VY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8008a0b3-066b-4c62-d9aa-384332c58634"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : Prepare data for model training\n",
        "\n",
        "Run the code below to prepare your data for training\n",
        "\n",
        "**Do not modify the code!**"
      ],
      "metadata": {
        "id": "n8RWYT103YGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = f'/content/drive/My Drive/CEE_Act9-3/train'"
      ],
      "metadata": {
        "id": "qmD6BEJA3jyc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_main = ImageFolder(train_img_path, transform=transform)\n",
        "dataset_tmp = torch.utils.data.random_split(dataset_main, [0.9, 0.1])\n",
        "\n",
        "dataset_train = dataset_tmp[0]\n",
        "dataset_val = dataset_tmp[1]"
      ],
      "metadata": {
        "id": "oWCi7QLZ39JW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32             # If you get GPU memory error, reduce this value by half at a time\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "LiIFVaG_4eJa"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : Train the model\n",
        "\n",
        "Run the code below to train your machine learning model, this will take a few minutes\n",
        "\n",
        "**Do not modify the code!**"
      ],
      "metadata": {
        "id": "crTl4VS55I6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "class ImageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageModel, self).__init__()\n",
        "\n",
        "        self.efficientnet = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.preprocess = EfficientNet_B0_Weights.DEFAULT.transforms()\n",
        "        for param in self.efficientnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.efficientnet.classifier[1] = nn.Linear(1280, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        x = self.efficientnet(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Cc7EQjfk4W7R"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7XJ2XmrgCHdj"
      },
      "outputs": [],
      "source": [
        "import copy, time\n",
        "\n",
        "def train_model(model, dataloader_train, dataloader_val, num_epoch, lossfn, optimizer, save_path=None):\n",
        "    best_model_weight = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    val_acc_his = []\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        print(f'epoch {epoch+1} / {num_epoch}')\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        running_loss_train = 0.0\n",
        "        running_loss_val = 0.0\n",
        "\n",
        "        for image, label in dataloader_train:\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(image)\n",
        "            loss = lossfn(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_train += loss.item() * image.size(0)\n",
        "\n",
        "        epoch_loss_train = running_loss_train / (len(dataloader_train.dataset) * 5)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for image, label in dataloader_val:\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(image)\n",
        "            loss = lossfn(output, label)\n",
        "\n",
        "            running_loss_val += loss.item() * image.size(0)\n",
        "        \n",
        "        epoch_loss_val = running_loss_val / len(dataloader_val.dataset)\n",
        "        val_acc_his.append(epoch_loss_val)\n",
        "\n",
        "        print(f'epoch {epoch+1} training completed ; training loss: {epoch_loss_train:.6f} ; validation loss: {epoch_loss_val:.6f}')\n",
        "\n",
        "        if epoch_loss_val < best_loss:\n",
        "            print('validation loss improved, saving model')\n",
        "            best_loss = epoch_loss_val\n",
        "            best_model_weight = copy.deepcopy(model.state_dict())\n",
        "            if save_path: torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            print(f'validation loss did not improved from {best_loss:.6f}')\n",
        "\n",
        "        print(f'epoch {epoch+1} completed in {time.time() - start_time:.4f} seconds')\n",
        "        print()\n",
        "\n",
        "    model.load_state_dict(best_model_weight)\n",
        "    return model, val_acc_his"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "pvRPZmtxaoQe"
      },
      "outputs": [],
      "source": [
        "model = ImageModel()\n",
        "model.to(device)\n",
        "\n",
        "lossfn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = f'/content/drive/My Drive/CEE_Act9-3/model-3.pt'"
      ],
      "metadata": {
        "id": "POYim93M7-PW"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_cnt = 20\n",
        "model, val_acc_his = train_model(model, train_loader, val_loader, epoch_cnt, lossfn, optimizer, model_save_path)"
      ],
      "metadata": {
        "id": "Wu5VJShs8S3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06c3a1c-9040-49cb-bcb7-fb6ebcf61521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 / 20\n",
            "epoch 1 training completed ; training loss: 0.194050 ; validation loss: 0.548956\n",
            "validation loss improved, saving model\n",
            "epoch 1 completed in 192.4167 seconds\n",
            "\n",
            "epoch 2 / 20\n",
            "epoch 2 training completed ; training loss: 0.096869 ; validation loss: 0.266732\n",
            "validation loss improved, saving model\n",
            "epoch 2 completed in 45.9934 seconds\n",
            "\n",
            "epoch 3 / 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 : Test the model\n",
        "\n",
        "Now, let's test the model with another set of data!\n",
        "\n",
        "**Do not modify the code except the one marked with comments!**"
      ],
      "metadata": {
        "id": "wUV-F2-t_xHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/leo2tigers/compengess2023-act9.git'"
      ],
      "metadata": {
        "id": "MEmSM9pe_wgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcf7b65-0890-4329-90cd-df50baa2efc9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'compengess2023-act9' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your room no. (401, 404, 502) here\n",
        "room_no = 502"
      ],
      "metadata": {
        "id": "A3sZPCaN_9Fo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets.folder import default_loader\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, transform=None):\n",
        "        self.paths = paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = default_loader(f'compengess2023-act9/test/{room_no}/{self.paths[index]}')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def test_model(img_paths):\n",
        "    image = ImageDataset(img_paths, transform=transform)\n",
        "    loader = torch.utils.data.DataLoader(image, batch_size=1)\n",
        "\n",
        "    all_predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img in loader:\n",
        "            predictions = list(nn.Softmax(dim=1)(model(img.to(device))).cpu().numpy())\n",
        "            for prediction in predictions:\n",
        "                all_predictions.append(prediction)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "def detail_result(label_list, filename_and_pred):\n",
        "    print(filename_and_pred[0])\n",
        "    for i in range(len(label_list)):\n",
        "        print(f'{label_list[i]}: {filename_and_pred[1][i]*100:.3f}%')\n",
        "    print()"
      ],
      "metadata": {
        "id": "04w1GaolAE9e"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = sorted(os.listdir(f\"/content/drive/My Drive/CEE_Act9-3/train\"))\n",
        "file_list = os.listdir(f'./compengess2023-act9/test/{room_no}')\n",
        "res = list(zip(file_list, test_model(file_list)))\n",
        "for entry in res: detail_result(label_list, entry)"
      ],
      "metadata": {
        "id": "Cdpizmqk_n1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420d1467-59f5-4189-83df-d69a9bc2ebb9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_20230329_091237.jpg\n",
            "sparkling_colalime: 13.628%\n",
            "sparkling_honeyyuzu: 44.985%\n",
            "sparkling_jplemon: 35.616%\n",
            "sparkling_whitepeach: 5.772%\n",
            "\n",
            "IMG_20230329_090650.jpg\n",
            "sparkling_colalime: 25.474%\n",
            "sparkling_honeyyuzu: 3.094%\n",
            "sparkling_jplemon: 68.503%\n",
            "sparkling_whitepeach: 2.929%\n",
            "\n",
            "IMG_20230328_233551.jpg\n",
            "sparkling_colalime: 6.716%\n",
            "sparkling_honeyyuzu: 33.557%\n",
            "sparkling_jplemon: 58.950%\n",
            "sparkling_whitepeach: 0.777%\n",
            "\n",
            "IMG_20230328_233610.jpg\n",
            "sparkling_colalime: 6.253%\n",
            "sparkling_honeyyuzu: 5.475%\n",
            "sparkling_jplemon: 87.308%\n",
            "sparkling_whitepeach: 0.963%\n",
            "\n",
            "IMG_20230329_090544.jpg\n",
            "sparkling_colalime: 16.994%\n",
            "sparkling_honeyyuzu: 2.015%\n",
            "sparkling_jplemon: 80.759%\n",
            "sparkling_whitepeach: 0.233%\n",
            "\n",
            "IMG_20230328_233629.jpg\n",
            "sparkling_colalime: 2.291%\n",
            "sparkling_honeyyuzu: 66.909%\n",
            "sparkling_jplemon: 30.649%\n",
            "sparkling_whitepeach: 0.151%\n",
            "\n",
            "IMG_20230329_090720.jpg\n",
            "sparkling_colalime: 3.145%\n",
            "sparkling_honeyyuzu: 1.874%\n",
            "sparkling_jplemon: 94.420%\n",
            "sparkling_whitepeach: 0.561%\n",
            "\n",
            "IMG_20230329_091205.jpg\n",
            "sparkling_colalime: 46.020%\n",
            "sparkling_honeyyuzu: 14.644%\n",
            "sparkling_jplemon: 36.208%\n",
            "sparkling_whitepeach: 3.129%\n",
            "\n",
            "IMG_20230328_233532.jpg\n",
            "sparkling_colalime: 8.540%\n",
            "sparkling_honeyyuzu: 19.298%\n",
            "sparkling_jplemon: 71.385%\n",
            "sparkling_whitepeach: 0.776%\n",
            "\n",
            "IMG_20230329_091223.jpg\n",
            "sparkling_colalime: 1.425%\n",
            "sparkling_honeyyuzu: 90.907%\n",
            "sparkling_jplemon: 7.384%\n",
            "sparkling_whitepeach: 0.283%\n",
            "\n",
            "IMG_20230329_090621.jpg\n",
            "sparkling_colalime: 0.814%\n",
            "sparkling_honeyyuzu: 17.589%\n",
            "sparkling_jplemon: 81.329%\n",
            "sparkling_whitepeach: 0.267%\n",
            "\n",
            "IMG_20230329_091257.jpg\n",
            "sparkling_colalime: 15.078%\n",
            "sparkling_honeyyuzu: 27.402%\n",
            "sparkling_jplemon: 44.050%\n",
            "sparkling_whitepeach: 13.470%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR2SZoIZfF_3",
        "outputId": "584a2143-840d-4dad-f4db-5ab4070dd253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IMG_20230329_091237.jpg',\n",
              " 'IMG_20230329_090650.jpg',\n",
              " 'IMG_20230328_233551.jpg',\n",
              " 'IMG_20230328_233610.jpg',\n",
              " 'IMG_20230329_090544.jpg',\n",
              " 'IMG_20230328_233629.jpg',\n",
              " 'IMG_20230329_090720.jpg',\n",
              " 'IMG_20230329_091205.jpg',\n",
              " 'IMG_20230328_233532.jpg',\n",
              " 'IMG_20230329_091223.jpg',\n",
              " 'IMG_20230329_090621.jpg',\n",
              " 'IMG_20230329_091257.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please show the result from the cell above to an instructor or TA to be graded**\n",
        "\n",
        "***- THIS IS THE END OF PART 3 -***"
      ],
      "metadata": {
        "id": "T3IDb3xLBf4q"
      }
    }
  ]
}