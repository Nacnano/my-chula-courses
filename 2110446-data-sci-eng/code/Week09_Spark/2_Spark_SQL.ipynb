{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "This notebook introduces how to perform basic data transformation and exploration using Spark SQL on [Avocado Prices dataset](https://www.kaggle.com/datasets/neuromusic/avocado-prices) in Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 11:07:53 WARN Utils: Your hostname, Natawuts-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.203.201.152 instead (on interface en0)\n",
      "24/11/19 11:07:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 11:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark SQL')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Data Preparation\n",
    "\n",
    "First, we read a csv file.  We can provide option such as delimiter and header.  We then rename the colume names to remove dot ('.') in the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'avocado.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out data and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- AveragePrice: double (nullable = true)\n",
      " |-- Total Volume: double (nullable = true)\n",
      " |-- 4046: double (nullable = true)\n",
      " |-- 4225: double (nullable = true)\n",
      " |-- 4770: double (nullable = true)\n",
      " |-- Total Bags: double (nullable = true)\n",
      " |-- Small Bags: double (nullable = true)\n",
      " |-- Large Bags: double (nullable = true)\n",
      " |-- XLarge Bags: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those columns with spaces or dots in the column names will be a bit difficult for Spark SQL, esepcially when we want to filter data.  We must first remove spaces/dots in the column names.  This is optional, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c.replace(' ', '_') for c in df.columns]\n",
    "df = df.toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Date',\n",
       " 'AveragePrice',\n",
       " 'Total_Volume',\n",
       " '4046',\n",
       " '4225',\n",
       " '4770',\n",
       " 'Total_Bags',\n",
       " 'Small_Bags',\n",
       " 'Large_Bags',\n",
       " 'XLarge_Bags',\n",
       " 'type',\n",
       " 'year',\n",
       " 'region']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "| Id|      Date|AveragePrice|Total_Volume|   4046|     4225| 4770|Total_Bags|Small_Bags|Large_Bags|XLarge_Bags|        type|year|region|\n",
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
      "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
      "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
      "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39|75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|\n",
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *'inferSchema=True'*, Spark SQL seems to guess datatype correctly.  However, if it is incorrect, we can cast each column to proper type using **'cast'** and replace back to the same column using **'withColumn'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spark SQL Commands\n",
    "\n",
    "We can select some columns using **'select'** and select some rows using **'filter'**.  Note that we can perform basic math to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----+------+\n",
      "|      Date|AveragePrice|Total_Bags|year|region|\n",
      "+----------+------------+----------+----+------+\n",
      "|2015-12-27|        1.33|   8696.87|2015|Albany|\n",
      "|2015-12-20|        1.35|   9505.56|2015|Albany|\n",
      "|2015-12-13|        0.93|   8145.35|2015|Albany|\n",
      "|2015-12-06|        1.08|   5811.16|2015|Albany|\n",
      "|2015-11-29|        1.28|   6183.95|2015|Albany|\n",
      "+----------+------------+----------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Date'], df['AveragePrice'], df['Total_Bags'], df['year'], df['region']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------+----------+\n",
      "|      Date|((Small_Bags + Large_Bags) + XLarge_Bags)|Total_Bags|\n",
      "+----------+-----------------------------------------+----------+\n",
      "|2015-12-27|                                  8696.87|   8696.87|\n",
      "|2015-12-20|                                  9505.56|   9505.56|\n",
      "|2015-12-13|                                  8145.35|   8145.35|\n",
      "|2015-12-06|                                  5811.16|   5811.16|\n",
      "|2015-11-29|                                  6183.95|   6183.95|\n",
      "+----------+-----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Date'], df['Small_Bags']+df['Large_Bags']+df['XLarge_Bags'], \n",
    "          df['Total_Bags']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+---------------------------+\n",
      "|      Date|Total_Bags|Total_Volume|(Total_Volume / Total_Bags)|\n",
      "+----------+----------+------------+---------------------------+\n",
      "|2015-12-27|   8696.87|    64236.62|          7.386176865929926|\n",
      "|2015-12-20|   9505.56|    54876.98|          5.773145401217814|\n",
      "|2015-12-13|   8145.35|   118220.22|         14.513829362765259|\n",
      "|2015-12-06|   5811.16|    78992.15|         13.593181051631687|\n",
      "|2015-11-29|   6183.95|     51039.6|          8.253559618043484|\n",
      "+----------+----------+------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Date'], df['Total_Bags'], df['Total_Volume'], \n",
    "          df['Total_Volume']/df['Total_Bags']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+-------+--------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "| Id|      Date|AveragePrice|Total_Volume|   4046|    4225| 4770|Total_Bags|Small_Bags|Large_Bags|XLarge_Bags|        type|year|region|\n",
      "+---+----------+------------+------------+-------+--------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  3|2015-12-06|        1.08|    78992.15| 1132.0|71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "|  4|2015-11-29|        1.28|     51039.6| 941.48|43838.39|75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|\n",
      "|  5|2015-11-22|        1.26|    55979.78|1184.27|48067.99|43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|\n",
      "+---+----------+------------+------------+-------+--------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Total_Bags'] < 8000).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|Total_Bags|\n",
      "+----------+----------+\n",
      "|2016-12-04|   6704.29|\n",
      "|2016-12-04|   7088.23|\n",
      "|2016-10-30|   6794.05|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Total_Bags'] < 8000) & (df.year > 2015)).select('Date', 'Total_Bags').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|Total_Bags|\n",
      "+----------+----------+\n",
      "|2016-12-04|   6704.29|\n",
      "|2016-12-04|   7088.23|\n",
      "|2016-10-30|   6794.05|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Total_Bags < 8000 and year > 2015').select('Date', 'Total_Bags').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+--------+\n",
      "| Id|      Date|AveragePrice|Total_Volume|     4046|     4225|    4770|Total_Bags|Small_Bags|Large_Bags|XLarge_Bags|        type|year|  region|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+--------+\n",
      "|  0|2015-12-27|        0.92|    439968.4|141447.28|130341.75|20174.55| 148004.82| 116383.07|  31621.75|        0.0|conventional|2015|SanDiego|\n",
      "|  1|2015-12-20|        0.94|   420476.44|130565.46| 119298.5|25052.93| 145559.55| 111019.22|  34540.33|        0.0|conventional|2015|SanDiego|\n",
      "|  2|2015-12-13|        0.84|    462548.3|155111.21|118664.89|16726.87| 172045.33| 128053.44|  43991.89|        0.0|conventional|2015|SanDiego|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('region == \"SanDiego\"').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and Groupby Functions\n",
    "We can use several built-in aggegrate functions.  We can also use groupby for group operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Distinct Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore those categorical data including region and type using select-distinct.  Note that it can be applied to those numerical data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        type|\n",
      "+------------+\n",
      "|     organic|\n",
      "|conventional|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            region|\n",
      "+------------------+\n",
      "|     PhoenixTucson|\n",
      "|       GrandRapids|\n",
      "|     SouthCarolina|\n",
      "|           TotalUS|\n",
      "|  WestTexNewMexico|\n",
      "|        Louisville|\n",
      "|      Philadelphia|\n",
      "|        Sacramento|\n",
      "|     DallasFtWorth|\n",
      "|      Indianapolis|\n",
      "|          LasVegas|\n",
      "|         Nashville|\n",
      "|        GreatLakes|\n",
      "|           Detroit|\n",
      "|            Albany|\n",
      "|          Portland|\n",
      "|  CincinnatiDayton|\n",
      "|          SanDiego|\n",
      "|             Boise|\n",
      "|HarrisburgScranton|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('region').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "|2015|\n",
      "|2016|\n",
      "|2017|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('year').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using group and groupby functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-----------------+\n",
      "|min(AveragePrice)| avg(AveragePrice)|max(AveragePrice)|\n",
      "+-----------------+------------------+-----------------+\n",
      "|             0.44|1.4059784097758825|             3.25|\n",
      "+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min('AveragePrice'), avg('AveragePrice'), max('AveragePrice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| avg(AveragePrice)|\n",
      "+------------------+\n",
      "|1.3981656804733738|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('region == \"SanDiego\"').select(avg('AveragePrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby function allows us to work data in groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        type|count|\n",
      "+------------+-----+\n",
      "|     organic| 9123|\n",
      "|conventional| 9126|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------------+\n",
      "|year|        type| avg(AveragePrice)|\n",
      "+----+------------+------------------+\n",
      "|2015|conventional|1.0779629629629648|\n",
      "|2015|     organic|1.6733238332739595|\n",
      "|2016|conventional|1.1055947293447288|\n",
      "|2016|     organic|1.5716844729344717|\n",
      "|2017|conventional|1.2948881900768692|\n",
      "|2017|     organic|1.7355209790209802|\n",
      "|2018|conventional| 1.127885802469136|\n",
      "|2018|     organic| 1.567175925925925|\n",
      "+----+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('year', 'type').agg({'AveragePrice': 'avg'}).orderBy('year', 'type').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Function\n",
    "We can create user-defined function using udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricegroup_mapping(price): \n",
    "    if price < 1:\n",
    "        return 'cheap'\n",
    "    if price < 2:\n",
    "        return 'moderate'\n",
    "    return 'expensive'\n",
    "\n",
    "to_pricegroup = udf(pricegroup_mapping, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------------------+\n",
      "|      Date|AveragePrice|pricegroup_mapping(AveragePrice)|\n",
      "+----------+------------+--------------------------------+\n",
      "|2015-12-27|        1.33|                        moderate|\n",
      "|2015-12-20|        1.35|                        moderate|\n",
      "|2015-12-13|        0.93|                           cheap|\n",
      "|2015-12-06|        1.08|                        moderate|\n",
      "|2015-11-29|        1.28|                        moderate|\n",
      "+----------+------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Date', 'AveragePrice', to_pricegroup('AveragePrice')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *'withColumn'*, we can compute values and store them to the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|AveragePrice|pricegroup|\n",
      "+------------+----------+\n",
      "|        1.33|  moderate|\n",
      "|        1.35|  moderate|\n",
      "|        0.93|     cheap|\n",
      "|        1.08|  moderate|\n",
      "|        1.28|  moderate|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn('pricegroup', to_pricegroup(df.AveragePrice))\n",
    "new_df.select('AveragePrice', 'pricegroup').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Pandas API\n",
    "Since release 3.2, Spark provides a Pandas API that enables users to use Pandas-compatible command with just one import (pyspark.pandas).  This lowers learning-curve and enables other features e.g. plotting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = ps.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.AveragePrice.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pdf.groupby('year').Date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
