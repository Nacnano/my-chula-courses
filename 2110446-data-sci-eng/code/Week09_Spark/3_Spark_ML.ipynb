{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML: Predicting Avocado Prices\n",
    "\n",
    "This notebook introduces how to train a ML model using Spark ML.  This bases on an excellent article in Towards Data Science [First Steps in Machine Learning with Apache Spark](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3) using [Avocado Prices dataset](https://www.kaggle.com/datasets/neuromusic/avocado-prices) in Kaggle.\n",
    "\n",
    "The objective of this model is to predict the average price of avocado given datetime, supply amounts, and region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 11:29:57 WARN Utils: Your hostname, Natawuts-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.203.201.152 instead (on interface en0)\n",
      "24/11/19 11:29:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark SQL')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, we read a csv file.  We can provide option such as delimiter and header.  We then rename the colume names to remove dot ('.') in the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'avocado.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_avocado = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c.replace(' ', '_') for c in df_avocado.columns]\n",
    "df_avocado = df_avocado.toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- AveragePrice: double (nullable = true)\n",
      " |-- Total_Volume: double (nullable = true)\n",
      " |-- 4046: double (nullable = true)\n",
      " |-- 4225: double (nullable = true)\n",
      " |-- 4770: double (nullable = true)\n",
      " |-- Total_Bags: double (nullable = true)\n",
      " |-- Small_Bags: double (nullable = true)\n",
      " |-- Large_Bags: double (nullable = true)\n",
      " |-- XLarge_Bags: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split data into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_avocado_train, df_avocado_test) = df_avocado.randomSplit([0.75, 0.25], seed=214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML Pipeline\n",
    "For this pipeline, we will create several transformers using built-in estimators/transformers.  These include:\n",
    "\n",
    "\n",
    "| SparkML Feature | Feature Type | Data Type |\n",
    "|:-----------------|:--------------:|:--------------:|\n",
    "| SQLTransformer  | Tranformer   | Numerical |\n",
    "| MinMaxScaler    | Estimator    | Numerical |\n",
    "| StandardScaler  | Estimator    | Numerical |\n",
    "| StringIndexer   | Estimator    | Categorical |\n",
    "| VectorAssembler | Transformer  | Both |\n",
    "\n",
    "Using these components, we create the following pipeline:\n",
    "\n",
    "| Pipeline Stage | SparkML Feature |\n",
    "|:----------|:----------|\n",
    "| sql_transformer | SQLTransformer |\n",
    "| month_vec_asm_transfromer | VectorAssembler |\n",
    "| month_scaler_transfromer | MinMaxScaler |\n",
    "| numerical_vec_asm_transformer | VectorAssembler |\n",
    "| std_scaler_transformer | StandardScaler |\n",
    "| str_indexer_transformer | StringIndexer |\n",
    "| categorical_vec_asm_transformer | VectorAssembler |\n",
    "| all_vec_asm_transformer | VectorAssembler |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Feature Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sql_transformer: numeric column selection and log-transform\n",
    "Create a transformer to select columns and log-transform some numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['`AveragePrice`', '`type`']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['AveragePrice', 'type']\n",
    "cols = [f\"`{col}`\" for col in cols]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOG(`4225`+1) AS `LOG_4225`',\n",
       " 'LOG(`4770`+1) AS `LOG_4770`',\n",
       " 'LOG(`Small_Bags`+1) AS `LOG_Small_Bags`',\n",
       " 'LOG(`Large_Bags`+1) AS `LOG_Large_Bags`',\n",
       " 'LOG(`XLarge_Bags`+1) AS `LOG_XLarge_Bags`']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cols =  ['4225', '4770', 'Small_Bags', 'Large_Bags', 'XLarge_Bags']\n",
    "log_cols = [f\"LOG(`{col}`+1) AS `LOG_{col}`\" for col in log_cols]\n",
    "log_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT`AveragePrice`, `type`, LOG(`4225`+1) AS `LOG_4225`, LOG(`4770`+1) AS `LOG_4770`, LOG(`Small_Bags`+1) AS `LOG_Small_Bags`, LOG(`Large_Bags`+1) AS `LOG_Large_Bags`, LOG(`XLarge_Bags`+1) AS `LOG_XLarge_Bags`, \\n    YEAR(__THIS__.Date)-2000 AS year, MONTH(__THIS__.Date) AS month\\n    FROM __THIS__\\n    '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = f\"\"\"SELECT{', '.join(cols)}, {', '.join(log_cols)}, \n",
    "    YEAR(__THIS__.Date)-2000 AS year, MONTH(__THIS__.Date) AS month\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    "statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_transformer = SQLTransformer(statement=statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "| Id|      Date|AveragePrice|Total_Volume|     4046|     4225|    4770|Total_Bags|Small_Bags|Large_Bags|XLarge_Bags|        type|year|          region|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "|  0|2015-12-27|        0.49|  1137707.43| 738314.8|286858.37|11642.46|  100891.8|  70749.02|  30142.78|        0.0|conventional|2015|   PhoenixTucson|\n",
      "|  0|2015-12-27|        0.71|   776404.39|451904.51|141599.36|15486.97| 167413.55| 123158.22|  33065.33|    11190.0|conventional|2015|WestTexNewMexico|\n",
      "|  0|2015-12-27|         0.8|  1020390.64|494425.64|276556.76|84912.97| 164495.27| 136560.04|   12277.7|   15657.53|conventional|2015|   DallasFtWorth|\n",
      "|  0|2015-12-27|         0.8|  2326942.14|976982.58|455203.42|86202.11| 808554.03| 722787.61|  74359.03|   11407.39|conventional|2015|      LosAngeles|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado_train.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|AveragePrice|        type|          LOG_4225|          LOG_4770|    LOG_Small_Bags|    LOG_Large_Bags|  LOG_XLarge_Bags|year|month|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|        0.49|conventional|12.566747374652527| 9.362499927974252|11.166908098190957|10.313733879047971|              0.0|  15|   12|\n",
      "|        0.71|conventional|11.860764002611406| 9.647818872531012| 11.72123326879331| 10.40627082310141|9.322865162818028|  15|   12|\n",
      "|         0.8|conventional| 12.53017497505446|11.349393905288467|11.824526973139381| 9.415621332905047|9.658771095406955|  15|   12|\n",
      "|         0.8|conventional|13.028501871764691|11.364461534887267|13.490872079413348| 11.21667384527801|9.342104328605496|  15|   12|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_transformer.transform(df_avocado_train).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### month_vec_asm_transformer / month_scaler_transformer: create month vectors and normalize their values\n",
    "\n",
    "After using SQLTransformer, we then tranform *'month'* column into month vector and then normalize their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+---------+\n",
      "|AveragePrice|        type|          LOG_4225|          LOG_4770|    LOG_Small_Bags|    LOG_Large_Bags|  LOG_XLarge_Bags|year|month|month_vec|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+---------+\n",
      "|        0.49|conventional|12.566747374652527| 9.362499927974252|11.166908098190957|10.313733879047971|              0.0|  15|   12|   [12.0]|\n",
      "|        0.71|conventional|11.860764002611406| 9.647818872531012| 11.72123326879331| 10.40627082310141|9.322865162818028|  15|   12|   [12.0]|\n",
      "|         0.8|conventional| 12.53017497505446|11.349393905288467|11.824526973139381| 9.415621332905047|9.658771095406955|  15|   12|   [12.0]|\n",
      "|         0.8|conventional|13.028501871764691|11.364461534887267|13.490872079413348| 11.21667384527801|9.342104328605496|  15|   12|   [12.0]|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_vec_asm_transformer = VectorAssembler(inputCols=['month'], outputCol='month_vec')\n",
    "\n",
    "df_avocado_month_ass = month_vec_asm_transformer.transform(sql_transformer.transform(df_avocado_train))\n",
    "df_avocado_month_ass.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer that normalizes month vector using an estimator, *\"MinMaxScaler\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+\n",
      "|month|month_vec|month_scaled|\n",
      "+-----+---------+------------+\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "+-----+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_scaler_estimator = MinMaxScaler(inputCol='month_vec', outputCol='month_scaled')\n",
    "month_scaler_transformer = month_scaler_estimator.fit(df_avocado_month_ass)\n",
    "\n",
    "month_scaler_transformer.transform(df_avocado_month_ass)\\\n",
    "    .select( ['month', 'month_vec', 'month_scaled'] )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numerical_vec_asm_transformer/std_scaler_transformer : assemble numerical features vector and scale all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------------+--------------------+\n",
      "|year|month_scaled|          LOG_4225|        features_num|\n",
      "+----+------------+------------------+--------------------+\n",
      "|  15|       [1.0]|12.566747374652527|[15.0,1.0,12.5667...|\n",
      "|  15|       [1.0]|11.860764002611406|[15.0,1.0,11.8607...|\n",
      "|  15|       [1.0]| 12.53017497505446|[15.0,1.0,12.5301...|\n",
      "|  15|       [1.0]|13.028501871764691|[15.0,1.0,13.0285...|\n",
      "+----+------------+------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_vec_asm_transformer = VectorAssembler(\n",
    "    inputCols=[\n",
    "      'year', 'month_scaled', 'LOG_4225', \n",
    "      'LOG_4770', 'LOG_Small_Bags', \n",
    "      'LOG_Large_Bags', 'LOG_XLarge_Bags'\n",
    "    ],\n",
    "    outputCol='features_num'\n",
    ")\n",
    "df_avocado_numerical = numerical_vec_asm_transformer.transform(month_scaler_transformer.transform(df_avocado_month_ass))\n",
    "df_avocado_numerical.select('year', 'month_scaled', 'LOG_4225','features_num').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_scaled                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295]|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976]  |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.4787880607250015,1.8713767178927097,1.4321533934378963,1.1582533794554424,2.5870627060190463] |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scaling the numerical features using a StandardScaler\n",
    "std_scaler_estimator = StandardScaler(\n",
    "    inputCol=\"features_num\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "std_scaler_transformer = std_scaler_estimator.fit(df_avocado_numerical)\n",
    "std_scaler_transformer.transform(df_avocado_numerical).select(['features_scaled']).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature Transformers\n",
    "Transforming categorical features usually involve text transformation e.g. one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str_indexer_transformer: encoding categorical data\n",
    "We create a transformer using \"StringIndexer\", which is an estimator that produces StringIndexerModel.  This is similar to perform one-hot encoder on the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|        type|type_index|\n",
      "+------------+----------+\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "+------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type_indexer_estimator = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "type_indexer_transformer = type_indexer_estimator.fit(df_avocado_train)\n",
    "\n",
    "type_indexer_transformer.transform(df_avocado_train)\\\n",
    "  .select( [\"type\", \"type_index\"] ).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------+\n",
      "|        type|type_index|features_cat|\n",
      "+------------+----------+------------+\n",
      "|conventional|       0.0|       [0.0]|\n",
      "|conventional|       0.0|       [0.0]|\n",
      "|conventional|       0.0|       [0.0]|\n",
      "|conventional|       0.0|       [0.0]|\n",
      "+------------+----------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_vec_asm_transformer = VectorAssembler(\n",
    "    inputCols=['type_index'],\n",
    "    outputCol='features_cat'\n",
    ")\n",
    "categorical_vec_asm_transformer.transform(\n",
    "    type_indexer_transformer.transform(df_avocado_train)\n",
    ").select('type', 'type_index', 'features_cat').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline: merge both numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_asm_transformer = VectorAssembler(\n",
    "        inputCols=['features_scaled', 'features_cat'],\n",
    "        outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_prep_pipeline = Pipeline(stages=[sql_transformer, month_vec_asm_transformer,\n",
    "                                         month_scaler_transformer, \n",
    "                                         numerical_vec_asm_transformer,\n",
    "                                         std_scaler_transformer,\n",
    "                                         type_indexer_transformer,\n",
    "                                         categorical_vec_asm_transformer,\n",
    "                                         all_vec_asm_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = feature_prep_pipeline.fit(df_avocado_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform training dataset using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train_transformed = pipeline_model.transform(df_avocado_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|features                                                                                                                                    |AveragePrice|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.4787880607250015,1.8713767178927097,1.4321533934378963,1.1582533794554424,2.5870627060190463,0.0] |0.81        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado_train_transformed.select('features', 'AveragePrice').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We will train a linear regression model using transformed training dataset.  In order to do this, we will have to fit an estimator, *'LinearRegression'* to transformed training dataset to create a model, which is a *transformer*, that can be used to test the testing dataset.\n",
    "\n",
    "Note that this example focuses on how to create a pipeline.  Spark also provides hyperparameter tuning function.  However, this is out of the scope of this example.  Please refer to [First Steps in Machine Learning with Apache Spark](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_estimator = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "\n",
    "    # Hyperaparameters\n",
    "    maxIter=1000,\n",
    "    regParam=0.3,       # Regularization\n",
    "    elasticNetParam=0.8 # Regularization mixing parameter. 1 for L1, 0 for L2.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 11:35:32 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/19 11:35:32 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "linear_reg_model = linear_reg_estimator.fit(df_avocado_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|AveragePrice|prediction        |\n",
      "+------------+------------------+\n",
      "|0.8         |1.4003505112793717|\n",
      "|0.95        |1.4003505112793717|\n",
      "|0.98        |1.4003505112793717|\n",
      "|1.07        |1.4116333911023091|\n",
      "|1.39        |1.4116333911023091|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado_train_pred = linear_reg_model.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_pred.select(\n",
    "  ['AveragePrice', 'prediction']\n",
    ").sample(False, 0.1, 0).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Spark provides several evaluation functions.  We will have to select the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3978489578943717"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_eval.evaluate(df_avocado_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
