{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####สามารถดูวิธีการรับ API key ได้ที่ : https://www.canva.com/design/DAGO-1qw5mc/bvFxBFvOd3XJmmbSR_ryfw/edit?utm_content=DAGO-1qw5mc&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton\n"
      ],
      "metadata": {
        "id": "Kx0xlZR3Vprb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Call OpenAI API"
      ],
      "metadata": {
        "id": "_WlsoxFyDSdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE0iW8DWGJCO",
        "outputId": "c081ad5a-df84-4523-f088-26f9dd7cf76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.13 (from langchain-community)\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.13->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.104-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-community-0.2.12 langchain-core-0.2.34 langchain-text-splitters-0.2.2 langsmith-0.1.104 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting openai==1.3.9\n",
            "  Downloading openai-1.3.9-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.9) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.3.9) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.3.9) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.9) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.9) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.9) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.3.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.3.9) (2.20.1)\n",
            "Downloading openai-1.3.9-py3-none-any.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-1.3.9\n",
            "Collecting llama-index==0.5.6\n",
            "  Downloading llama_index-0.5.6.tar.gz (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dataclasses_json in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (0.6.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (0.2.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (1.26.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (8.5.0)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (1.3.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (2.1.4)\n",
            "Collecting tiktoken (from llama-index==0.5.6)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (4.12.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json->llama-index==0.5.6) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json->llama-index==0.5.6) (0.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (0.1.104)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.6) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.6) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.6) (2024.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index==0.5.6) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->llama-index==0.5.6) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=0.26.4->llama-index==0.5.6) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=0.26.4->llama-index==0.5.6) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=0.26.4->llama-index==0.5.6) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=0.26.4->llama-index==0.5.6) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.26.4->llama-index==0.5.6) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain->llama-index==0.5.6) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain->llama-index==0.5.6) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->llama-index==0.5.6) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=0.26.4->llama-index==0.5.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=0.26.4->llama-index==0.5.6) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.5.6) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->llama-index==0.5.6) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->llama-index==0.5.6) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->llama-index==0.5.6) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json->llama-index==0.5.6) (1.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain->llama-index==0.5.6) (3.0.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-index\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-index: filename=llama_index-0.5.6-py3-none-any.whl size=248133 sha256=9e8b10129859071c17bcbf435faec1a78da952a26d602af9facdfad88927f117\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/9f/0a/a5d7181a1819086f8288d1becdad81de07fc874b55075dde19\n",
            "Successfully built llama-index\n",
            "Installing collected packages: tiktoken, llama-index\n",
            "Successfully installed llama-index-0.5.6 tiktoken-0.7.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.9)\n",
            "Collecting openai\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.9\n",
            "    Uninstalling openai-1.3.9:\n",
            "      Successfully uninstalled openai-1.3.9\n",
            "Successfully installed jiter-0.5.0 openai-1.42.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community langchain-core\n",
        "!pip install openai==1.3.9\n",
        "!pip install llama-index==0.5.6\n",
        "!pip install python-dotenv\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here and hit enter:\") ###ใส่ OpenAI API key ที่ได้มา"
      ],
      "metadata": {
        "id": "bh_TUDniGYqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All of OpenAI model : https://platform.openai.com/docs/models/\n",
        "model = [\n",
        "    \"chatgpt-4o-latest\",\n",
        "    \"gpt-4o\",\n",
        "    \"gpt-4-turbo\",\n",
        "    \"gpt-3.5-turbo-0125\",\n",
        "]"
      ],
      "metadata": {
        "id": "yZYMFr92bxLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_requirements = \"You are a helpful AI designed to perform tasks.\"\n",
        "prompt = \"what is arttoys?\"\n",
        "model = \"chatgpt-4o-latest\"\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_requirements},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUlRcnafZAZa",
        "outputId": "13395064-7e42-4769-f5a6-7fa401e234e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Art toys, also known as designer toys or collectible toys, are small-scale figures or sculptures that are often designed by contemporary artists, graphic designers, or illustrators. These toys are typically produced in limited editions and are considered a form of artistic expression. Unlike mass-produced toys, art toys are usually made for collectors and enthusiasts rather than for children to play with.\n",
            "\n",
            "### Characteristics of Art Toys:\n",
            "1. **Artistic Design:** Art toys often feature unique and creative designs that reflect the vision of the artist. These designs can be quirky, imaginative, or abstract.\n",
            "   \n",
            "2. **Limited Editions:** Many art toys are produced in limited quantities, making them more valuable and sought after by collectors.\n",
            "\n",
            "3. **Materials:** These toys can be made from a variety of materials, such as vinyl, resin, wood, or metal.\n",
            "\n",
            "4. **Themes:** Art toys may explore various themes, including pop culture, urban culture, fantasy, and more.\n",
            "\n",
            "5. **Collaborations:** Artists sometimes collaborate with toy companies or other creators to produce special series or crossover designs.\n",
            "\n",
            "### Categories of Art Toys:\n",
            "- **Designer Vinyl Toys:** These are the most common form of art toys and are usually made of vinyl. They often feature sleek, stylized designs.\n",
            "  \n",
            "- **Resin Toys:** Made from resin, these toys may feature more intricate details and are often hand-painted.\n",
            "\n",
            "- **Custom Toys:** Some artists take existing toys and customize them to create unique, one-of-a-kind pieces.\n",
            "\n",
            "- **Blind Box Toys:** These are sold in packaging that hides the specific toy inside, creating an element of surprise for the collector.\n",
            "\n",
            "### Notable Artists/Brands in Art Toys:\n",
            "- **Kidrobot:** One of the most well-known brands in the designer toy scene, famous for its Dunny and Munny lines.\n",
            "  \n",
            "- **Takashi Murakami:** A famous contemporary artist who has ventured into the world of art toys.\n",
            "\n",
            "- **KAWS:** A well-known artist who has created a range of iconic toys and sculptures.\n",
            "\n",
            "- **Medicom Toy:** A Japanese company famous for its \"BE@RBRICK\" and \"Kubrick\" toy lines.\n",
            "\n",
            "### Popularity and Culture:\n",
            "Art toys have grown in popularity in the last two decades, merging the worlds of art, fashion, and pop culture. They are often showcased in galleries, art fairs, and conventions, and some pieces have even become highly desirable items in the art market.\n",
            "\n",
            "Collectors and fans often form communities around specific artists or types of art toys, and the market for these toys can be quite active, with some pieces selling for significant sums of money due to their rarity and artistic value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####OpenAI Batch API this method save 50% API cost."
      ],
      "metadata": {
        "id": "9ypczuzKDb9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#โมเดลทั้งหมดที่รองกับการ Call API เเบบ Batch\n",
        "batch_OpenAI_model= [\"gpt-4o\",\n",
        "\"gpt-4o-2024-08-06\",\n",
        "\"gpt-4o-mini\",\n",
        "\"gpt-4-turbo\",\n",
        "\"gpt-4\",\n",
        "\"gpt-4-32k\",\n",
        "\"gpt-3.5-turbo\",\n",
        "\"gpt-3.5-turbo-16k\",\n",
        "\"gpt-4-turbo-preview\",\n",
        "\"gpt-4-vision-preview\",\n",
        "\"gpt-4-turbo-2024-04-09\",\n",
        "\"gpt-4-0314\",\n",
        "\"gpt-4-32k-0314\",\n",
        "\"gpt-4-32k-0613\",\n",
        "\"gpt-3.5-turbo-0301\",\n",
        "\"gpt-3.5-turbo-16k-0613\",\n",
        "\"gpt-3.5-turbo-1106\",\n",
        "\"gpt-3.5-turbo-0613\",\n",
        "\"text-embedding-3-large\",\n",
        "\"text-embedding-3-small\",\n",
        "\"text-embedding-ada-002\"]"
      ],
      "metadata": {
        "id": "5leS-qUJjOpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"Hello how are you?\",\n",
        "          \"Who are you?\",\n",
        "          \"What is ChatGPT?\",\n",
        "          \"What is LLM?\",\n",
        "          \"What is AI?\"\n",
        "          ]"
      ],
      "metadata": {
        "id": "vsGXpkwnGySc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###สร้างไฟล์ jsonl\n",
        "a = []\n",
        "for i in range (len(prompt)) :\n",
        "  a.append({\"custom_id\": \"request-\"+str(i+1), \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an helpful assistant.\"},{\"role\": \"user\", \"content\": prompt[i]}],\"max_tokens\": 2000}})"
      ],
      "metadata": {
        "id": "3dL8K3BQGxMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "for i in range (len(a)) :\n",
        "  with open(\"batchinput.jsonl\", \"a\", encoding='utf-8') as final:\n",
        "      json.dump(a[i], final, ensure_ascii=False)\n",
        "      final.write('\\n')\n",
        "#files.download('batchinput.jsonl')\n",
        "!cp batchinput.jsonl \"/content/batchinput.jsonl\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDBKGSbrHcX8",
        "outputId": "c2cf89e8-942f-4e39-9b95-9dd9bb3acc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: 'batchinput.jsonl' and '/content/batchinput.jsonl' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "batch_input_file = client.files.create(\n",
        "  file=open(\"/content/batchinput.jsonl\", \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")"
      ],
      "metadata": {
        "id": "ozquTwkQHfrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_input_file_id = batch_input_file.id\n",
        "\n",
        "client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvBSdbCNHlm3",
        "outputId": "08b230fe-8a99-4830-ac62-63f59d27185c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_8AzmtMLuDNPHyiPlxZtkZCeI', completion_window='24h', created_at=1724079064, endpoint='/v1/chat/completions', input_file_id='file-nrE1KoNT277a8mNQw3mnYJvF', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1724165464, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "###ดูผล status จากไฟล์ Batch\n",
        "client.batches.retrieve(\"batch_8AzmtMLuDNPHyiPlxZtkZCeI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTOpzrL3Hvx4",
        "outputId": "e4eed63e-486b-4d02-b68b-6db0a3972f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_8AzmtMLuDNPHyiPlxZtkZCeI', completion_window='24h', created_at=1724079064, endpoint='/v1/chat/completions', input_file_id='file-nrE1KoNT277a8mNQw3mnYJvF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1724079087, error_file_id=None, errors=None, expired_at=None, expires_at=1724165464, failed_at=None, finalizing_at=1724079086, in_progress_at=1724079064, metadata=None, output_file_id='file-ESXASVbA0ZIASm9ER1J3MpB7', request_counts=BatchRequestCounts(completed=5, failed=0, total=5))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "###ดูผล output จากไฟล์ Batch\n",
        "file_response = client.files.content(\"file-ESXASVbA0ZIASm9ER1J3MpB7\")\n",
        "print(file_response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxzMQv9JH7a-",
        "outputId": "29a4ebc0-b22a-4ea2-f2da-d166edabcd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"batch_req_Q4FRy5Y8xzraXsVN2FPdFjh7\", \"custom_id\": \"request-1\", \"response\": {\"status_code\": 200, \"request_id\": \"ca8f733f87aafe9e75643bf2e51f0d5b\", \"body\": {\"id\": \"chatcmpl-9xy11JGmBmMYqMKOS5Uy8N1nhxNp3\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 22, \"completion_tokens\": 30, \"total_tokens\": 52}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n",
            "{\"id\": \"batch_req_Zuh1PmBg1vfHc71fOlrJH7n5\", \"custom_id\": \"request-2\", \"response\": {\"status_code\": 200, \"request_id\": \"fef0f4c72c7e8e13df13125ea2287aa8\", \"body\": {\"id\": \"chatcmpl-9xy11ulqzX1cO4HPxUAxukW6KrcRV\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I am an artificial intelligence developed to assist you with information, tasks, and answering questions. How can I help you today?\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 21, \"completion_tokens\": 25, \"total_tokens\": 46}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n",
            "{\"id\": \"batch_req_jHjUH9pGM9jIeW65dMRKTSFM\", \"custom_id\": \"request-3\", \"response\": {\"status_code\": 200, \"request_id\": \"061ad9c37c32c730cf2e4c179f3ab7ae\", \"body\": {\"id\": \"chatcmpl-9xy11hrFl3B4E7JMueH2FicsIu6Qb\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"ChatGPT is an advanced language processing AI model developed by OpenAI. It's designed to understand and generate human-like text based on the input it receives. This model is a part of the Generative Pre-trained Transformer (GPT) family, specifically leveraging GPT-3 or even GPT-4, depending on the version you are referring to.\\n\\nChatGPT works by predicting the next word in a sentence, given all the previous words in the input. This allows it to generate coherent and contextually relevant responses, making it useful for a variety of applications including customer support, content creation, tutoring, data analysis, and more.\\n\\nThe model has been trained on a diverse range of internet text, although it doesn't know specific documents it was trained on and doesn't have direct access to any personal data unless it has been shared with it in the course of the conversation. The training process involves feeding the model vast amounts of text and using machine learning techniques to help it understand patterns in the language.\\n\\nOverall, ChatGPT is a versatile and powerful tool for generating human-like text and aiding users in a wide array of tasks requiring language comprehension and production.\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 21, \"completion_tokens\": 224, \"total_tokens\": 245}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n",
            "{\"id\": \"batch_req_fY72iIL90GrSDqaiQ9uaDME6\", \"custom_id\": \"request-4\", \"response\": {\"status_code\": 200, \"request_id\": \"e27d56ee7ef65e3e1131798aeaf77dbd\", \"body\": {\"id\": \"chatcmpl-9xy11IDO8B3tMl8dRXcQG9gOCMgt2\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"\\\"LLM\\\" can refer to different things depending on the context. Here are a few common meanings:\\n\\n1. **Master of Laws (LL.M.)**: An advanced, postgraduate academic degree in law, typically pursued by individuals who already have a first degree in law. It allows for specialization in specific areas such as international law, tax law, human rights law, etc.\\n\\n2. **Large Language Model**: In the context of artificial intelligence and natural language processing, an LLM refers to a type of AI model trained on vast amounts of text data to understand and generate human-like text. Examples include GPT-3 by OpenAI, BERT by Google, and others. These models are used in various applications, such as chatbots, machine translation, and text summarization.\\n\\n3. **Logical Link Management**: In networking, LLM refers to protocols and mechanisms that manage the communication links between network nodes to ensure data is transmitted efficiently and correctly.\\n\\nThe most relevant interpretation usually depends on the specific field or context being discussed. If you can provide more context, I can offer a more precise explanation.\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 21, \"completion_tokens\": 223, \"total_tokens\": 244}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n",
            "{\"id\": \"batch_req_spyNHes8nx8Sr5TgH9ck7Jf3\", \"custom_id\": \"request-5\", \"response\": {\"status_code\": 200, \"request_id\": \"f20c7019290152b5e7b9e5662fc89936\", \"body\": {\"id\": \"chatcmpl-9xy11JkHtpdCvblvInrjBT32xN25x\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn. These machines can perform tasks that typically require human intelligence, such as:\\n\\n1. **Learning:** Acquiring information and rules for using that information.\\n2. **Reasoning:** Using rules to reach approximate or definite conclusions.\\n3. **Problem Solving:** Identifying and solving complex problems.\\n4. **Perception:** Interpreting sensory information to understand the environment.\\n5. **Language Understanding:** Comprehending and generating human language.\\n\\nThere are two primary types of AI:\\n\\n1. **Narrow AI (Weak AI):** Designed and trained for a specific task or narrow range of tasks. Examples include virtual personal assistants like Siri and Alexa, recommendation systems like those used by Netflix or Amazon, and facial recognition software.\\n\\n2. **General AI (Strong AI):** Hypothetical AI that has the ability to perform any intellectual task that a human can do. It would have the ability to understand, learn, and apply knowledge in ways comparable to human intelligence. This level of AI does not currently exist.\\n\\nAI technologies often involve various subfields, such as:\\n\\n- **Machine Learning (ML):** A subset of AI involving algorithms that enable computers to learn from and make decisions based on data. Types of machine learning include supervised learning, unsupervised learning, and reinforcement learning.\\n- **Deep Learning:** A subset of machine learning involving neural networks with many layers, designed to mimic the human brain's structure and function. It is especially powerful for tasks like image and speech recognition.\\n- **Natural Language Processing (NLP):** AI's ability to understand and generate human language, enabling interactions between computers and humans in everyday language.\\n- **Computer Vision:** Allowing AI to interpret and make decisions based on visual information from the world.\\n\\nAI has vast applications across numerous fields including healthcare, robotics, automotive (self-driving cars), finance, entertainment, and more. Its development continues to advance rapidly, leading to ongoing discussions about ethical implications, future uses, and societal impacts.\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 20, \"completion_tokens\": 418, \"total_tokens\": 438}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_response.text.split(\"\\n\")[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uftWQULJ1aq",
        "outputId": "f49d2534-85dd-4603-ff3a-59c561556321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"batch_req_Zuh1PmBg1vfHc71fOlrJH7n5\", \"custom_id\": \"request-2\", \"response\": {\"status_code\": 200, \"request_id\": \"fef0f4c72c7e8e13df13125ea2287aa8\", \"body\": {\"id\": \"chatcmpl-9xy11ulqzX1cO4HPxUAxukW6KrcRV\", \"object\": \"chat.completion\", \"created\": 1724079079, \"model\": \"gpt-4o-2024-05-13\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I am an artificial intelligence developed to assist you with information, tasks, and answering questions. How can I help you today?\", \"refusal\": null}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 21, \"completion_tokens\": 25, \"total_tokens\": 46}, \"system_fingerprint\": \"fp_3aa7262c27\"}}, \"error\": null}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##ปรินต์ output text\n",
        "import json\n",
        "\n",
        "response = []\n",
        "# Assuming `file_response.text` contains the file content as a string\n",
        "for i in file_response.text.split(\"\\n\"):\n",
        "    i = i.strip()  # Remove leading and trailing whitespace\n",
        "    if not i:  # Skip empty lines\n",
        "        continue\n",
        "    try:\n",
        "        response.append(json.loads(i))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Skipping line due to error: {e}\")\n",
        "        print(f\"Problematic line: {i}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "E8MXwAGHKVB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in response :\n",
        "  print(i[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
        "  print(\"-----------------------------------------------------------------------------------------------------\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8GDjPh8KXNX",
        "outputId": "a9c70454-1932-42fa-b120-5e91d39aa66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "I am an artificial intelligence developed to assist you with information, tasks, and answering questions. How can I help you today?\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "ChatGPT is an advanced language processing AI model developed by OpenAI. It's designed to understand and generate human-like text based on the input it receives. This model is a part of the Generative Pre-trained Transformer (GPT) family, specifically leveraging GPT-3 or even GPT-4, depending on the version you are referring to.\n",
            "\n",
            "ChatGPT works by predicting the next word in a sentence, given all the previous words in the input. This allows it to generate coherent and contextually relevant responses, making it useful for a variety of applications including customer support, content creation, tutoring, data analysis, and more.\n",
            "\n",
            "The model has been trained on a diverse range of internet text, although it doesn't know specific documents it was trained on and doesn't have direct access to any personal data unless it has been shared with it in the course of the conversation. The training process involves feeding the model vast amounts of text and using machine learning techniques to help it understand patterns in the language.\n",
            "\n",
            "Overall, ChatGPT is a versatile and powerful tool for generating human-like text and aiding users in a wide array of tasks requiring language comprehension and production.\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\"LLM\" can refer to different things depending on the context. Here are a few common meanings:\n",
            "\n",
            "1. **Master of Laws (LL.M.)**: An advanced, postgraduate academic degree in law, typically pursued by individuals who already have a first degree in law. It allows for specialization in specific areas such as international law, tax law, human rights law, etc.\n",
            "\n",
            "2. **Large Language Model**: In the context of artificial intelligence and natural language processing, an LLM refers to a type of AI model trained on vast amounts of text data to understand and generate human-like text. Examples include GPT-3 by OpenAI, BERT by Google, and others. These models are used in various applications, such as chatbots, machine translation, and text summarization.\n",
            "\n",
            "3. **Logical Link Management**: In networking, LLM refers to protocols and mechanisms that manage the communication links between network nodes to ensure data is transmitted efficiently and correctly.\n",
            "\n",
            "The most relevant interpretation usually depends on the specific field or context being discussed. If you can provide more context, I can offer a more precise explanation.\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn. These machines can perform tasks that typically require human intelligence, such as:\n",
            "\n",
            "1. **Learning:** Acquiring information and rules for using that information.\n",
            "2. **Reasoning:** Using rules to reach approximate or definite conclusions.\n",
            "3. **Problem Solving:** Identifying and solving complex problems.\n",
            "4. **Perception:** Interpreting sensory information to understand the environment.\n",
            "5. **Language Understanding:** Comprehending and generating human language.\n",
            "\n",
            "There are two primary types of AI:\n",
            "\n",
            "1. **Narrow AI (Weak AI):** Designed and trained for a specific task or narrow range of tasks. Examples include virtual personal assistants like Siri and Alexa, recommendation systems like those used by Netflix or Amazon, and facial recognition software.\n",
            "\n",
            "2. **General AI (Strong AI):** Hypothetical AI that has the ability to perform any intellectual task that a human can do. It would have the ability to understand, learn, and apply knowledge in ways comparable to human intelligence. This level of AI does not currently exist.\n",
            "\n",
            "AI technologies often involve various subfields, such as:\n",
            "\n",
            "- **Machine Learning (ML):** A subset of AI involving algorithms that enable computers to learn from and make decisions based on data. Types of machine learning include supervised learning, unsupervised learning, and reinforcement learning.\n",
            "- **Deep Learning:** A subset of machine learning involving neural networks with many layers, designed to mimic the human brain's structure and function. It is especially powerful for tasks like image and speech recognition.\n",
            "- **Natural Language Processing (NLP):** AI's ability to understand and generate human language, enabling interactions between computers and humans in everyday language.\n",
            "- **Computer Vision:** Allowing AI to interpret and make decisions based on visual information from the world.\n",
            "\n",
            "AI has vast applications across numerous fields including healthcare, robotics, automotive (self-driving cars), finance, entertainment, and more. Its development continues to advance rapidly, leading to ongoing discussions about ethical implications, future uses, and societal impacts.\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Call Gemini API"
      ],
      "metadata": {
        "id": "vPwz8z6MD5Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "oloW5WPFD_0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "###ใส่ Gemini API key ที่ได้มา\n",
        "API = input() #You can get free tier API key for free!!!\n",
        "genai.configure(api_key=API)"
      ],
      "metadata": {
        "id": "tajmM7CAEISs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#โมเดลที่มีให้บริการ\n",
        "all_model = [\"gemini-1.0-pro-001\", \"gemini-1.5-pro-001\", \"gemini-1.5-flash-001\"]"
      ],
      "metadata": {
        "id": "V7pFFMD6GLGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash-001\")"
      ],
      "metadata": {
        "id": "lWzEmYMNFMeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Hello\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eRjbFQeOFQKz",
        "outputId": "193a37b8-c0e9-4740-e7a4-5ac73c0df59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nbxJ4MZVIXW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Call Claude API"
      ],
      "metadata": {
        "id": "fahjUda_IY15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-LuG3X4IdrT",
        "outputId": "35acd698-a3c3-4c68-eee3-04a28cb688ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.34.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Downloading anthropic-0.34.1-py3-none-any.whl (891 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.5/891.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API = input() ###ใส่ Anthropic API key ที่ได้มา"
      ],
      "metadata": {
        "id": "kJELvKnXxFBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "all_model = [\"claude-3-5-sonnet-20240620\", \"claude-3-opus-20240229\", \"claude-3-haiku-20240307\", \"claude-3-sonnet-20240229\" ]\n",
        "prompt = input()\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=API,\n",
        ")\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    system=\"You are helpful assistant.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVgB-uv8ImI6",
        "outputId": "f5eeb625-10f8-4d3e-ef8c-759d454648ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whst is LLM?\n",
            "LLM stands for \"Large Language Model.\" It refers to a type of artificial intelligence model designed to understand, generate, and manipulate human language. Some key points about LLMs include:\n",
            "\n",
            "1. Size: These models are typically very large, often containing billions of parameters.\n",
            "\n",
            "2. Training: They are trained on vast amounts of text data from various sources like books, websites, and articles.\n",
            "\n",
            "3. Capabilities: LLMs can perform a wide range of language tasks, including:\n",
            "   - Text generation\n",
            "   - Translation\n",
            "   - Summarization\n",
            "   - Question answering\n",
            "   - Sentiment analysis\n",
            "   - And more\n",
            "\n",
            "4. Examples: Some well-known LLMs include GPT (Generative Pre-trained Transformer) series, BERT, T5, and others.\n",
            "\n",
            "5. Applications: They are used in various fields, including chatbots, content creation, research, and data analysis.\n",
            "\n",
            "6. Challenges: LLMs can sometimes produce biased or inaccurate information and require significant computational resources.\n",
            "\n",
            "7. Ongoing research: The field of LLMs is rapidly evolving, with continuous improvements in model architecture, training techniques, and applications.\n",
            "\n",
            "LLMs have significantly advanced natural language processing and are at the forefront of AI research and applications in language-related tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####NVIDIA API (Llama-3.1-405b-instruct)\n"
      ],
      "metadata": {
        "id": "v6p5X3I89dim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOENAPMI9mU_",
        "outputId": "6e205cf5-bbd0-44dd-a829-8ef27eb42d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.42.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = input() ###ใส่ NVIDIA API key ที่ได้มา\n",
        "client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = api_key\n",
        ")"
      ],
      "metadata": {
        "id": "KvM_NKTn9qtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"meta/llama-3.1-405b-instruct\",\n",
        "  messages=[{\"role\":\"user\",\"content\":\"สวัสดีครับ\"}],\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_tokens=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "rE2pglDS94pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kay3SE-L-BlV",
        "outputId": "6fff94e1-cf4d-4dfc-f875-96f63f81eb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "สวัสดีครับ! มีอะไรที่ผมสามารถช่วยคุณได้บ้างครับ?\n"
          ]
        }
      ]
    }
  ]
}